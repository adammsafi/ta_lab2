---
title: "soFar_20251108"
author: "Adam Safi"
created: 2025-11-08T19:04:00+00:00
modified: 2025-11-17T21:05:00+00:00
original_path: "C:\Users\asafi\Documents\ProjectTT\Plans&Status\soFar_20251108.docx"
original_size_bytes: 32231
---
Here’s the story arc in plain English—what changed at each step, why
it changed, and what it unlocked.

**Phase 1 — Single-file exploration (CSV →
notebooks/scripts)**

**What you did:** Pulled daily BTC data to a CSV and
analyzed it with two Python files: a driver script (BTC Price
Analysis.py) and a helper library (Data\_Analysis\_Functions.py).  
**Why:** Fastest way to prototype ideas (returns, vol,
EMAs, regimes, plots) without worrying about packaging.  
**What it unlocked:** Rapid iteration, visual feedback, and
a concrete sense of which features mattered (ema slopes, flip segments,
comovement regimes).  
**Tradeoffs:** Growing complexity, brittle imports,
duplicated logic, and harder reuse/testing as the codebase expanded.

**Phase 2 — From scripts to a package (ta\_lab →
ta\_lab2)**

**What you did:** Reorganized the ad-hoc analysis into a
real Python package with modules
(features/returns/vol/ema/regimes/segments/viz), public APIs, and shims
for backward compatibility.  
**Why:** Maintainability and scale—clear module boundaries,
versioning, tests, and CI become possible. This is reflected in your
commit themes (compatibility shims, refactors, indicators, regimes
stats, viz integration).  
**What it unlocked:**

* Reusability: importable functions across projects.
* Stability: backward-compat shims so old code still runs.
* Velocity: smaller, focused modules let you add features (e.g.,
  multi-timeframe resampling, robust calendar features) without breaking
  everything.  
  **Tradeoffs:** Up-front structure/overhead (naming,
  packaging, CI), but far lower tech debt later.

**Phase 3 — From static files to data pipelines (fedtools2 +
directories)**

**What you did:** Proved you can **pull from an
API** and **persist to a database** (locally first),
and organized supporting folders (Data\_Tools, ETL, FinancialData,
FedData).  
**Why:** CSVs are great for exploration, but they don’t
scale or stay fresh. A DB-backed pipeline enables repeatability,
idempotency, and incremental updates—foundation for production
workflows.  
**What it unlocked:**

* Structured storage (schema evolution, indexes).
* Scheduled ingestion (timers) and auditability (timestamps, change
  logs).
* Separation of concerns (ingest → transform → analyze).  
  **Tradeoffs:** You now must manage schemas, migrations,
  credentials, and job reliability.

**Phase 4 — Cloud enablement (VM + remote DB)**

**What you did:** Moved the database to a VM, connected
securely from local, and planned automation (timers/schedulers) for API
pulls and DB updates.  
**Why:** Availability and scale—jobs can run when your
laptop is off; infra can grow independently of your dev machine; remote
access supports collaboration and future services.  
**What it unlocked:**

* Always-on ingestion and analytics.
* Clear boundary between **data platform** (VM/DB) and
  **analysis apps** (ta\_lab2).
* Path to multi-environment practices (dev/stage/prod).  
  **Tradeoffs:** Ops overhead (monitoring, backups, security
  hardening, costs).

**How the commits map to this evolution**

* **Packaging & Compat:** “Initial package
  structure,” “legacy shims,” “df-friendly APIs,” “newest\_top flipping” →
  transforming script logic into stable, importable modules without
  breaking older entry points.
* **Feature Depth:** “multi-timeframe resampling,”
  “ADX vectorization,” “EMA comovement stats/hierarchy,” “trend
  labeling/segments,” “viz integration” → richer analytics built on
  modular foundations.
* **Quality & Release:** “tests,” “ci + release
  automation,” “changelog” → maturing from prototype to versioned package
  with guardrails.
* **Config & Pipelines:** “config shims/paths,”
  “pipeline returns summary,” “placeholder data for config” →
  standardizing configuration and enabling repeatable runs.

**Big picture progression (capabilities ↑, tech debt
↓)**

1. **Prototype** (CSV + 2 files) → 2)
   **Library/Package** (ta\_lab2) → 3) **Data
   Platform** (API→DB pipelines) → 4) **Cloud-Native**
   (VM + schedulers + remote DB).  
   Each step trades a bit of speed for a lot of **reliability,
   reproducibility, and scale**.

**What this sets you up for next**

* **Data contracts & schemas:** lock column
  names/types and version your DB schema (Alembic) to keep ingestion and
  analytics in sync.
* **Job orchestration:** start simple (Windows Task
  Scheduler/crontab) → graduate to Airflow/Prefect when you need
  dependency graphs and SLAs.
* **Observability:** structured logs, metrics,
  alerting (failures, late data, anomaly flags).
* **Model/Signal governance:** versioned indicators
  & signals, backtest registries, and performance dashboards.
* **Packaging polish:** semantic versioning, API docs
  (mkdocs), example notebooks that import ta\_lab2 against the live
  DB.

In short: you moved from **“try ideas quickly”** →
**“codify reusable analytics”** → **“treat data as a
product”** → **“run it reliably in the cloud.”**

**You said:**

Does the progression make logical sense, and how do i know put this
all together, work on it, track the development. There are a lot of
different things I want to work on first, so i need an in depth and
detailed plan, a high level overview of that plan and realistic
timelines and deadlines, in addition to all this a plan for
documentation, and a plan to educate myself as I go so I can understand
exactly how this was built, what the components do and not get ahead of
my skis

**ChatGPT said:**

Thought for 18s

Absolutely—the progression is logical. You went from quick
exploration → maintainable package → reliable data pipeline →
cloud-ready automation. Here’s a complete plan you can execute, track,
and learn from as you go.

**High-level overview (one page)**

* **Goal:** Stand up a small, production-ish analytics
  stack for BTC (and later macro/FRED) with clean packages, a DB-backed
  pipeline, automated updates, and publishable docs.
* **Workstreams:**

  1. **Data platform** (ingest → store →
     schedule)
  2. **Analytics package** (ta\_lab2)
  3. **Visualization + notebooks**
  4. **CI/CD + quality** (tests, lint, release)
  5. **Docs + runbooks**
  6. **Learning track** (targeted topics aligned to the
     work)
* **Operating model:** 1–2 focused priorities per
  week, strict WIP limit, weekly demo, weekly retro, GitHub Project board,
  semantic versioning, conventional commits.
* **Timeline:** 12 weeks (Nov 10, 2025 → Jan 30, 2026)
  with holiday buffers.

**Detailed 12-week plan (dates, deliverables,
acceptance)**

Start: **Mon, Nov 10, 2025**. End: **Fri, Jan 30,
2026**. US holidays noted (lighter scope Weeks 3, 7–8).

**Week 1 (Nov 10–14): Repo hygiene & project
scaffolding**

**Deliverables**

* GitHub Projects board (Columns: Backlog / Next /
  In-Progress / Review / Done).
* Labels: pkg, pipeline, db, viz, infra, docs, good-first,
  blocked, bug.
* Issue templates (bug/feature/tech-debt) + PR template (checklist,
  tests, docs touched).
* CODEOWNERS, CONTRIBUTING.md, SECURITY.md
  (basic).
* Decide Python version & env tool (e.g., uv or conda). Lock
  pyproject.toml.

**Acceptance**

* Fresh clone installs with one command; pytest -q runs a
  sample test.
* Project board has an initial backlog of 30–50 granular
  issues.

**Week 2 (Nov 17–21): Stabilize ta\_lab2 surface
area**

**Deliverables**

* Finalize module boundaries (e.g., features, returns, vol, ema,
  regimes, segments, viz, io).
* Backward-compat shims kept but **clearly
  deprecated** with warnings.
* Type hints, pydantic models (where config/data contracts
  help).
* Unit tests to 60%+ coverage for public APIs.
* Release **v0.3.1** (patch) with
  CHANGELOG.

**Acceptance**

* from ta\_lab2 import ... works from a clean environment.
* API docs autogenerate locally (mkdocs serve or pdoc).

**Week 3 (Nov 24–28, Thanksgiving): Local ingest POC (FRED or
CMC)**

**Deliverables**

* fedtools2 minimal CLI: ingest fred --series CPIAUCSL (as example)
  and ingest cmc --symbol BTC.
* Local Postgres schema v1: ohlcv, series\_metadata, ingest\_log.
  Simple Alembic migration.
* Idempotent writes (UPSERT on natural keys), ingestion audit row
  per run.

**Acceptance**

* Two successive runs don’t duplicate rows.
* SELECT COUNT(\*), MIN(date), MAX(date) returns expected
  window.

**Week 4 (Dec 1–5): Schema hardening + contracts**

**Deliverables**

* Data contracts (column names/types, nullability, constraints)
  written as Markdown + Alembic migration scripts.
* Indexes for common access patterns (symbol+date).
* io module in ta\_lab2 to load from DB, not just CSV.

**Acceptance**

* ta\_lab2.io.load\_ohlcv(symbol="BTC", source="db") returns a typed
  DataFrame with the contract enforced.

**Week 5 (Dec 8–12): Cloud VM + secure access**

**Deliverables**

* VM provisioned (OCI, per your earlier plan). Hardened SSH, UFW
  rules, non-root Postgres.
* Service user + systemd for ingestion job runner.
* Local → VM DB connectivity via SSH tunnel; connection
  string documented.

**Acceptance**

* psql from local via tunnel works.
* Latency test + a small ingest run executes on the VM.

**Week 6 (Dec 15–19): Scheduling + reliability
basics**

**Deliverables**

* Timers: start with cron/Windows Task Scheduler equivalents on
  VM.
* Logging: structured JSON logs; log rotation; a daily
  success/failed summary.
* Retries with exponential backoff; dead-letter (persist failed
  payload metadata).

**Acceptance**

* Kill the internet and watch a job retry then report failure;
  restore and re-run succeeds.
* Daily job produces one log file + one ingest\_log record.

**Week 7 (Dec 22–26, Holidays): Analytics ↔︎ DB integration
(light)**

**Deliverables**

* ta\_lab2 functions that operate directly on DB-loaded frames;
  e.g., EMA comovement stats computed over a date range pulled from
  DB.
* Cache layer (parquet on disk) to avoid re-querying unchanged
  spans.

**Acceptance**

* One end-to-end notebook: DB → compute indicators → regimes →
  chart.

**Week 8 (Dec 29–Jan 2, Holidays): Visualization &
notebooks (light)**

**Deliverables**

* viz utilities stabilized (matplotlib defaults consistent; no
  seaborn unless you decide to).
* 2–3 example notebooks: “Returns & Vol”, “EMA Trends &
  Regimes”, “Segments Explorer”.

**Acceptance**

* Notebooks run top-to-bottom from a clean environment with a small
  sample DB.

**Week 9 (Jan 5–9): Observability & Ops polish**

**Deliverables**

* Metrics: job duration, rows ingested, rows changed,
  time-since-last-update.
* Alerts: email/Telegram on failures or data staleness > X
  hours.
* Runbook: “How to fix failed ingest,” “How to rotate keys,” “How
  to restore from backup.”

**Acceptance**

* Trigger a controlled failure; alert fires; runbook steps resolve
  it.

**Week 10 (Jan 12–16): Documentation sprint**

**Deliverables**

* Docs site (mkdocs + material):

  + **Concepts:** data contracts, regimes,
    segments
  + **How-to:** install, ingest, query, compute,
    visualize
  + **Reference:** API (autogenerated), CLI
    commands
  + **Ops:** runbooks, backups, restore,
    monitoring
  + **ADR folder:** short “Architecture Decision
    Records” (one page each)
* Add “guided tours” (3 short docs tying together tasks
  end-to-end).

**Acceptance**

* A new reader can go from zero → first chart in <30 minutes
  following the docs.

**Week 11 (Jan 19–23): Testing, CI/CD, release**

**Deliverables**

* CI: lint (ruff), type check (pyright/mypy), tests (pytest with
  coverage gate), build, publish.
* Pre-commit hooks (black/ruff/pyproject sync).
* Release **v0.4.0** (minor): stable IO,
  regimes/segments/viz APIs, docs published.

**Acceptance**

* A tagged release builds and publishes; CHANGELOG generated via
  conventional commits.

**Week 12 (Jan 26–30): Buffer, hardening, roadmap
v0.5**

**Deliverables**

* Close open bugs; performance passes on hotspots; small refactors
  with benchmarks.
* Next-quarter roadmap (scope, themes, risks).
* Post-mortem: what slowed you down, what to standardize.

**Acceptance**

* Board is green; only known low-priority items remain; roadmap PR
  approved.

**How to track the work (practical mechanics)**

**Git & releases**

* **Branching:** main (protected), release/\*, feat/\*,
  fix/\*, docs/\*.
* **Commits:** Conventional Commits (feat:, fix:,
  docs:, refactor:, chore:, test:).
* **Releases:** Semantic Versioning.

  + Patch = bugfix/internal (0.3.1)
  + Minor = new features, backward-compatible (0.4.0)
  + Major = breaking changes (1.0.0)

**GitHub Projects board**

* **Columns:** Backlog → Next → In-Progress → Review →
  Done.
* **WIP limit:** Max 3 cards In-Progress.
* **Cadence:**

  + **Daily 15-min** stand-alone check: What’s blocked?
    What’s next?
  + **Weekly 45-min** demo/retro: Show output, log 2–3
    improvements.
  + **Monthly** milestone review: commit to next month’s
    scope.

**Issue hygiene (short, consistent)**

* Title: imperative (“Add UPSERT to ohlcv writer”).
* Body: **Context**, **Definition of
  Done**, **Test/Acceptance**,
  **Risk/Notes**.
* Size: aim for 0.5–1.5 days each; split otherwise.

**Documentation plan (living, minimal, complete)**

* **Tooling:** mkdocs-material (or pdoc) + docs/ in
  ta\_lab2 and fedtools2.
* **Structure:**

  + docs/01-getting-started.md
  + docs/02-concepts/ (returns, vol, EMA, regimes, segments, data
    contracts)
  + docs/03-howto/ (install, ingest, query, compute, visualize,
    schedule)
  + docs/04-reference/ (autogenerated API)
  + docs/05-ops/ (runbooks: backups, restore, rotate keys, failure
    modes)
  + docs/06-adr/ (short ADRs: e.g., “Why Postgres,” “Why mkdocs,”
    “Why Alembic”)
  + examples/ (notebooks referenced by docs)
* **Quality bar (“docs DoD”):** every public function
  has a docstring + example; every CLI command has a --help example; every
  change that alters behavior updates docs in the same PR.

**Learning plan (so you don’t get ahead of your
skis)**

Tie learning to the week’s work; go deep only on what’s immediately
useful.

| **Week** | **Topic** | **Focus** |
| --- | --- | --- |
| 1 | Project ops | GitHub Projects, issue writing, semantic versioning, conventional commits |
| 2 | Python packaging | pyproject.toml, build backends, dependency pinning, type hints |
| 3 | Data ingestion | HTTP basics, retries, idempotency, FRED/CMC payloads |
| 4 | Data contracts | Schema design, Alembic migrations, indexes, keys |
| 5 | Linux & Postgres admin | SSH, UFW, systemd, roles, pg\_hba, backups |
| 6 | Scheduling & reliability | cron, logging, backoff, failure taxonomies |
| 7 | Analytics from DB | Efficient queries, caching, IO boundaries |
| 8 | Matplotlib fundamentals | Clean plots, axes APIs, saving figures |
| 9 | Observability | Metrics, alerting patterns, SLOs for data freshness |
| 10 | Documentation | mkdocs, docstrings, examples that actually run |
| 11 | CI/CD | Pytest, coverage, lint/type checks, release automation |
| 12 | Performance & cleanup | Profiling (%%timeit, cProfile), small refactors |

Keep a **Learning Log** in docs/notes/learning-log.md:
date, topic, what clicked, what’s still fuzzy, one follow-up
question.

**Prioritization & scope control**

* **Impact/Effort matrix:** Only start high-impact,
  low-effort first; defer high-effort unless impact is huge.
* **WIP limit:** Never more than **3**
  issues in progress.
* **Definition of Done (DoD):** code + tests + docs +
  typed + linted + CHANGELOG + example updated.

**DoD checklist (copy into PR template)**

* Unit tests for new/changed behavior
* Types and docstrings
* Docs updated (user + API)
* Example notebook (if user-facing)
* Changelog entry
* Benchmark if performance-sensitive

**Risks & mitigations**

* **Scope creep:** Use WIP limit + weekly demo to keep
  focus.
* **Hidden coupling:** Keep IO boundaries explicit (io
  module). Use ADRs for decisions.
* **Data quality drift:** Idempotent ingest +
  ingest\_log + anomaly checks (row counts, last date).
* **Holiday time loss:** Weeks 3, 7–8 already scoped
  lightly.
* **Single-point failure (VM):** Weekly base backup;
  document restore drill.

**“First 48 hours” checklist**

1. Create the GitHub Project, labels, templates, PR
   template.
2. Freeze Python version and env tool; commit
   pyproject.toml.
3. Add CI skeleton (lint/test on PR).
4. Write the first 20 issues (granular). Move 2–3 to
   **Next**.
5. Cut **v0.3.1-alpha** tag just to exercise
   release plumbing.
6. Start Week-1 tasks and book the end-of-week demo to
   yourself.

If you want, I can turn this into:

* a **GitHub Project starter** (CSV or JSON),
* an **issue pack** you can paste as GitHub issues,
  and
* a **mkdocs skeleton** you can drop into
  docs/.

Top of Form
