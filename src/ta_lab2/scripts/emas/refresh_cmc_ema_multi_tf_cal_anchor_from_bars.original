from __future__ import annotations

"""
refresh_cmc_ema_multi_tf_cal_anchor_from_bars.py

Runner for anchored calendar EMA refresh (US / ISO) WITH state tables.

Writes to:
  public.cmc_ema_multi_tf_cal_anchor_us
  public.cmc_ema_multi_tf_cal_anchor_iso

State tables (per scheme):
  public.cmc_ema_multi_tf_cal_anchor_us_state
  public.cmc_ema_multi_tf_cal_anchor_iso_state

Incremental logic:
  - Track last canonical close per (id, tf, period)
  - For CAL_ANCHOR, canonical closes are: roll_bar = FALSE
  - If state exists: dirty window start = MIN(last_canonical_ts) (scoped to selected ids)
  - If no state (or --full-refresh): run from --start

Hardening added vs the earlier pasted version:
  1) State table tolerant of nullable last_canonical_ts (CREATE won't "fix" existing tables).
     Runner drops NULL watermarks before computing dirty window.
  2) Watermark is scoped to selected ids (so unrelated ids can't drag start backward).
  3) State update validates that output table contains required columns (ts + roll_bar).
     (Optionally supports alternate timestamp column names if you extend TS_CANDIDATES.)
"""

import argparse
import os
import sys
from multiprocessing import Pool, cpu_count
from typing import List, Optional, Tuple

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from sqlalchemy.pool import NullPool

from ta_lab2.features.m_tf.ema_multi_tf_cal_anchor import (
    write_multi_timeframe_ema_cal_anchor_to_db,
)
from ta_lab2.scripts.bars.common_snapshot_contract import (
    resolve_db_url,
    get_engine,
    parse_ids,
    load_all_ids,
    load_periods,
)
from ta_lab2.scripts.emas.logging_config import setup_logging, add_logging_args, get_worker_logger
from ta_lab2.scripts.emas.state_management import (
    ensure_ema_state_table,
    load_ema_state,
    update_ema_state_from_output,
    compute_dirty_window_start,
)


# ---------------------------------------------------------------------
# Environment / IPython helper
# ---------------------------------------------------------------------

def _in_ipython() -> bool:
    try:
        from IPython import get_ipython  # type: ignore
        return get_ipython() is not None
    except Exception:
        return False


# ---------------------------------------------------------------------
# Output schema validation (ts + roll_bar)
# ---------------------------------------------------------------------

# If you ever rename the timestamp column, add candidates here.
TS_CANDIDATES = ("ts",)  # could extend to: ("ts", "time_close", "time_end")


def _get_table_columns(engine: Engine, schema: str, table: str) -> List[str]:
    sql = text(
        """
        SELECT column_name
        FROM information_schema.columns
        WHERE table_schema = :schema
          AND table_name   = :table
        ORDER BY ordinal_position;
        """
    )
    with engine.connect() as conn:
        rows = conn.execute(sql, {"schema": schema, "table": table}).fetchall()
    return [str(r[0]) for r in rows]


def _pick_ts_column(cols: List[str]) -> Optional[str]:
    cols_set = {c.lower() for c in cols}
    for cand in TS_CANDIDATES:
        if cand.lower() in cols_set:
            # return the canonical spelling from candidates
            return cand
    return None


def _require_output_schema(engine: Engine, schema: str, out_table: str) -> tuple[str, str]:
    """
    Returns (ts_col, roll_flag_col) after validating the output schema.
    """
    cols = _get_table_columns(engine, schema, out_table)
    cols_l = {c.lower() for c in cols}

    ts_col = _pick_ts_column(cols)
    if not ts_col:
        raise RuntimeError(
            f"[ema_anchor] Output table {schema}.{out_table} missing timestamp column. "
            f"Expected one of: {TS_CANDIDATES}. Found columns: {cols}"
        )

    roll_flag_col = "roll_bar"
    if roll_flag_col.lower() not in cols_l:
        raise RuntimeError(
            f"[ema_anchor] Output table {schema}.{out_table} missing '{roll_flag_col}' column. "
            f"Found columns: {cols}"
        )

    return ts_col, roll_flag_col


# ---------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------

def _filter_ids_by_bar_availability(
    engine: Engine,
    ids: List[int],
    scheme: str,
    min_bars_required: int,
    logger,
) -> List[int]:
    """
    Filter IDs to only those with sufficient canonical bars for the given scheme.

    Returns: List of IDs that have >= min_bars_required canonical bars in at least one TF
    """
    bars_table = f"public.cmc_price_bars_multi_tf_cal_anchor_{scheme.lower()}"

    sql = text(f"""
        SELECT DISTINCT id
        FROM {bars_table}
        WHERE id = ANY(:ids)
          AND is_partial_end = FALSE
        GROUP BY id, tf
        HAVING COUNT(*) >= :min_bars
    """)

    with engine.connect() as conn:
        result = pd.read_sql(sql, conn, params={
            "ids": ids,
            "min_bars": min_bars_required
        })

    if result.empty:
        logger.warning(f"No IDs have >= {min_bars_required} canonical bars in {bars_table}")
        return []

    valid_ids = sorted(set(int(row.id) for row in result.itertuples(index=False)))

    filtered_count = len(ids) - len(valid_ids)
    if filtered_count > 0:
        logger.info(f"Filtered out {filtered_count} IDs with insufficient data")
    logger.info(f"Processing {len(valid_ids)} IDs with >= {min_bars_required} canonical bars")

    return valid_ids


# ---------------------------------------------------------------------
# State handling - Now using shared state_management module
# ---------------------------------------------------------------------

def _update_state_anchor(engine: Engine, schema: str, state_table: str, out_table: str, scheme: str) -> None:
    """
    CAL_ANCHOR canonical closes are roll_bar = FALSE.
    This update reads bar metadata from the bars table and EMA metadata from output table.
    """
    ts_col, roll_flag_col = _require_output_schema(engine, schema, out_table)

    # Bars table for this scheme
    bars_table = f"cmc_price_bars_multi_tf_cal_anchor_{scheme.lower()}"

    # Use shared state update function with anchor-specific parameters
    update_ema_state_from_output(
        engine,
        schema,
        state_table,
        out_table,
        use_canonical_ts=True,
        ts_column=ts_col,
        roll_filter=f"{roll_flag_col} = FALSE",
        bars_table=bars_table,
        bars_schema=schema,
        bars_partial_filter="is_partial_end = FALSE"
    )


# ---------------------------------------------------------------------
# Worker
# ---------------------------------------------------------------------

def _process_id_scheme_worker(args_tuple: Tuple) -> int:
    """
    Worker function for parallel processing of (id, scheme) combinations.

    Returns: Number of rows upserted
    """
    (id_, scheme, resolved_db_url, periods, start, end, schema, daily_table,
     out_table, log_level, log_file) = args_tuple

    # Each worker gets its own logger with unique name for tracking
    worker_id = f"{id_}-{scheme}"
    logger = get_worker_logger(
        name="ema_cal_anchor",
        worker_id=worker_id,
        log_level=log_level,
        log_file=log_file,
    )

    try:
        logger.info(f"Starting EMA computation for id={id_}, scheme={scheme}")

        # Create engine with NullPool for worker (avoid connection pooling issues)
        engine = create_engine(resolved_db_url, poolclass=NullPool, future=True)

        n = write_multi_timeframe_ema_cal_anchor_to_db(
            [id_],
            calendar_scheme=scheme,
            start=start,
            end=end,
            ema_periods=periods,
            db_url=resolved_db_url,
            schema=schema,
            daily_table=daily_table,
            out_table=out_table,
            update_existing=True,  # Always upsert in workers
            verbose=False,
        )

        engine.dispose()

        logger.info(f"Completed: {n:,} rows written")
        return int(n or 0)

    except Exception as e:
        error_msg = str(e).lower()
        if "too many clients" in error_msg or "max_connections" in error_msg:
            logger.error(
                f"DATABASE CONNECTION LIMIT REACHED. "
                f"Solutions: (1) Reduce --num-processes, (2) Increase Postgres max_connections. "
                f"Error: {e}"
            )
        else:
            logger.error(f"Worker failed: {e}", exc_info=True)
        return 0


# ---------------------------------------------------------------------
# CLI / Runner
# ---------------------------------------------------------------------

def _parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(
        prog="refresh_cmc_ema_multi_tf_cal_anchor_from_bars",
        description="Incremental refresh for anchored calendar EMAs (US/ISO)",
        epilog="""
CONNECTION LIMITS: This script uses multiprocessing. Each worker needs database connections.
If you see "too many clients already" errors:
  1. Reduce --num-processes (default: 4, safe for most setups)
  2. Increase Postgres max_connections in postgresql.conf
  3. Check for other processes holding connections (pg_stat_activity)
        """
    )

    p.add_argument("--ids", default="all", help="all | comma-separated list, e.g. 1,52,1027")
    p.add_argument("--scheme", default="both", choices=["us", "iso", "both"], help="US, ISO, or both")

    p.add_argument("--schema", default="public")
    p.add_argument("--daily-table", default="cmc_price_histories7")

    p.add_argument("--start", default="2010-01-01")
    p.add_argument("--end", default=None)

    p.add_argument(
        "--periods",
        default="6,9,10,12,14,17,20,21,26,30,50,52,77,100,200,252,365",
        help="comma-separated EMA periods, or 'lut' to load from public.ema_alpha_lookup",
    )

    p.add_argument("--no-update", action="store_true", help="If set, delete+rewrite range instead of upsert")
    p.add_argument("--full-refresh", action="store_true", help="Ignore state; run full history from --start.")
    p.add_argument(
        "--num-processes",
        type=int,
        default=None,
        help="Number of parallel processes. Default: min(cpu_count(), 4) to avoid connection exhaustion"
    )

    add_logging_args(p)

    return p.parse_args(argv)


def main(argv: Optional[List[str]] = None) -> int:
    args = _parse_args(argv)

    # Setup logging
    logger = setup_logging(
        name="ema_anchor",
        level=args.log_level,
        log_file=args.log_file,
        quiet=args.quiet,
        debug=args.debug,
    )

    try:
        db_url = resolve_db_url(None)
    except Exception as e:
        logger.error(f"Failed to resolve DB URL: {e}", exc_info=True)
        return 1

    engine = get_engine(db_url)

    periods_arg = (args.periods or "").strip().lower()
    if periods_arg == "lut":
        periods = load_periods(engine, "lut", lut_table=f"{args.schema}.ema_alpha_lookup")
        if not periods:
            logger.error("ema_alpha_lookup returned no periods")
            return 1
        logger.info(f"Loaded {len(periods)} periods from ema_alpha_lookup")
    else:
        periods = [int(x.strip()) for x in args.periods.split(",") if x.strip()]
        logger.info(f"Using {len(periods)} periods from command line")

    scheme = args.scheme.upper()

    try:
        ids_result = parse_ids(args.ids)
        if ids_result == "all":
            ids = load_all_ids(db_url, f"{args.schema}.{args.daily_table}")
            logger.info(f"Loaded {len(ids)} ids from {args.schema}.{args.daily_table}")
        else:
            ids = ids_result
            logger.info(f"Processing {len(ids)} ids from command line")
    except Exception as e:
        logger.error(f"Failed to load ids: {e}", exc_info=True)
        return 1

    logger.info(f"Configuration: schema={args.schema}, start={args.start}, end={args.end}")
    logger.debug(f"Periods: {periods}")
    logger.info(f"Scheme: {scheme.lower()}, full_refresh={args.full_refresh}")

    # Determine number of parallel processes
    if args.num_processes:
        num_processes = args.num_processes
    else:
        num_processes = min(cpu_count(), 4)
        logger.info(f"Auto-limited parallel processes to {num_processes} (to avoid connection exhaustion)")

    # Determine which schemes to run
    schemes_to_run = []
    if scheme == "BOTH":
        schemes_to_run = ["US", "ISO"]
    elif scheme == "US":
        schemes_to_run = ["US"]
    else:
        schemes_to_run = ["ISO"]

    total_rows = 0

    try:
        for s in schemes_to_run:
            out_table = f"cmc_ema_multi_tf_cal_anchor_{s.lower()}"
            state_table = f"{out_table}_state"

            logger.info(f"Processing scheme={s}")
            logger.debug(f"Output table: {args.schema}.{out_table}")
            logger.debug(f"State table: {args.schema}.{state_table}")

            # Filter IDs by bar availability for this scheme
            min_bars_required = min(periods)
            logger.info(f"Filtering IDs with >= {min_bars_required} canonical bars in scheme={s}...")
            valid_ids = _filter_ids_by_bar_availability(
                engine=engine,
                ids=ids,
                scheme=s,
                min_bars_required=min_bars_required,
                logger=logger,
            )

            if not valid_ids:
                logger.warning(f"No valid IDs found for scheme={s}. Skipping.")
                continue

            ensure_ema_state_table(engine, args.schema, state_table)

            # Load state for incremental refresh
            state_df = pd.DataFrame()
            if not args.full_refresh:
                state_df = load_ema_state(engine, args.schema, state_table)
                if state_df.empty:
                    logger.info("No existing state found - running full refresh from --start")
                else:
                    logger.info(f"Loaded state with {len(state_df)} records")

            # Build work items for each ID with individual start_ts from state
            work_items = []
            for id_ in valid_ids:
                # Compute incremental start for this ID
                if args.full_refresh:
                    start_ts = args.start
                else:
                    dirty_start = compute_dirty_window_start(
                        state_df,
                        selected_ids=[id_],
                        default_start=args.start,
                    )
                    start_ts = dirty_start.isoformat() if dirty_start else args.start

                work_items.append(
                    (id_, s, db_url, periods, start_ts, args.end,
                     args.schema, args.daily_table, out_table,
                     args.log_level, args.log_file)
                )

            logger.info(f"Processing {len(work_items)} IDs in parallel (scheme={s})")

            # Execute in parallel
            with Pool(processes=num_processes) as pool:
                results = pool.map(_process_id_scheme_worker, work_items)

            scheme_total = sum(results)
            total_rows += scheme_total
            logger.info(f"Total upserted rows for scheme={s}: {scheme_total:,}")

            # Update state table
            logger.info("Updating state table...")
            _update_state_anchor(engine, args.schema, state_table, out_table, s)
            logger.info(f"State updated in {state_table}")

        logger.info(f"Anchored calendar EMA refresh complete: {total_rows:,} total rows")
        return 0

    except Exception as e:
        logger.error(f"Refresh failed: {e}", exc_info=True)
        return 1
    finally:
        engine.dispose()
        logger.debug("Database engine disposed")


if __name__ == "__main__":
    code = main()

    # Avoid noisy "SystemExit: 0" in Spyder/IPython.
    if _in_ipython():
        pass
    else:
        raise SystemExit(code)
