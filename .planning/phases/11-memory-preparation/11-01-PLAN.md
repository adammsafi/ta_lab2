---
phase: 11-memory-preparation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ta_lab2/tools/ai_orchestrator/memory/snapshot/__init__.py
  - src/ta_lab2/tools/ai_orchestrator/memory/snapshot/extract_codebase.py
  - src/ta_lab2/tools/ai_orchestrator/memory/snapshot/extract_conversations.py
  - src/ta_lab2/tools/ai_orchestrator/memory/snapshot/batch_indexer.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "AST extraction produces structured code analysis for any Python file"
    - "Conversation extraction parses Claude Code JSONL transcripts"
    - "Batch indexer adds memories to Mem0 with rate limiting"
  artifacts:
    - path: "src/ta_lab2/tools/ai_orchestrator/memory/snapshot/extract_codebase.py"
      provides: "AST-based Python file analysis"
      exports: ["extract_code_structure", "extract_directory_tree"]
    - path: "src/ta_lab2/tools/ai_orchestrator/memory/snapshot/extract_conversations.py"
      provides: "Claude Code JSONL parsing"
      exports: ["extract_conversation", "link_conversations_to_phases"]
    - path: "src/ta_lab2/tools/ai_orchestrator/memory/snapshot/batch_indexer.py"
      provides: "Batch memory operations with rate limiting"
      exports: ["batch_add_memories", "BatchIndexResult"]
  key_links:
    - from: "batch_indexer.py"
      to: "mem0_client.py"
      via: "get_mem0_client import"
      pattern: "from.*mem0_client import get_mem0_client"
    - from: "extract_codebase.py"
      to: "GitPython"
      via: "git metadata extraction"
      pattern: "from git import Repo"
---

<objective>
Create extraction and indexing infrastructure for Phase 11 memory snapshot operations.

Purpose: Establish reusable scripts that can extract code structure (AST), parse conversation history (JSONL), and batch-add memories to the existing Mem0+Qdrant system. These scripts form the foundation for all subsequent Phase 11 plans.

Output: Three core modules in src/ta_lab2/tools/ai_orchestrator/memory/snapshot/ with tested extraction and indexing capabilities.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-memory-preparation/11-CONTEXT.md
@.planning/phases/11-memory-preparation/11-RESEARCH.md

# Existing memory infrastructure to extend
@src/ta_lab2/tools/ai_orchestrator/memory/mem0_client.py
@src/ta_lab2/tools/ai_orchestrator/memory/metadata.py
@src/ta_lab2/tools/ai_orchestrator/memory/migration.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AST-based code extraction module</name>
  <files>
    src/ta_lab2/tools/ai_orchestrator/memory/snapshot/__init__.py
    src/ta_lab2/tools/ai_orchestrator/memory/snapshot/extract_codebase.py
  </files>
  <action>
Create the snapshot subpackage with __init__.py exposing public API.

Create extract_codebase.py with:
1. `extract_code_structure(file_path: Path) -> dict` - Parse Python file using ast module:
   - Extract functions (name, line_start, line_end, args, docstring)
   - Extract classes (name, line_start, methods list)
   - Extract imports (module, names)
   - Return dict with file path, functions, classes, imports, line_count, size_bytes

2. `get_file_git_metadata(repo_path: Path, file_path: Path) -> dict` - Using GitPython:
   - Get latest commit affecting file (iter_commits with max_count=1)
   - Return commit_hash (7-char), author_name, author_email, committed_datetime (ISO), message
   - Handle untracked files gracefully (return {"tracked": False, "commit_hash": "untracked"})

3. `extract_directory_tree(root_path: Path, exclusions: list[str]) -> list[dict]` - Walk directory:
   - Skip exclusions (__pycache__, .venv, .git, .pyc, data files)
   - For each .py file: call extract_code_structure + get_file_git_metadata
   - Return list of dicts with file info, code structure, and git metadata

Use pathlib for all path operations. Import git from gitpython. Handle encoding errors gracefully (skip file with warning).

Follow existing code style from migration.py (logging, docstrings, type hints).
  </action>
  <verify>
Run: `python -c "from ta_lab2.tools.ai_orchestrator.memory.snapshot import extract_code_structure; print('Import OK')"`
Run: `python -c "from ta_lab2.tools.ai_orchestrator.memory.snapshot.extract_codebase import extract_directory_tree; print(extract_directory_tree.__doc__)"`
  </verify>
  <done>
extract_codebase.py exports extract_code_structure, get_file_git_metadata, extract_directory_tree. Module importable without errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create conversation extraction module</name>
  <files>
    src/ta_lab2/tools/ai_orchestrator/memory/snapshot/extract_conversations.py
  </files>
  <action>
Create extract_conversations.py with:

1. `extract_conversation(jsonl_path: Path) -> list[dict]` - Parse Claude Code JSONL:
   - Read line by line with json.loads()
   - Handle message types: user-message, assistant-message, tool-use
   - Extract role, content, timestamp, message_id (where available)
   - Wrap in try/except, skip malformed lines with logging.warning
   - Return list of message dicts

2. `extract_phase_boundaries(planning_dir: Path, repo: Repo) -> dict[int, dict]` - Map phases to time ranges:
   - Scan .planning/phases/NN-* directories
   - For each phase, find SUMMARY.md files
   - Get git commit dates (first commit = start, last commit = end)
   - Return {phase_num: {"name": str, "start": ISO, "end": ISO, "summary_file": str}}

3. `link_conversations_to_phases(messages: list[dict], phase_boundaries: dict) -> dict[int, list[dict]]`:
   - Group messages by phase using timestamp date comparison
   - Handle messages outside phase boundaries (assign to "untracked" key)
   - Return {phase_num: [messages]}

4. `find_conversation_files(claude_projects_dir: Path, project_name: str) -> list[Path]`:
   - Look for project folder matching pattern (e.g., "C--Users-asafi-Downloads-ta-lab2")
   - Return all .jsonl files in that folder
   - Handle missing directory gracefully (return empty list with warning)

Use json stdlib for parsing. Handle UnicodeDecodeError with fallback encoding.
  </action>
  <verify>
Run: `python -c "from ta_lab2.tools.ai_orchestrator.memory.snapshot.extract_conversations import extract_conversation; print('Import OK')"`
Run: `python -c "from ta_lab2.tools.ai_orchestrator.memory.snapshot.extract_conversations import find_conversation_files; print(find_conversation_files.__doc__)"`
  </verify>
  <done>
extract_conversations.py exports extract_conversation, extract_phase_boundaries, link_conversations_to_phases, find_conversation_files. Module importable.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create batch memory indexer module</name>
  <files>
    src/ta_lab2/tools/ai_orchestrator/memory/snapshot/batch_indexer.py
    src/ta_lab2/tools/ai_orchestrator/memory/snapshot/__init__.py
  </files>
  <action>
Create batch_indexer.py with:

1. `@dataclass BatchIndexResult` - Result tracking (similar to MigrationResult in migration.py):
   - total: int, added: int, skipped: int, errors: int, error_ids: list[str]
   - __str__ method for human-readable summary

2. `batch_add_memories(client, memories: list[dict], batch_size: int = 50, delay_seconds: float = 0.5) -> BatchIndexResult`:
   - Process memories in batches of batch_size
   - Each memory dict has: {"content": str, "metadata": dict}
   - Call client.add() with infer=False (disable LLM conflict detection for bulk)
   - Use time.sleep(delay_seconds) between batches to avoid rate limits
   - Log progress every batch: "Batch N: X/Y memories added"
   - Track errors and continue processing (don't fail entire batch)
   - Return BatchIndexResult

3. `create_snapshot_metadata(source: str, directory: str, file_type: str, file_path: str, **kwargs) -> dict`:
   - Create standardized metadata for snapshots
   - Use existing create_metadata() from metadata.py as base
   - Add: milestone="v0.5.0", phase="pre_reorg", directory, file_type, file_path
   - Accept **kwargs for additional fields (function_count, class_count, commit_hash, etc.)
   - Include both simple tag ["pre_reorg_v0.5.0"] and structured metadata per CONTEXT.md

4. `format_file_content_for_memory(file_info: dict) -> str`:
   - Take dict from extract_code_structure (with git metadata merged)
   - Format into human-readable text suitable for memory content:
    ```
    File: {relative_path}
    Directory: {directory_name}
    Lines: {line_count}
    Functions: {function_names, comma-separated, max 10}
    Classes: {class_names, comma-separated, max 10}
    Commit: {commit_hash}

    Summary: Python module with N functions, M classes.
    ```
   - Keep under 500 chars for efficient embedding

Update __init__.py to export all public functions from all three modules.
  </action>
  <verify>
Run: `python -c "from ta_lab2.tools.ai_orchestrator.memory.snapshot import batch_add_memories, BatchIndexResult; print('Import OK')"`
Run: `python -c "from ta_lab2.tools.ai_orchestrator.memory.snapshot.batch_indexer import create_snapshot_metadata; m = create_snapshot_metadata('pre_reorg_v0.5.0', 'ta_lab2', 'source_code', 'test.py'); print(m.keys())"`
  </verify>
  <done>
batch_indexer.py exports BatchIndexResult, batch_add_memories, create_snapshot_metadata, format_file_content_for_memory. All snapshot modules importable from package.
  </done>
</task>

</tasks>

<verification>
1. All modules importable: `python -c "from ta_lab2.tools.ai_orchestrator.memory.snapshot import *"`
2. No syntax errors: `python -m py_compile src/ta_lab2/tools/ai_orchestrator/memory/snapshot/*.py`
3. Type hints valid: `python -c "import ast; [ast.parse(open(f).read()) for f in ['src/ta_lab2/tools/ai_orchestrator/memory/snapshot/extract_codebase.py', 'src/ta_lab2/tools/ai_orchestrator/memory/snapshot/extract_conversations.py', 'src/ta_lab2/tools/ai_orchestrator/memory/snapshot/batch_indexer.py']]"`
</verification>

<success_criteria>
- snapshot subpackage exists with __init__.py
- extract_codebase.py can parse Python files with AST and get git metadata
- extract_conversations.py can parse Claude Code JSONL files
- batch_indexer.py can batch-add memories with rate limiting
- All exports follow existing code patterns (logging, type hints, docstrings)
</success_criteria>

<output>
After completion, create `.planning/phases/11-memory-preparation/11-01-SUMMARY.md`
</output>
