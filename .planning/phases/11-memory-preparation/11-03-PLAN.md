---
phase: 11-memory-preparation
plan: 03
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - src/ta_lab2/tools/ai_orchestrator/memory/snapshot/run_external_dirs_snapshot.py
  - .planning/phases/11-memory-preparation/snapshots/external_dirs_snapshot.json
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Data_Tools directory indexed in memory with pre_integration_v0.5.0 tag"
    - "ProjectTT directory indexed in memory with pre_integration_v0.5.0 tag"
    - "fredtools2 directory indexed in memory with pre_integration_v0.5.0 tag"
    - "fedtools2 directory indexed in memory with pre_integration_v0.5.0 tag"
    - "Memory queries can find files by directory name"
  artifacts:
    - path: "src/ta_lab2/tools/ai_orchestrator/memory/snapshot/run_external_dirs_snapshot.py"
      provides: "Executable script for external directory snapshots"
      exports: ["run_external_dirs_snapshot", "main"]
    - path: ".planning/phases/11-memory-preparation/snapshots/external_dirs_snapshot.json"
      provides: "Snapshot metadata for all external directories"
      contains: "directory_stats"
  key_links:
    - from: "run_external_dirs_snapshot.py"
      to: "extract_codebase.py"
      via: "extract_directory_tree call"
      pattern: "extract_directory_tree"
    - from: "run_external_dirs_snapshot.py"
      to: "batch_indexer.py"
      via: "batch_add_memories call"
      pattern: "batch_add_memories"
---

<objective>
Index external directories (Data_Tools, ProjectTT, fredtools2, fedtools2) into memory system with pre_integration_v0.5.0 tag.

Purpose: Requirement MEMO-12 requires capturing current state of all 4 external directories before integration. This creates baseline snapshots that enable tracking what gets integrated vs archived during v0.5.0.

Output: All external Python files indexed in Mem0 with structured metadata, plus consolidated snapshot manifest.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-memory-preparation/11-CONTEXT.md
@.planning/phases/11-memory-preparation/11-RESEARCH.md
@.planning/phases/11-memory-preparation/11-01-SUMMARY.md

# Snapshot infrastructure from Plan 01
@src/ta_lab2/tools/ai_orchestrator/memory/snapshot/extract_codebase.py
@src/ta_lab2/tools/ai_orchestrator/memory/snapshot/batch_indexer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create external directories snapshot script</name>
  <files>
    src/ta_lab2/tools/ai_orchestrator/memory/snapshot/run_external_dirs_snapshot.py
  </files>
  <action>
Create run_external_dirs_snapshot.py with:

1. Define external directory configuration (all 4 directories per CONTEXT.md and MEMO-12):
   ```python
   EXTERNAL_DIRS = [
       {
           "name": "Data_Tools",
           "path": Path("C:/Users/asafi/Downloads/Data_Tools"),
           "description": "Data processing and utility scripts"
       },
       {
           "name": "ProjectTT",
           "path": Path("C:/Users/asafi/Documents/ProjectTT"),
           "description": "Project documentation, schemas, and planning materials"
       },
       {
           "name": "fredtools2",
           "path": Path("C:/Users/asafi/Downloads/fredtools2"),
           "description": "FRED economic data tools"
       },
       {
           "name": "fedtools2",
           "path": Path("C:/Users/asafi/Downloads/fedtools2"),
           "description": "Federal Reserve data tools"
       }
   ]

   EXCLUSIONS = [
       "__pycache__", ".pyc", ".venv", "venv", "env",
       ".git", "dist", "build", "*.egg-info",
       ".csv", ".xlsx", ".json"
   ]
   ```

2. `validate_directories() -> list[dict]`:
   - Check each directory in EXTERNAL_DIRS exists
   - Return list of valid directories (log warnings for missing)
   - Fail gracefully if directory doesn't exist (skip it, continue with others)

3. `run_external_dir_snapshot(dir_config: dict, dry_run: bool = False) -> dict`:
   - Similar to run_ta_lab2_snapshot but for external directory
   - Get git metadata if .git exists in directory, otherwise use file mtime
   - For ProjectTT: Also handle .docx and .xlsx files per CONTEXT.md (extract text via pypandoc if available, else skip with warning)
   - For each file_info, create memory dict:
     - content: format_file_content_for_memory(file_info)
     - metadata: create_snapshot_metadata with:
       - source="pre_integration_v0.5.0"
       - directory=dir_config["name"]
       - file_type="source_code" (or "documentation" for ProjectTT docs)
       - file_path=relative path from dir root
   - Return stats dict per directory

4. `run_all_external_snapshots(dry_run: bool = False) -> dict`:
   - Validate directories first
   - Loop through each valid directory, call run_external_dir_snapshot
   - Aggregate stats into combined result:
     ```python
     {
         "directories": {
             "Data_Tools": {...stats...},
             "ProjectTT": {...stats...},
             "fredtools2": {...stats...},
             "fedtools2": {...stats...}
         },
         "totals": {
             "total_files": X,
             "total_functions": Y,
             "memories_added": Z
         },
         "missing_directories": []  # List any that didn't exist
     }
     ```
   - Return combined stats

5. `save_external_snapshot_manifest(stats: dict, output_path: Path)`:
   - Save JSON with all directory stats and totals
   - Include timestamp, list of missing directories (if any)

6. `main()` with CLI, --dry-run flag, logging config
  </action>
  <verify>
Run dry-run: `python src/ta_lab2/tools/ai_orchestrator/memory/snapshot/run_external_dirs_snapshot.py --dry-run`
Expected: Shows file counts for all 4 directories (Data_Tools, ProjectTT, fredtools2, fedtools2).
  </verify>
  <done>
run_external_dirs_snapshot.py exists with all 4 directories configured. Dry-run shows discovery working for available directories.
  </done>
</task>

<task type="auto">
  <name>Task 2: Execute external directories snapshot and validate</name>
  <files>
    .planning/phases/11-memory-preparation/snapshots/external_dirs_snapshot.json
  </files>
  <action>
Execute the snapshot process:

1. Run the snapshot script (not dry-run):
   ```bash
   python src/ta_lab2/tools/ai_orchestrator/memory/snapshot/run_external_dirs_snapshot.py
   ```

2. Verify memories were added by querying each directory:
   ```python
   from ta_lab2.tools.ai_orchestrator.memory.mem0_client import get_mem0_client
   client = get_mem0_client()

   for dir_name in ["Data_Tools", "ProjectTT", "fredtools2", "fedtools2"]:
       results = client.search(
           query=f"Files in {dir_name} directory",
           user_id="orchestrator",
           limit=10
       )
       print(f"{dir_name}: {len(results)} memories found")
   ```

3. Verify pre_integration tag filtering:
   ```python
   results = client.search(
       query="pre_integration snapshot external directories",
       user_id="orchestrator",
       limit=20
   )
   print(f"Total pre_integration tagged: {len(results)}")
   ```

4. Verify snapshot manifest was created at:
   .planning/phases/11-memory-preparation/snapshots/external_dirs_snapshot.json

5. Document results:
   - Files indexed per directory (all 4: Data_Tools, ProjectTT, fredtools2, fedtools2)
   - Total functions discovered per directory
   - Any directories that were missing/skipped
   - Any errors encountered
  </action>
  <verify>
Check manifest exists: `cat .planning/phases/11-memory-preparation/snapshots/external_dirs_snapshot.json`
Query test for each directory:
- `python -c "from ta_lab2.tools.ai_orchestrator.memory.mem0_client import get_mem0_client; c = get_mem0_client(); r = c.search('Data_Tools files', 'orchestrator', limit=5); print(f'Data_Tools: {len(r)} results')"`
- `python -c "from ta_lab2.tools.ai_orchestrator.memory.mem0_client import get_mem0_client; c = get_mem0_client(); r = c.search('ProjectTT files', 'orchestrator', limit=5); print(f'ProjectTT: {len(r)} results')"`
- `python -c "from ta_lab2.tools.ai_orchestrator.memory.mem0_client import get_mem0_client; c = get_mem0_client(); r = c.search('fredtools2 files', 'orchestrator', limit=5); print(f'fredtools2: {len(r)} results')"`
- `python -c "from ta_lab2.tools.ai_orchestrator.memory.mem0_client import get_mem0_client; c = get_mem0_client(); r = c.search('fedtools2 files', 'orchestrator', limit=5); print(f'fedtools2: {len(r)} results')"`
  </verify>
  <done>
external_dirs_snapshot.json exists with stats for all 4 directories. Memory queries return results for Data_Tools, ProjectTT, fredtools2, fedtools2 with pre_integration_v0.5.0 tags.
  </done>
</task>

</tasks>

<verification>
1. Snapshot manifest exists: `.planning/phases/11-memory-preparation/snapshots/external_dirs_snapshot.json`
2. Search query "Files in Data_Tools" returns results
3. Search query "Files in ProjectTT" returns results
4. Search query "Files in fredtools2" returns results
5. Search query "Files in fedtools2" returns results
6. All results have pre_integration_v0.5.0 source metadata
</verification>

<success_criteria>
- run_external_dirs_snapshot.py exists with all 4 directories configured
- external_dirs_snapshot.json contains file inventory for all accessible directories
- Data_Tools, ProjectTT, fredtools2, fedtools2 indexed in memory with pre_integration_v0.5.0 tag
- Memory queries can find files by directory name
- Script logs which directories were processed vs skipped
</success_criteria>

<output>
After completion, create `.planning/phases/11-memory-preparation/11-03-SUMMARY.md`
</output>
