---
phase: 17-verification-validation
plan: 05
type: execute
wave: 3
depends_on: ["17-01"]
files_modified:
  - tests/validation/test_data_loss.py
autonomous: true

must_haves:
  truths:
    - "No files lost during v0.5.0 reorganization (verified via checksums)"
    - "File count equation holds: baseline_count = current_count + archived_count"
    - "Memory can answer 'where did file X go?' for moved files"
  artifacts:
    - path: "tests/validation/test_data_loss.py"
      provides: "Data loss validation tests"
      exports: ["test_no_files_lost_from_baseline", "test_file_count_accounting", "test_memory_tracks_file_moves"]
  key_links:
    - from: "tests/validation/test_data_loss.py"
      to: "ta_lab2.tools.archive.validate"
      via: "Import validation functions"
      pattern: "from ta_lab2\\.tools\\.archive"
    - from: "tests/validation/test_data_loss.py"
      to: ".planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json"
      via: "Baseline loading"
      pattern: "pre_reorg_snapshot"
---

<objective>
Create data loss validation tests using Phase 12 baseline

Purpose: Verify no files were lost during v0.5.0 reorganization (Phases 11-16)
Output: tests/validation/test_data_loss.py with checksum and memory verification
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-verification-validation/17-CONTEXT.md
@.planning/phases/17-verification-validation/17-RESEARCH.md

# Phase 12 validation tooling (provides create_snapshot, validate_no_data_loss)
@.planning/phases/12-archive-foundation/12-03-SUMMARY.md
@src/ta_lab2/tools/archive/validate.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create data loss validation test suite</name>
  <files>tests/validation/test_data_loss.py</files>
  <action>
Create tests/validation/test_data_loss.py with comprehensive data loss validation.

Include these tests:

1. **test_no_files_lost_from_baseline():**
   - Load Phase 12 baseline snapshot from .planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json
   - Create current snapshot of src/ta_lab2 and tests (not .venv)
   - Use validate_no_data_loss() to compare checksums
   - Files can be moved (checksum in both), but not deleted
   - Skip if baseline not found

2. **test_file_count_accounting():**
   - Count files in src/ta_lab2 (current)
   - Count files in .archive (archived)
   - Count files in tests (current)
   - Verify: baseline_src_tests <= current + archived
   - This catches accidental deletions even if checksums change

3. **test_memory_tracks_file_moves():**
   - Marked with @pytest.mark.orchestrator (requires chromadb/mem0)
   - Query memory for known moved files (from Phase 14 migrations)
   - Verify memory returns moved_to relationships
   - Sample files to check:
     - Data_Tools scripts moved to ta_lab2/tools/data_tools/
     - ProjectTT docs moved to .archive/documentation/
   - Skip if chromadb not installed

```python
"""Validate no data loss during v0.5.0 reorganization.

Uses Phase 12 baseline (9,620 files with SHA256 checksums) to verify
all files still exist, potentially at new locations after moves.
"""
from pathlib import Path
import pytest

# Import Phase 12 validation tooling
from ta_lab2.tools.archive.validate import (
    create_snapshot,
    load_snapshot,
    validate_no_data_loss,
)


BASELINE_PATH = Path(".planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json")


def test_no_files_lost_from_baseline():
    """Validate all src/tests files from Phase 12 baseline still exist.

    Files can move (checksum same, path different), but cannot be deleted.
    Uses checksum-based validation that tracks files through moves.
    """
    if not BASELINE_PATH.exists():
        pytest.skip(f"Baseline not found: {BASELINE_PATH}")

    # Load Phase 12 baseline
    baseline = load_snapshot(BASELINE_PATH)

    # Create current snapshot of src + tests + .archive
    # (We check all locations where files might have moved)
    current_src = create_snapshot(Path("src"), pattern="**/*.py", compute_checksums=True)
    current_tests = create_snapshot(Path("tests"), pattern="**/*.py", compute_checksums=True)

    # Check .archive if it exists
    archive_path = Path(".archive")
    current_archive = None
    if archive_path.exists():
        current_archive = create_snapshot(archive_path, pattern="**/*.py", compute_checksums=True)

    # Combine all current checksums
    all_current_checksums = set()
    all_current_checksums.update(current_src.file_checksums.values())
    all_current_checksums.update(current_tests.file_checksums.values())
    if current_archive:
        all_current_checksums.update(current_archive.file_checksums.values())

    # Get baseline checksums for src and tests only (not .venv)
    baseline_src_tests = {
        path: checksum
        for path, checksum in baseline.file_checksums.items()
        if path.startswith("src/") or path.startswith("tests/")
    }

    # Find missing checksums (files deleted, not moved)
    missing = []
    for path, checksum in baseline_src_tests.items():
        if checksum not in all_current_checksums:
            missing.append(f"{path} (checksum: {checksum[:16]}...)")

    if missing:
        pytest.fail(
            f"Data loss detected - {len(missing)} files from baseline not found:\n"
            + "\n".join(f"  - {m}" for m in missing[:20])
            + (f"\n  ... and {len(missing) - 20} more" if len(missing) > 20 else "")
        )


def test_file_count_accounting():
    """Verify file count equation: baseline_count <= current + archived.

    This catches scenarios where both file and checksum were lost.
    Uses inequality because new files may have been added.
    """
    if not BASELINE_PATH.exists():
        pytest.skip(f"Baseline not found: {BASELINE_PATH}")

    baseline = load_snapshot(BASELINE_PATH)

    # Count baseline src + tests files (not .venv)
    baseline_src_tests_count = sum(
        1 for path in baseline.file_checksums.keys()
        if path.startswith("src/") or path.startswith("tests/")
    )

    # Count current files
    current_src = create_snapshot(Path("src"), pattern="**/*.py", compute_checksums=False)
    current_tests = create_snapshot(Path("tests"), pattern="**/*.py", compute_checksums=False)

    archive_count = 0
    archive_path = Path(".archive")
    if archive_path.exists():
        current_archive = create_snapshot(archive_path, pattern="**/*.py", compute_checksums=False)
        archive_count = current_archive.total_files

    current_total = current_src.total_files + current_tests.total_files + archive_count

    # We allow new files (>=), but baseline count should not exceed current
    # If baseline > current, files were deleted
    if baseline_src_tests_count > current_total:
        pytest.fail(
            f"File count mismatch - potential data loss:\n"
            f"  Baseline (src+tests): {baseline_src_tests_count}\n"
            f"  Current src: {current_src.total_files}\n"
            f"  Current tests: {current_tests.total_files}\n"
            f"  Archived: {archive_count}\n"
            f"  Current total: {current_total}\n"
            f"  Missing: {baseline_src_tests_count - current_total} files"
        )


@pytest.mark.orchestrator
def test_memory_tracks_file_moves():
    """Verify memory can answer 'where did file X go?' for moved files.

    Queries memory for moved_to relationships created during Phases 13-16.
    """
    pytest.importorskip("chromadb", reason="chromadb required for memory queries")
    pytest.importorskip("mem0ai", reason="mem0ai required for memory queries")

    try:
        from ta_lab2.tools.ai_orchestrator.memory import get_client
    except ImportError:
        pytest.skip("Memory client not available")

    # Sample of files known to be moved in Phases 13-16
    # These should have moved_to relationships in memory
    moved_files = [
        # From Phase 14 (Data_Tools migration)
        ("generate_function_map.py", "ta_lab2.tools.data_tools.analysis"),
        ("tree_structure.py", "ta_lab2.tools.data_tools.analysis"),
    ]

    try:
        client = get_client()
    except Exception as e:
        pytest.skip(f"Could not initialize memory client: {e}")

    missing_tracking = []
    for old_name, expected_location in moved_files:
        # Query memory for this file's new location
        results = client.search(
            f"where is {old_name} now? moved_to relationship",
            user_id="ta_lab2",
            limit=5
        )

        # Check if any result mentions the expected location
        found = False
        for result in results:
            if expected_location in str(result):
                found = True
                break

        if not found:
            missing_tracking.append(f"{old_name} -> expected in {expected_location}")

    if missing_tracking:
        pytest.fail(
            f"Memory missing moved_to relationships for {len(missing_tracking)} files:\n"
            + "\n".join(f"  - {m}" for m in missing_tracking)
        )
```
  </action>
  <verify>pytest tests/validation/test_data_loss.py --collect-only shows 3 tests</verify>
  <done>tests/validation/test_data_loss.py exists with checksum and memory validation tests</done>
</task>

<task type="auto">
  <name>Task 2: Run data loss validation</name>
  <files></files>
  <action>
Run the data loss validation tests to verify v0.5.0 reorganization integrity.

Commands:
1. Run non-orchestrator tests (checksum validation):
   pytest tests/validation/test_data_loss.py -m "not orchestrator" -v

2. If orchestrator deps installed, run memory tracking test:
   pytest tests/validation/test_data_loss.py -v

Expected outcomes:
- test_no_files_lost_from_baseline: PASS (all checksums found)
- test_file_count_accounting: PASS (counts match within tolerance)
- test_memory_tracks_file_moves: PASS or SKIP (depending on deps)

If failures:
- Document which files are missing
- Check .archive/ for accidentally deleted files
- Check git history with `git log --all -- <filename>`
  </action>
  <verify>pytest tests/validation/test_data_loss.py -m "not orchestrator" -v passes or documents issues</verify>
  <done>Data loss validation complete with results documented</done>
</task>

</tasks>

<verification>
1. tests/validation/test_data_loss.py exists
2. Test uses Phase 12 baseline (9,620 files)
3. Checksum validation covers src + tests + .archive
4. Memory tracking test uses orchestrator marker
5. Tests pass or clearly document missing files
</verification>

<success_criteria>
- No files lost from baseline (checksum-verified)
- File count equation holds: baseline <= current + archived
- Memory tracks moved_to relationships (if orchestrator deps available)
- Zero data loss from v0.5.0 reorganization
</success_criteria>

<output>
After completion, create `.planning/phases/17-verification-validation/17-05-SUMMARY.md`
</output>
