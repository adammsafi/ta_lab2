---
phase: 02-memory-core-chromadb-integration
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ta_lab2/tools/ai_orchestrator/memory/query.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Query embeddings are 1536-dim using text-embedding-3-small model"
    - "No ChromaDB dimension mismatch errors on search_memories()"
    - "inject_memory_context() returns formatted memories transitively"
  artifacts:
    - path: "src/ta_lab2/tools/ai_orchestrator/memory/query.py"
      provides: "Semantic search with correct 1536-dim embeddings"
      contains: "get_embedding"
  key_links:
    - from: "src/ta_lab2/tools/ai_orchestrator/memory/query.py"
      to: "src/ta_lab2/tools/ai_orchestrator/memory/update.py"
      via: "lazy import inside search_memories function"
      pattern: "from \\.update import get_embedding"
---

<objective>
Fix semantic search embedding dimension mismatch causing query failures.

Purpose: ChromaDB collection has 1536-dim embeddings (text-embedding-3-small) but search_memories() uses query_texts which triggers ChromaDB's default 384-dim embedder, causing dimension mismatch crash.

Output: Working semantic search that generates 1536-dim query embeddings using the same model as stored memories.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Gap source
@.planning/phases/02-memory-core-chromadb-integration/02-VERIFICATION.md

# Files to modify
@src/ta_lab2/tools/ai_orchestrator/memory/query.py
@src/ta_lab2/tools/ai_orchestrator/memory/update.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix search_memories() to use 1536-dim query embeddings</name>
  <files>src/ta_lab2/tools/ai_orchestrator/memory/query.py</files>
  <action>
Modify search_memories() function to generate query embeddings using the same model as stored memories:

1. Add lazy import inside search_memories() function body (after getting client, around line 82):
   ```python
   from .update import get_embedding
   ```

2. Replace lines 94-99 (the collection.query call) with:
   ```python
   # Generate query embedding using same model as collection (1536-dim)
   query_embedding = get_embedding([query])[0]

   raw_results = collection.query(
       query_embeddings=[query_embedding],  # NOT query_texts
       n_results=max_results,
       where=where_filter,
       include=["documents", "metadatas", "distances"]
   )
   ```

This ensures the query embedding has 1536 dimensions (text-embedding-3-small) matching the stored memories, instead of ChromaDB's default 384-dim embedder.
  </action>
  <verify>
1. Verify embedding dimension is 1536:
```bash
python -c "
from ta_lab2.tools.ai_orchestrator.memory.update import get_embedding
emb = get_embedding(['test query'])[0]
print(f'Query embedding dimensions: {len(emb)}')
assert len(emb) == 1536, f'Expected 1536, got {len(emb)}'
print('Dimension check PASSED')
"
```

2. Verify search works without dimension mismatch:
```bash
python -c "
from ta_lab2.tools.ai_orchestrator.memory.query import search_memories
from ta_lab2.tools.ai_orchestrator.memory.client import get_memory_client
client = get_memory_client()
print(f'Collection count: {client.collection.count()}')
results = search_memories('EMA calculation', max_results=3)
print(f'Search returned {results.filtered_count} results')
for r in results.results:
    print(f'  - {r.similarity:.2f}: {r.content[:60]}...')
"
```
Should return results without dimension mismatch error.

3. Verify inject_memory_context() works transitively:
```bash
python -c "
from ta_lab2.tools.ai_orchestrator.memory.injection import inject_memory_context
context = inject_memory_context('How do I calculate multi-timeframe EMAs?', max_memories=3)
print('Context injection result:')
print(context[:500])
print(f'...[{len(context)} chars total]')
"
```
Should return formatted memory context without errors.
  </verify>
  <done>
- search_memories() uses get_embedding() producing 1536-dim vectors
- No dimension mismatch errors occur
- inject_memory_context() returns formatted memories
- Full RAG pipeline operational: query -> embed (1536-dim) -> search -> format
  </done>
</task>

</tasks>

<verification>
1. Query embedding dimension is exactly 1536 (text-embedding-3-small)
2. Dimension mismatch error no longer occurs
3. search_memories() returns relevant results (similarity > 0.7)
4. inject_memory_context() returns formatted context string
5. Both success criteria from VERIFICATION.md gaps are satisfied
</verification>

<success_criteria>
- get_embedding(['test'])[0] returns exactly 1536 dimensions
- search_memories("EMA calculation") returns results without error
- inject_memory_context() returns formatted markdown context
- No 384-dim vs 1536-dim dimension mismatch errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-memory-core-chromadb-integration/02-05-SUMMARY.md`
</output>
