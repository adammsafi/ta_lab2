---
phase: 02-memory-core-chromadb-integration
plan: 03
type: execute
wave: 3
depends_on: ["02-01", "02-02"]
files_modified:
  - src/ta_lab2/tools/ai_orchestrator/memory/update.py
  - src/ta_lab2/tools/ai_orchestrator/memory/api.py
  - src/ta_lab2/tools/ai_orchestrator/memory/__init__.py
  - src/ta_lab2/tools/ai_orchestrator/__init__.py
  - tests/orchestrator/test_memory_update.py
  - tests/orchestrator/test_memory_api.py
autonomous: true

# Note: This plan has 4 tasks with 6 files - borderline context budget.
# If context issues arise during execution, consider splitting into:
# - 02-03: update.py + test_memory_update.py (update pipeline)
# - 02-04: api.py + test_memory_api.py + __init__.py updates (API layer)

must_haves:
  truths:
    - "New memories can be added without corrupting existing embeddings"
    - "Memory API exposes search endpoint via HTTP"
    - "Claude/ChatGPT/Gemini can access memories through REST API"
    - "Adding duplicate memory IDs updates content without errors"
  artifacts:
    - path: "src/ta_lab2/tools/ai_orchestrator/memory/update.py"
      provides: "Incremental memory update pipeline"
      exports: ["add_memories", "add_memory", "MemoryUpdateResult"]
    - path: "src/ta_lab2/tools/ai_orchestrator/memory/api.py"
      provides: "FastAPI REST endpoint for cross-platform access"
      exports: ["create_memory_api", "MemorySearchRequest", "MemorySearchResponse"]
    - path: "tests/orchestrator/test_memory_update.py"
      provides: "Update pipeline tests"
      min_lines: 60
    - path: "tests/orchestrator/test_memory_api.py"
      provides: "API endpoint tests"
      min_lines: 40
  key_links:
    - from: "src/ta_lab2/tools/ai_orchestrator/memory/update.py"
      to: "OpenAI embeddings API"
      via: "openai.embeddings.create"
      pattern: "client\\.embeddings\\.create"
    - from: "src/ta_lab2/tools/ai_orchestrator/memory/api.py"
      to: "memory/query.py"
      via: "search_memories import"
      pattern: "from.*query.*import.*search_memories"
---

<objective>
Incremental update pipeline and cross-platform REST API (MEMO-04, MEMO-07)

Purpose: Complete the memory integration by enabling incremental updates (add new memories without breaking existing) and exposing a REST API for cross-platform access. Claude/ChatGPT/Gemini are cloud services that cannot access local filesystem - they need HTTP endpoints.

Output: add_memories() function with embedding generation, FastAPI endpoints for search/stats, comprehensive tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-memory-core-chromadb-integration/02-RESEARCH.md

# Depends on client and search from Wave 1 and Wave 2 plans
@.planning/phases/02-memory-core-chromadb-integration/02-01-SUMMARY.md
@.planning/phases/02-memory-core-chromadb-integration/02-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create incremental update module</name>
  <files>src/ta_lab2/tools/ai_orchestrator/memory/update.py</files>
  <action>
Create memory/update.py with incremental update functions:

```python
"""Incremental memory update pipeline.

Adds new memories without breaking existing embeddings.
Implements MEMO-07: Incremental update pipeline.
"""
import logging
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)

# Embedding model constants (from existing 3,763 memories)
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSIONS = 1536


@dataclass
class MemoryInput:
    """Input for adding a new memory."""

    memory_id: str
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class MemoryUpdateResult:
    """Result of memory update operation."""

    added: int = 0
    updated: int = 0
    failed: int = 0
    errors: List[str] = field(default_factory=list)
    duration_ms: float = 0.0

    @property
    def total_processed(self) -> int:
        return self.added + self.updated + self.failed

    def __str__(self) -> str:
        return (
            f"MemoryUpdate: added={self.added}, updated={self.updated}, "
            f"failed={self.failed}, duration={self.duration_ms:.1f}ms"
        )


def get_embedding(
    texts: List[str],
    model: str = EMBEDDING_MODEL
) -> List[List[float]]:
    """Generate embeddings using OpenAI API.

    Args:
        texts: List of texts to embed
        model: OpenAI embedding model name

    Returns:
        List of embedding vectors (1536 dimensions each)

    Raises:
        ValueError: If OpenAI API key not configured
        Exception: If API call fails
    """
    try:
        from openai import OpenAI
    except ImportError:
        raise ImportError("openai package required. Install with: pip install openai")

    import os
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        from ta_lab2.tools.ai_orchestrator.config import load_config
        config = load_config()
        api_key = config.openai_api_key

    if not api_key:
        raise ValueError("OPENAI_API_KEY not configured")

    client = OpenAI(api_key=api_key)

    # Clean texts (replace newlines with spaces)
    cleaned_texts = [text.replace("\n", " ").strip() for text in texts]

    response = client.embeddings.create(
        input=cleaned_texts,
        model=model
    )

    embeddings = [item.embedding for item in response.data]

    # Validate dimensions
    for i, emb in enumerate(embeddings):
        if len(emb) != EMBEDDING_DIMENSIONS:
            raise ValueError(
                f"Embedding {i} has {len(emb)} dimensions, expected {EMBEDDING_DIMENSIONS}"
            )

    return embeddings


def add_memory(
    memory_id: str,
    content: str,
    metadata: Optional[Dict[str, Any]] = None,
    client=None
) -> MemoryUpdateResult:
    """Add a single memory to the store.

    Uses upsert to handle duplicates gracefully.

    Args:
        memory_id: Unique identifier for the memory
        content: Text content of the memory
        metadata: Optional metadata dict
        client: Optional MemoryClient instance

    Returns:
        MemoryUpdateResult with operation details
    """
    return add_memories(
        memories=[MemoryInput(
            memory_id=memory_id,
            content=content,
            metadata=metadata or {}
        )],
        client=client
    )


def add_memories(
    memories: List[MemoryInput],
    batch_size: int = 50,
    client=None
) -> MemoryUpdateResult:
    """Add multiple memories with batch embedding generation.

    Uses ChromaDB upsert for atomic add-or-update operations.
    Validates embedding dimensions before insertion.

    Args:
        memories: List of MemoryInput objects
        batch_size: Batch size for embedding generation (default 50)
        client: Optional MemoryClient instance

    Returns:
        MemoryUpdateResult with stats and any errors
    """
    import time
    start_time = time.perf_counter()

    if client is None:
        from .client import get_memory_client
        client = get_memory_client()

    result = MemoryUpdateResult()
    collection = client.collection

    # Process in batches
    for batch_start in range(0, len(memories), batch_size):
        batch = memories[batch_start:batch_start + batch_size]
        batch_num = batch_start // batch_size + 1

        try:
            # Prepare data
            ids = [m.memory_id for m in batch]
            documents = [m.content for m in batch]
            metadatas = [m.metadata for m in batch]

            # Generate embeddings
            try:
                embeddings = get_embedding(documents)
            except Exception as e:
                error_msg = f"Batch {batch_num}: Embedding generation failed: {e}"
                logger.error(error_msg)
                result.errors.append(error_msg)
                result.failed += len(batch)
                continue

            # Check which IDs already exist
            try:
                existing = collection.get(ids=ids, include=[])
                existing_ids = set(existing.get("ids", []))
            except Exception:
                existing_ids = set()

            # Upsert (atomic add-or-update)
            try:
                collection.upsert(
                    ids=ids,
                    embeddings=embeddings,
                    documents=documents,
                    metadatas=metadatas
                )

                # Count adds vs updates
                for memory_id in ids:
                    if memory_id in existing_ids:
                        result.updated += 1
                    else:
                        result.added += 1

                logger.info(f"Batch {batch_num}: Processed {len(batch)} memories")

            except Exception as e:
                error_msg = f"Batch {batch_num}: Upsert failed: {e}"
                logger.error(error_msg)
                result.errors.append(error_msg)
                result.failed += len(batch)

        except Exception as e:
            error_msg = f"Batch {batch_num}: Unexpected error: {e}"
            logger.error(error_msg)
            result.errors.append(error_msg)
            result.failed += len(batch)

    result.duration_ms = (time.perf_counter() - start_time) * 1000
    logger.info(f"Memory update complete: {result}")
    return result


def delete_memory(memory_id: str, client=None) -> bool:
    """Delete a memory by ID.

    Args:
        memory_id: ID of memory to delete
        client: Optional MemoryClient instance

    Returns:
        True if deleted, False if not found
    """
    if client is None:
        from .client import get_memory_client
        client = get_memory_client()

    try:
        # Check if exists
        existing = client.collection.get(ids=[memory_id], include=[])
        if not existing.get("ids"):
            return False

        client.collection.delete(ids=[memory_id])
        logger.info(f"Deleted memory: {memory_id}")
        return True
    except Exception as e:
        logger.error(f"Failed to delete memory {memory_id}: {e}")
        return False
```
  </action>
  <verify>Run `python -c "from ta_lab2.tools.ai_orchestrator.memory.update import MemoryInput, MemoryUpdateResult; print('Update module loads')"` - should load without error</verify>
  <done>Incremental update module with batch embedding, upsert, and validation</done>
</task>

<task type="auto">
  <name>Task 2: Create FastAPI REST endpoint for cross-platform access</name>
  <files>src/ta_lab2/tools/ai_orchestrator/memory/api.py</files>
  <action>
Create memory/api.py with FastAPI endpoints for cross-platform memory access:

```python
"""FastAPI REST API for cross-platform memory access.

Exposes memory search to Claude/ChatGPT/Gemini via HTTP.
Implements MEMO-04: Cross-platform memory sharing.

Run server:
    uvicorn ta_lab2.tools.ai_orchestrator.memory.api:app --port 8080
"""
import logging
from typing import Optional, List

try:
    from fastapi import FastAPI, HTTPException, Query
    from pydantic import BaseModel, Field
    FASTAPI_AVAILABLE = True
except ImportError:
    FASTAPI_AVAILABLE = False
    # Dummy classes for when FastAPI not installed
    class BaseModel:
        pass
    class Field:
        def __init__(self, *args, **kwargs):
            pass

logger = logging.getLogger(__name__)


# Request/Response Models
class MemorySearchRequest(BaseModel):
    """Request body for memory search."""
    query: str = Field(..., description="Search query text")
    max_results: int = Field(5, ge=1, le=20, description="Maximum results")
    min_similarity: float = Field(0.7, ge=0.0, le=1.0, description="Minimum similarity")
    memory_type: Optional[str] = Field(None, description="Filter by memory type")


class MemoryResult(BaseModel):
    """Individual memory result."""
    memory_id: str
    content: str
    metadata: dict
    similarity: float


class MemorySearchResponse(BaseModel):
    """Response from memory search."""
    query: str
    memories: List[MemoryResult]
    count: int
    threshold_used: float


class MemoryStatsResponse(BaseModel):
    """Response from stats endpoint."""
    total_memories: int
    collection_name: str
    distance_metric: str
    is_valid: bool


class ContextInjectionRequest(BaseModel):
    """Request for formatted context injection."""
    query: str = Field(..., description="Query for context retrieval")
    max_memories: int = Field(5, ge=1, le=10, description="Maximum memories")
    min_similarity: float = Field(0.7, ge=0.0, le=1.0, description="Minimum similarity")
    max_length: int = Field(4000, ge=100, le=10000, description="Maximum context length")


class ContextInjectionResponse(BaseModel):
    """Response with formatted context."""
    query: str
    context: str
    memory_count: int
    estimated_tokens: int


def create_memory_api() -> "FastAPI":
    """Create FastAPI application for memory API.

    Returns:
        FastAPI application instance

    Raises:
        ImportError: If FastAPI not installed
    """
    if not FASTAPI_AVAILABLE:
        raise ImportError(
            "FastAPI required for memory API. Install with: pip install fastapi uvicorn"
        )

    app = FastAPI(
        title="ta_lab2 Memory API",
        description="Semantic memory search for cross-platform AI access",
        version="1.0.0"
    )

    @app.get("/health")
    async def health_check():
        """Health check endpoint."""
        from .validation import quick_health_check
        is_healthy = quick_health_check()
        return {"status": "healthy" if is_healthy else "unhealthy"}

    @app.get("/api/v1/memory/stats", response_model=MemoryStatsResponse)
    async def get_stats():
        """Get memory store statistics."""
        try:
            from .client import get_memory_client
            from .validation import validate_memory_store

            client = get_memory_client()
            validation = validate_memory_store(client)

            return MemoryStatsResponse(
                total_memories=validation.total_count,
                collection_name=client._collection_name,
                distance_metric=validation.distance_metric,
                is_valid=validation.is_valid
            )
        except Exception as e:
            logger.error(f"Stats failed: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.post("/api/v1/memory/search", response_model=MemorySearchResponse)
    async def search_memories_endpoint(request: MemorySearchRequest):
        """Semantic search endpoint for cross-platform memory access.

        Used by Claude, ChatGPT, Gemini to retrieve relevant project memories.
        """
        try:
            from .query import search_memories

            response = search_memories(
                query=request.query,
                max_results=request.max_results,
                min_similarity=request.min_similarity,
                memory_type=request.memory_type
            )

            return MemorySearchResponse(
                query=request.query,
                memories=[
                    MemoryResult(
                        memory_id=r.memory_id,
                        content=r.content,
                        metadata=r.metadata,
                        similarity=r.similarity
                    )
                    for r in response.results
                ],
                count=response.filtered_count,
                threshold_used=response.threshold_used
            )
        except Exception as e:
            logger.error(f"Search failed: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.post("/api/v1/memory/context", response_model=ContextInjectionResponse)
    async def get_context(request: ContextInjectionRequest):
        """Get formatted context for AI prompt injection.

        Returns memories formatted and ready to inject into prompts.
        """
        try:
            from .injection import inject_memory_context, estimate_context_tokens
            from .query import search_memories

            # Get search results for count
            search_response = search_memories(
                query=request.query,
                max_results=request.max_memories,
                min_similarity=request.min_similarity
            )

            # Get formatted context
            context = inject_memory_context(
                query=request.query,
                max_memories=request.max_memories,
                min_similarity=request.min_similarity,
                max_length=request.max_length
            )

            return ContextInjectionResponse(
                query=request.query,
                context=context,
                memory_count=search_response.filtered_count,
                estimated_tokens=estimate_context_tokens(context)
            )
        except Exception as e:
            logger.error(f"Context injection failed: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/api/v1/memory/types")
    async def get_memory_types():
        """Get list of available memory types."""
        try:
            from .query import get_memory_types
            types = get_memory_types()
            return {"types": types, "count": len(types)}
        except Exception as e:
            logger.error(f"Get types failed: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    return app


# Create app instance for uvicorn
if FASTAPI_AVAILABLE:
    app = create_memory_api()
else:
    app = None
```
  </action>
  <verify>Run `python -c "from ta_lab2.tools.ai_orchestrator.memory.api import FASTAPI_AVAILABLE; print(f'FastAPI available: {FASTAPI_AVAILABLE}')"` - should print availability status</verify>
  <done>FastAPI endpoints created for search, context injection, and stats</done>
</task>

<task type="auto">
  <name>Task 3: Update memory module exports and main orchestrator exports</name>
  <files>src/ta_lab2/tools/ai_orchestrator/memory/__init__.py, src/ta_lab2/tools/ai_orchestrator/__init__.py</files>
  <action>
Update memory/__init__.py to export update and API modules:

```python
"""Memory integration for AI orchestrator.

Provides ChromaDB client, semantic search, context injection,
incremental updates, and REST API for cross-platform access.

Quick start:
    from ta_lab2.tools.ai_orchestrator.memory import (
        search_memories,
        inject_memory_context,
        validate_memory_store
    )

    # Search memories
    results = search_memories("EMA calculation")

    # Get formatted context for AI prompt
    context = inject_memory_context("How do I backtest?")

Run API server:
    uvicorn ta_lab2.tools.ai_orchestrator.memory.api:app --port 8080
"""
from .client import MemoryClient, get_memory_client, reset_memory_client
from .validation import (
    MemoryValidationResult,
    validate_memory_store,
    quick_health_check
)
from .query import (
    SearchResult,
    SearchResponse,
    search_memories,
    get_memory_by_id,
    get_memory_types
)
from .injection import (
    format_memories_for_prompt,
    inject_memory_context,
    build_augmented_prompt,
    estimate_context_tokens
)
from .update import (
    MemoryInput,
    MemoryUpdateResult,
    add_memory,
    add_memories,
    delete_memory,
    get_embedding,
    EMBEDDING_MODEL,
    EMBEDDING_DIMENSIONS
)

# Conditional API imports (FastAPI optional)
try:
    from .api import (
        create_memory_api,
        MemorySearchRequest,
        MemorySearchResponse,
        MemoryStatsResponse,
        ContextInjectionRequest,
        ContextInjectionResponse,
        FASTAPI_AVAILABLE
    )
except ImportError:
    FASTAPI_AVAILABLE = False
    create_memory_api = None
    MemorySearchRequest = None
    MemorySearchResponse = None
    MemoryStatsResponse = None
    ContextInjectionRequest = None
    ContextInjectionResponse = None

__all__ = [
    # Client
    "MemoryClient",
    "get_memory_client",
    "reset_memory_client",
    # Validation
    "MemoryValidationResult",
    "validate_memory_store",
    "quick_health_check",
    # Query
    "SearchResult",
    "SearchResponse",
    "search_memories",
    "get_memory_by_id",
    "get_memory_types",
    # Injection
    "format_memories_for_prompt",
    "inject_memory_context",
    "build_augmented_prompt",
    "estimate_context_tokens",
    # Update
    "MemoryInput",
    "MemoryUpdateResult",
    "add_memory",
    "add_memories",
    "delete_memory",
    "get_embedding",
    "EMBEDDING_MODEL",
    "EMBEDDING_DIMENSIONS",
    # API (optional)
    "create_memory_api",
    "MemorySearchRequest",
    "MemorySearchResponse",
    "MemoryStatsResponse",
    "ContextInjectionRequest",
    "ContextInjectionResponse",
    "FASTAPI_AVAILABLE",
]
```

Update main orchestrator __init__.py to expose memory module:

Add to existing __init__.py imports:
```python
# Add after existing imports
from . import memory  # Expose memory submodule
```

Add to __all__ list:
```python
"memory",  # Memory submodule
```
  </action>
  <verify>Run `python -c "from ta_lab2.tools.ai_orchestrator import memory; print(dir(memory))"` - should list all exports</verify>
  <done>Memory module fully exported from orchestrator package</done>
</task>

<task type="auto">
  <name>Task 4: Create update and API tests</name>
  <files>tests/orchestrator/test_memory_update.py, tests/orchestrator/test_memory_api.py</files>
  <action>
Create tests/orchestrator/test_memory_update.py:

```python
"""Tests for memory update module."""
import pytest
from unittest.mock import MagicMock, patch
from ta_lab2.tools.ai_orchestrator.memory import (
    MemoryInput,
    MemoryUpdateResult,
    add_memory,
    add_memories,
    delete_memory,
    get_embedding,
    EMBEDDING_MODEL,
    EMBEDDING_DIMENSIONS,
    reset_memory_client
)


class TestMemoryUpdateResult:
    """Tests for MemoryUpdateResult dataclass."""

    def test_result_str(self):
        """Test string representation."""
        result = MemoryUpdateResult(added=5, updated=2, failed=1, duration_ms=100.5)
        output = str(result)
        assert "added=5" in output
        assert "updated=2" in output
        assert "failed=1" in output

    def test_total_processed(self):
        """Test total_processed property."""
        result = MemoryUpdateResult(added=5, updated=2, failed=1)
        assert result.total_processed == 8


class TestGetEmbedding:
    """Tests for embedding generation."""

    @patch('ta_lab2.tools.ai_orchestrator.memory.update.OpenAI')
    def test_get_embedding_calls_openai(self, mock_openai_class):
        """Test embedding generation calls OpenAI API."""
        mock_client = MagicMock()
        mock_response = MagicMock()
        mock_response.data = [MagicMock(embedding=[0.1] * EMBEDDING_DIMENSIONS)]
        mock_client.embeddings.create.return_value = mock_response
        mock_openai_class.return_value = mock_client

        with patch.dict('os.environ', {'OPENAI_API_KEY': 'test-key'}):
            embeddings = get_embedding(["test text"])

        assert len(embeddings) == 1
        assert len(embeddings[0]) == EMBEDDING_DIMENSIONS
        mock_client.embeddings.create.assert_called_once()

    @patch('ta_lab2.tools.ai_orchestrator.memory.update.OpenAI')
    def test_get_embedding_validates_dimensions(self, mock_openai_class):
        """Test embedding validation rejects wrong dimensions."""
        mock_client = MagicMock()
        mock_response = MagicMock()
        mock_response.data = [MagicMock(embedding=[0.1] * 768)]  # Wrong size
        mock_client.embeddings.create.return_value = mock_response
        mock_openai_class.return_value = mock_client

        with patch.dict('os.environ', {'OPENAI_API_KEY': 'test-key'}):
            with pytest.raises(ValueError, match="dimensions"):
                get_embedding(["test text"])


class TestAddMemories:
    """Tests for add_memories function."""

    def setup_method(self):
        reset_memory_client()

    def teardown_method(self):
        reset_memory_client()

    @patch('ta_lab2.tools.ai_orchestrator.memory.update.get_embedding')
    @patch('ta_lab2.tools.ai_orchestrator.memory.update.get_memory_client')
    def test_add_memories_upserts(self, mock_get_client, mock_get_embedding):
        """Test add_memories uses upsert."""
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_collection.get.return_value = {"ids": []}  # No existing
        mock_client.collection = mock_collection
        mock_get_client.return_value = mock_client

        mock_get_embedding.return_value = [[0.1] * EMBEDDING_DIMENSIONS]

        memories = [MemoryInput(
            memory_id="test1",
            content="Test content",
            metadata={"type": "test"}
        )]

        result = add_memories(memories, client=mock_client)

        assert result.added == 1
        assert result.updated == 0
        assert result.failed == 0
        mock_collection.upsert.assert_called_once()

    @patch('ta_lab2.tools.ai_orchestrator.memory.update.get_embedding')
    @patch('ta_lab2.tools.ai_orchestrator.memory.update.get_memory_client')
    def test_add_memories_counts_updates(self, mock_get_client, mock_get_embedding):
        """Test add_memories correctly counts updates vs adds."""
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_collection.get.return_value = {"ids": ["existing1"]}  # One exists
        mock_client.collection = mock_collection
        mock_get_client.return_value = mock_client

        mock_get_embedding.return_value = [[0.1] * EMBEDDING_DIMENSIONS] * 2

        memories = [
            MemoryInput(memory_id="existing1", content="Updated", metadata={}),
            MemoryInput(memory_id="new1", content="New", metadata={})
        ]

        result = add_memories(memories, client=mock_client)

        assert result.added == 1
        assert result.updated == 1

    @patch('ta_lab2.tools.ai_orchestrator.memory.update.get_embedding')
    @patch('ta_lab2.tools.ai_orchestrator.memory.update.get_memory_client')
    def test_add_memories_handles_embedding_failure(self, mock_get_client, mock_get_embedding):
        """Test add_memories handles embedding generation failure."""
        mock_client = MagicMock()
        mock_get_client.return_value = mock_client
        mock_get_embedding.side_effect = Exception("API error")

        memories = [MemoryInput(memory_id="test1", content="Test", metadata={})]

        result = add_memories(memories, client=mock_client)

        assert result.failed == 1
        assert len(result.errors) == 1
        assert "Embedding generation failed" in result.errors[0]


class TestDeleteMemory:
    """Tests for delete_memory function."""

    def setup_method(self):
        reset_memory_client()

    def teardown_method(self):
        reset_memory_client()

    @patch('ta_lab2.tools.ai_orchestrator.memory.update.get_memory_client')
    def test_delete_existing_memory(self, mock_get_client):
        """Test deleting existing memory."""
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_collection.get.return_value = {"ids": ["test1"]}
        mock_client.collection = mock_collection
        mock_get_client.return_value = mock_client

        result = delete_memory("test1", client=mock_client)

        assert result is True
        mock_collection.delete.assert_called_once_with(ids=["test1"])

    @patch('ta_lab2.tools.ai_orchestrator.memory.update.get_memory_client')
    def test_delete_nonexistent_memory(self, mock_get_client):
        """Test deleting non-existent memory returns False."""
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_collection.get.return_value = {"ids": []}  # Not found
        mock_client.collection = mock_collection
        mock_get_client.return_value = mock_client

        result = delete_memory("nonexistent", client=mock_client)

        assert result is False
        mock_collection.delete.assert_not_called()
```

Create tests/orchestrator/test_memory_api.py:

```python
"""Tests for memory REST API."""
import pytest
from unittest.mock import MagicMock, patch

# Check if FastAPI available for testing
try:
    from fastapi.testclient import TestClient
    from ta_lab2.tools.ai_orchestrator.memory.api import create_memory_api, FASTAPI_AVAILABLE
    TESTING_AVAILABLE = FASTAPI_AVAILABLE
except ImportError:
    TESTING_AVAILABLE = False
    TestClient = None


@pytest.mark.skipif(not TESTING_AVAILABLE, reason="FastAPI not installed")
class TestMemoryAPI:
    """Tests for memory REST API endpoints."""

    @pytest.fixture
    def client(self):
        """Create test client."""
        app = create_memory_api()
        return TestClient(app)

    @patch('ta_lab2.tools.ai_orchestrator.memory.api.quick_health_check')
    def test_health_endpoint(self, mock_health, client):
        """Test health check endpoint."""
        mock_health.return_value = True

        response = client.get("/health")

        assert response.status_code == 200
        assert response.json()["status"] == "healthy"

    @patch('ta_lab2.tools.ai_orchestrator.memory.api.validate_memory_store')
    @patch('ta_lab2.tools.ai_orchestrator.memory.api.get_memory_client')
    def test_stats_endpoint(self, mock_get_client, mock_validate, client):
        """Test stats endpoint."""
        mock_client = MagicMock()
        mock_client._collection_name = "test_collection"
        mock_get_client.return_value = mock_client

        mock_validation = MagicMock()
        mock_validation.total_count = 3763
        mock_validation.distance_metric = "cosine"
        mock_validation.is_valid = True
        mock_validate.return_value = mock_validation

        response = client.get("/api/v1/memory/stats")

        assert response.status_code == 200
        data = response.json()
        assert data["total_memories"] == 3763
        assert data["distance_metric"] == "cosine"

    @patch('ta_lab2.tools.ai_orchestrator.memory.api.search_memories')
    def test_search_endpoint(self, mock_search, client):
        """Test search endpoint."""
        from ta_lab2.tools.ai_orchestrator.memory.query import SearchResult, SearchResponse

        mock_search.return_value = SearchResponse(
            query="test",
            results=[SearchResult(
                memory_id="id1",
                content="Test content",
                metadata={"type": "test"},
                similarity=0.9,
                distance=0.1
            )],
            total_found=1,
            filtered_count=1,
            search_time_ms=5.0,
            threshold_used=0.7
        )

        response = client.post(
            "/api/v1/memory/search",
            json={"query": "test query", "max_results": 5, "min_similarity": 0.7}
        )

        assert response.status_code == 200
        data = response.json()
        assert data["count"] == 1
        assert data["memories"][0]["similarity"] == 0.9

    @patch('ta_lab2.tools.ai_orchestrator.memory.api.search_memories')
    @patch('ta_lab2.tools.ai_orchestrator.memory.api.inject_memory_context')
    @patch('ta_lab2.tools.ai_orchestrator.memory.api.estimate_context_tokens')
    def test_context_endpoint(self, mock_tokens, mock_inject, mock_search, client):
        """Test context injection endpoint."""
        from ta_lab2.tools.ai_orchestrator.memory.query import SearchResponse

        mock_search.return_value = SearchResponse(
            query="test",
            results=[],
            total_found=2,
            filtered_count=2,
            search_time_ms=5.0,
            threshold_used=0.7
        )
        mock_inject.return_value = "# Formatted Context"
        mock_tokens.return_value = 100

        response = client.post(
            "/api/v1/memory/context",
            json={"query": "test query"}
        )

        assert response.status_code == 200
        data = response.json()
        assert data["context"] == "# Formatted Context"
        assert data["memory_count"] == 2

    @patch('ta_lab2.tools.ai_orchestrator.memory.api.get_memory_types')
    def test_types_endpoint(self, mock_types, client):
        """Test memory types endpoint."""
        mock_types.return_value = ["insight", "code", "decision"]

        response = client.get("/api/v1/memory/types")

        assert response.status_code == 200
        data = response.json()
        assert data["count"] == 3
        assert "insight" in data["types"]
```

Note: API tests use FastAPI's TestClient which tests the HTTP interface via mocked function imports.
For full HTTP server integration testing, you would run the server manually and use curl or httpx:
```bash
# Start server (if FastAPI installed)
uvicorn ta_lab2.tools.ai_orchestrator.memory.api:app --port 8080 &

# Test endpoints
curl http://localhost:8080/health
curl -X POST http://localhost:8080/api/v1/memory/search -H "Content-Type: application/json" -d '{"query": "EMA calculation"}'
```
  </action>
  <verify>Run `pytest tests/orchestrator/test_memory_update.py tests/orchestrator/test_memory_api.py -v` - all tests pass</verify>
  <done>Comprehensive tests for update pipeline and API endpoints (unit tests via TestClient; manual HTTP testing documented)</done>
</task>

</tasks>

<verification>
1. `python -c "from ta_lab2.tools.ai_orchestrator.memory import add_memory, MemoryInput; print('Update module OK')"` - imports work
2. `python -c "from ta_lab2.tools.ai_orchestrator.memory import create_memory_api; print('API module OK')"` - imports work (or reports FastAPI not installed)
3. `pytest tests/orchestrator/test_memory_update.py tests/orchestrator/test_memory_api.py -v` - all tests pass
4. If FastAPI installed: `uvicorn ta_lab2.tools.ai_orchestrator.memory.api:app --port 8080` starts server
</verification>

<success_criteria>
- add_memories() generates embeddings and upserts (MEMO-07 satisfied)
- REST API exposes /search and /context endpoints
- Cross-platform access possible via HTTP (MEMO-04 satisfied)
- Adding duplicate memory IDs updates content without errors (upsert behavior)
- 15+ tests pass for update and API modules
</success_criteria>

<output>
After completion, create `.planning/phases/02-memory-core-chromadb-integration/02-03-SUMMARY.md`
</output>
