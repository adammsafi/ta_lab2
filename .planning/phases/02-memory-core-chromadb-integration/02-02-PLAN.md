---
phase: 02-memory-core-chromadb-integration
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/ta_lab2/tools/ai_orchestrator/memory/query.py
  - src/ta_lab2/tools/ai_orchestrator/memory/injection.py
  - src/ta_lab2/tools/ai_orchestrator/memory/__init__.py
  - tests/orchestrator/test_memory_search.py
autonomous: true

must_haves:
  truths:
    - "Search returns relevant memories for a query"
    - "Results are filtered by similarity threshold (>0.7)"
    - "Context injection formats memories for AI prompts"
    - "Metadata filtering works (e.g., filter by memory type)"
  artifacts:
    - path: "src/ta_lab2/tools/ai_orchestrator/memory/query.py"
      provides: "Semantic search API"
      exports: ["search_memories", "SearchResult"]
    - path: "src/ta_lab2/tools/ai_orchestrator/memory/injection.py"
      provides: "Context injection for AI prompts"
      exports: ["inject_memory_context", "format_memories_for_prompt"]
    - path: "tests/orchestrator/test_memory_search.py"
      provides: "Search and injection tests"
      min_lines: 80
  key_links:
    - from: "src/ta_lab2/tools/ai_orchestrator/memory/query.py"
      to: "ChromaDB collection.query()"
      via: "MemoryClient.collection"
      pattern: "collection\\.query"
    - from: "src/ta_lab2/tools/ai_orchestrator/memory/injection.py"
      to: "query.py"
      via: "search_memories import"
      pattern: "from.*query.*import.*search_memories"
---

<objective>
Semantic search API and context injection for AI prompts (MEMO-02, MEMO-03)

Purpose: Enable retrieval of relevant memories from ChromaDB with similarity threshold filtering, and format them for injection into Claude/ChatGPT/Gemini prompts. This is the core RAG (Retrieval-Augmented Generation) functionality.

Output: search_memories() function with threshold filtering, inject_memory_context() for prompt construction, comprehensive tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-memory-core-chromadb-integration/02-RESEARCH.md

# Depends on 02-01 for MemoryClient
@.planning/phases/02-memory-core-chromadb-integration/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create semantic search query module</name>
  <files>src/ta_lab2/tools/ai_orchestrator/memory/query.py</files>
  <action>
Create memory/query.py with semantic search API:

```python
"""Semantic search API for ChromaDB memory store."""
import logging
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Optional, Dict, Any

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """Individual search result from memory query."""

    memory_id: str
    content: str
    metadata: Dict[str, Any]
    similarity: float  # 0.0 to 1.0, higher is better
    distance: float    # Raw ChromaDB distance (lower is better)

    def __str__(self) -> str:
        return f"Memory({self.memory_id[:8]}..., sim={self.similarity:.2f})"


@dataclass
class SearchResponse:
    """Response from memory search operation."""

    query: str
    results: List[SearchResult]
    total_found: int  # Before threshold filtering
    filtered_count: int  # After threshold filtering
    search_time_ms: float
    threshold_used: float

    def __str__(self) -> str:
        return (
            f"Search: '{self.query[:30]}...' -> "
            f"{self.filtered_count}/{self.total_found} results "
            f"(threshold={self.threshold_used})"
        )


def search_memories(
    query: str,
    max_results: int = 10,
    min_similarity: float = 0.7,
    memory_type: Optional[str] = None,
    metadata_filter: Optional[Dict[str, Any]] = None,
    client=None
) -> SearchResponse:
    """Search memories using semantic similarity.

    ChromaDB returns DISTANCE (lower = more similar), not similarity.
    This function converts to similarity (higher = better) for intuitive use.

    Requirement MEMO-02: relevance threshold >0.7 similarity.

    Args:
        query: Natural language search query
        max_results: Maximum number of results to return (top-K)
        min_similarity: Minimum similarity threshold (0.0-1.0, default 0.7)
                       Internally converted to distance: threshold = 1.0 - min_similarity
        memory_type: Optional filter by 'type' metadata field
        metadata_filter: Optional custom metadata filter (ChromaDB where clause)
        client: Optional MemoryClient instance

    Returns:
        SearchResponse with filtered results

    Example:
        >>> results = search_memories("How do I handle multi-timeframe EMAs?")
        >>> for r in results.results:
        ...     print(f"{r.similarity:.2f}: {r.content[:50]}...")
    """
    import time
    start_time = time.perf_counter()

    # Get client
    if client is None:
        from .client import get_memory_client
        client = get_memory_client()

    collection = client.collection

    # Build metadata filter
    where_filter = None
    if memory_type:
        where_filter = {"type": {"$eq": memory_type}}
    elif metadata_filter:
        where_filter = metadata_filter

    # Query ChromaDB
    # Note: ChromaDB returns distance, not similarity
    raw_results = collection.query(
        query_texts=[query],
        n_results=max_results,
        where=where_filter,
        include=["documents", "metadatas", "distances"]
    )

    # Convert distance to similarity and filter
    # For cosine distance: similarity = 1 - distance
    # For L2 distance: similarity approximation (less accurate)
    max_distance = 1.0 - min_similarity  # threshold conversion

    results: List[SearchResult] = []
    total_found = len(raw_results["ids"][0]) if raw_results["ids"] else 0

    for i in range(total_found):
        distance = raw_results["distances"][0][i]
        similarity = 1.0 - distance  # Convert to similarity

        # Filter by threshold
        if distance > max_distance:
            continue

        results.append(SearchResult(
            memory_id=raw_results["ids"][0][i],
            content=raw_results["documents"][0][i],
            metadata=raw_results["metadatas"][0][i] or {},
            similarity=round(similarity, 4),
            distance=round(distance, 4)
        ))

    elapsed_ms = (time.perf_counter() - start_time) * 1000

    response = SearchResponse(
        query=query,
        results=results,
        total_found=total_found,
        filtered_count=len(results),
        search_time_ms=round(elapsed_ms, 2),
        threshold_used=min_similarity
    )

    logger.debug(f"Memory search: {response}")
    return response


def get_memory_by_id(memory_id: str, client=None) -> Optional[SearchResult]:
    """Retrieve a specific memory by ID.

    Args:
        memory_id: The memory ID to retrieve
        client: Optional MemoryClient instance

    Returns:
        SearchResult if found, None otherwise
    """
    if client is None:
        from .client import get_memory_client
        client = get_memory_client()

    results = client.collection.get(
        ids=[memory_id],
        include=["documents", "metadatas", "embeddings"]
    )

    if not results["ids"]:
        return None

    return SearchResult(
        memory_id=results["ids"][0],
        content=results["documents"][0],
        metadata=results["metadatas"][0] or {},
        similarity=1.0,  # Exact match
        distance=0.0
    )


def get_memory_types(client=None, sample_size: int = 100) -> List[str]:
    """Get list of unique memory types from metadata.

    Args:
        client: Optional MemoryClient instance
        sample_size: Number of memories to sample for types

    Returns:
        List of unique type values
    """
    if client is None:
        from .client import get_memory_client
        client = get_memory_client()

    results = client.collection.get(
        limit=sample_size,
        include=["metadatas"]
    )

    types = set()
    for meta in results.get("metadatas", []):
        if meta and "type" in meta:
            types.add(meta["type"])

    return sorted(list(types))
```
  </action>
  <verify>Run `python -c "from ta_lab2.tools.ai_orchestrator.memory.query import search_memories; r = search_memories('EMA calculation'); print(f'Found: {r.filtered_count} results')"` - should find relevant memories</verify>
  <done>search_memories() returns filtered results with similarity scores, respects 0.7 threshold</done>
</task>

<task type="auto">
  <name>Task 2: Create context injection module</name>
  <files>src/ta_lab2/tools/ai_orchestrator/memory/injection.py</files>
  <action>
Create memory/injection.py with context formatting for AI prompts:

```python
"""Context injection utilities for AI prompts.

Formats retrieved memories for inclusion in Claude/ChatGPT/Gemini prompts.
Implements MEMO-03: Context injection system retrieves top-K memories.
"""
import logging
from typing import List, Optional

from .query import search_memories, SearchResult, SearchResponse

logger = logging.getLogger(__name__)


def format_memories_for_prompt(
    results: List[SearchResult],
    max_length: int = 4000,
    include_metadata: bool = True,
    include_similarity: bool = True
) -> str:
    """Format search results for AI prompt injection.

    Args:
        results: List of SearchResult from search_memories()
        max_length: Maximum total character length (to respect token limits)
        include_metadata: Include type and source metadata
        include_similarity: Include similarity score

    Returns:
        Formatted string ready for prompt injection
    """
    if not results:
        return "# No relevant memories found for this query.\n"

    lines = ["# Relevant Project Memories\n"]
    current_length = len(lines[0])

    for i, result in enumerate(results, 1):
        # Build memory block
        header = f"\n## Memory {i}"
        if include_similarity:
            header += f" (Relevance: {result.similarity:.0%})"
        header += "\n"

        meta_lines = []
        if include_metadata:
            if result.metadata.get("type"):
                meta_lines.append(f"**Type:** {result.metadata['type']}")
            if result.metadata.get("source_path"):
                meta_lines.append(f"**Source:** {result.metadata['source_path']}")

        content = result.content.strip()

        # Assemble block
        block_parts = [header]
        if meta_lines:
            block_parts.append("\n".join(meta_lines) + "\n")
        block_parts.append(f"\n{content}\n")
        block = "".join(block_parts)

        # Check length limit
        if current_length + len(block) > max_length:
            lines.append(f"\n*({len(results) - i + 1} more memories truncated)*\n")
            break

        lines.append(block)
        current_length += len(block)

    return "".join(lines)


def inject_memory_context(
    query: str,
    max_memories: int = 5,
    min_similarity: float = 0.7,
    memory_type: Optional[str] = None,
    max_length: int = 4000,
    client=None
) -> str:
    """Retrieve relevant memories and format for AI prompt context.

    This is the main entry point for RAG context injection.
    Combines search_memories() + format_memories_for_prompt().

    Args:
        query: The user query or task description
        max_memories: Maximum number of memories to retrieve (top-K)
        min_similarity: Minimum similarity threshold (default 0.7 per MEMO-02)
        memory_type: Optional filter by memory type
        max_length: Maximum character length for output
        client: Optional MemoryClient instance

    Returns:
        Formatted string ready to inject into AI prompt

    Example:
        >>> context = inject_memory_context("How do I backtest a strategy?")
        >>> prompt = f"Given this context:\n{context}\n\nQuestion: ..."
    """
    response = search_memories(
        query=query,
        max_results=max_memories,
        min_similarity=min_similarity,
        memory_type=memory_type,
        client=client
    )

    logger.info(
        f"Context injection: query='{query[:30]}...', "
        f"found={response.filtered_count} memories"
    )

    return format_memories_for_prompt(
        results=response.results,
        max_length=max_length
    )


def build_augmented_prompt(
    user_query: str,
    system_prompt: str = "",
    max_memories: int = 5,
    min_similarity: float = 0.7,
    memory_type: Optional[str] = None,
    client=None
) -> dict:
    """Build a complete RAG-augmented prompt structure.

    Returns a dict suitable for API calls to Claude/ChatGPT/Gemini.

    Args:
        user_query: The user's question or task
        system_prompt: Optional base system prompt
        max_memories: Maximum number of memories to retrieve
        min_similarity: Minimum similarity threshold
        memory_type: Optional filter by memory type
        client: Optional MemoryClient instance

    Returns:
        Dict with 'system', 'context', 'user' keys for prompt construction

    Example:
        >>> prompt = build_augmented_prompt("Explain EMA crossover strategy")
        >>> # Use prompt['system'] + prompt['context'] + prompt['user']
    """
    memory_context = inject_memory_context(
        query=user_query,
        max_memories=max_memories,
        min_similarity=min_similarity,
        memory_type=memory_type,
        client=client
    )

    return {
        "system": system_prompt or "You are a helpful assistant for the ta_lab2 project.",
        "context": memory_context,
        "user": user_query,
        "full_prompt": (
            f"{system_prompt}\n\n"
            f"{memory_context}\n\n"
            f"User Query: {user_query}"
        ) if system_prompt else f"{memory_context}\n\nUser Query: {user_query}"
    }


def estimate_context_tokens(text: str) -> int:
    """Estimate token count for context budgeting.

    Uses simple heuristic: ~4 characters per token for English.
    For accurate counts, use tiktoken library.

    Args:
        text: Text to estimate

    Returns:
        Estimated token count
    """
    # Rough heuristic - for accuracy use tiktoken
    return len(text) // 4
```
  </action>
  <verify>Run `python -c "from ta_lab2.tools.ai_orchestrator.memory.injection import inject_memory_context; c = inject_memory_context('EMA calculation'); print(c[:500])"` - should print formatted context</verify>
  <done>inject_memory_context() retrieves and formats memories for AI prompts</done>
</task>

<task type="auto">
  <name>Task 3: Update memory module exports</name>
  <files>src/ta_lab2/tools/ai_orchestrator/memory/__init__.py</files>
  <action>
Update memory/__init__.py to export query and injection modules:

```python
"""Memory integration for AI orchestrator.

Provides ChromaDB client, semantic search, and context injection
for RAG (Retrieval-Augmented Generation) with AI platforms.
"""
from .client import MemoryClient, get_memory_client, reset_memory_client
from .validation import (
    MemoryValidationResult,
    validate_memory_store,
    quick_health_check
)
from .query import (
    SearchResult,
    SearchResponse,
    search_memories,
    get_memory_by_id,
    get_memory_types
)
from .injection import (
    format_memories_for_prompt,
    inject_memory_context,
    build_augmented_prompt,
    estimate_context_tokens
)

__all__ = [
    # Client
    "MemoryClient",
    "get_memory_client",
    "reset_memory_client",
    # Validation
    "MemoryValidationResult",
    "validate_memory_store",
    "quick_health_check",
    # Query
    "SearchResult",
    "SearchResponse",
    "search_memories",
    "get_memory_by_id",
    "get_memory_types",
    # Injection
    "format_memories_for_prompt",
    "inject_memory_context",
    "build_augmented_prompt",
    "estimate_context_tokens",
]
```
  </action>
  <verify>Run `python -c "from ta_lab2.tools.ai_orchestrator.memory import search_memories, inject_memory_context; print('Exports OK')"` - should print without error</verify>
  <done>All query and injection functions exported from memory module</done>
</task>

<task type="auto">
  <name>Task 4: Create search and injection tests</name>
  <files>tests/orchestrator/test_memory_search.py</files>
  <action>
Create tests/orchestrator/test_memory_search.py with comprehensive tests:

```python
"""Tests for memory search and context injection modules."""
import pytest
from unittest.mock import MagicMock, patch
from ta_lab2.tools.ai_orchestrator.memory import (
    SearchResult,
    SearchResponse,
    search_memories,
    get_memory_by_id,
    get_memory_types,
    format_memories_for_prompt,
    inject_memory_context,
    build_augmented_prompt,
    estimate_context_tokens,
    reset_memory_client
)


class TestSearchResult:
    """Tests for SearchResult dataclass."""

    def test_search_result_str(self):
        """Test SearchResult string representation."""
        result = SearchResult(
            memory_id="abc123def456",
            content="Test content",
            metadata={"type": "test"},
            similarity=0.85,
            distance=0.15
        )
        output = str(result)
        assert "abc123de" in output
        assert "0.85" in output

    def test_search_response_str(self):
        """Test SearchResponse string representation."""
        response = SearchResponse(
            query="test query for searching memories",
            results=[],
            total_found=10,
            filtered_count=5,
            search_time_ms=12.5,
            threshold_used=0.7
        )
        output = str(response)
        assert "5/10" in output
        assert "0.7" in output


class TestSearchMemories:
    """Tests for search_memories function."""

    def setup_method(self):
        reset_memory_client()

    def teardown_method(self):
        reset_memory_client()

    @patch('ta_lab2.tools.ai_orchestrator.memory.query.get_memory_client')
    def test_search_returns_filtered_results(self, mock_get_client):
        """Test search filters by similarity threshold."""
        mock_client = MagicMock()
        mock_collection = MagicMock()

        # Mock query results with varying distances
        mock_collection.query.return_value = {
            "ids": [["id1", "id2", "id3"]],
            "documents": [["doc1", "doc2", "doc3"]],
            "metadatas": [[{"type": "a"}, {"type": "b"}, {"type": "c"}]],
            "distances": [[0.1, 0.25, 0.5]]  # 0.9, 0.75, 0.5 similarity
        }
        mock_client.collection = mock_collection
        mock_get_client.return_value = mock_client

        # With 0.7 threshold, only first two should pass
        response = search_memories("test", min_similarity=0.7, client=mock_client)

        assert response.total_found == 3
        assert response.filtered_count == 2
        assert len(response.results) == 2
        assert response.results[0].similarity == 0.9
        assert response.results[1].similarity == 0.75

    @patch('ta_lab2.tools.ai_orchestrator.memory.query.get_memory_client')
    def test_search_applies_metadata_filter(self, mock_get_client):
        """Test search applies memory_type filter."""
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_collection.query.return_value = {
            "ids": [["id1"]],
            "documents": [["doc1"]],
            "metadatas": [[{"type": "insight"}]],
            "distances": [[0.1]]
        }
        mock_client.collection = mock_collection
        mock_get_client.return_value = mock_client

        search_memories("test", memory_type="insight", client=mock_client)

        # Verify filter was passed
        call_args = mock_collection.query.call_args
        assert call_args.kwargs["where"] == {"type": {"$eq": "insight"}}

    @patch('ta_lab2.tools.ai_orchestrator.memory.query.get_memory_client')
    def test_search_converts_distance_to_similarity(self, mock_get_client):
        """Test distance to similarity conversion."""
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_collection.query.return_value = {
            "ids": [["id1"]],
            "documents": [["doc1"]],
            "metadatas": [[{}]],
            "distances": [[0.2]]  # Should become 0.8 similarity
        }
        mock_client.collection = mock_collection
        mock_get_client.return_value = mock_client

        response = search_memories("test", min_similarity=0.5, client=mock_client)

        assert response.results[0].similarity == 0.8
        assert response.results[0].distance == 0.2

    @patch('ta_lab2.tools.ai_orchestrator.memory.query.get_memory_client')
    def test_search_handles_empty_results(self, mock_get_client):
        """Test search handles no results gracefully."""
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_collection.query.return_value = {
            "ids": [[]],
            "documents": [[]],
            "metadatas": [[]],
            "distances": [[]]
        }
        mock_client.collection = mock_collection
        mock_get_client.return_value = mock_client

        response = search_memories("obscure query", client=mock_client)

        assert response.total_found == 0
        assert response.filtered_count == 0
        assert response.results == []


class TestFormatMemories:
    """Tests for format_memories_for_prompt function."""

    def test_format_empty_results(self):
        """Test formatting with no results."""
        output = format_memories_for_prompt([])
        assert "No relevant memories" in output

    def test_format_single_result(self):
        """Test formatting single result."""
        results = [SearchResult(
            memory_id="test123",
            content="This is the memory content.",
            metadata={"type": "insight", "source_path": "src/test.py"},
            similarity=0.85,
            distance=0.15
        )]

        output = format_memories_for_prompt(results)

        assert "Memory 1" in output
        assert "85%" in output  # Relevance
        assert "insight" in output
        assert "src/test.py" in output
        assert "This is the memory content" in output

    def test_format_respects_max_length(self):
        """Test formatting truncates at max_length."""
        results = [SearchResult(
            memory_id=f"id{i}",
            content="A" * 1000,  # Long content
            metadata={},
            similarity=0.9,
            distance=0.1
        ) for i in range(10)]

        output = format_memories_for_prompt(results, max_length=500)

        assert len(output) <= 600  # Allow some buffer for truncation message
        assert "truncated" in output.lower()

    def test_format_without_metadata(self):
        """Test formatting without metadata."""
        results = [SearchResult(
            memory_id="test123",
            content="Content only",
            metadata={},
            similarity=0.85,
            distance=0.15
        )]

        output = format_memories_for_prompt(results, include_metadata=False)

        assert "Type:" not in output
        assert "Content only" in output


class TestInjectMemoryContext:
    """Tests for inject_memory_context function."""

    @patch('ta_lab2.tools.ai_orchestrator.memory.injection.search_memories')
    def test_inject_combines_search_and_format(self, mock_search):
        """Test inject_memory_context combines search and formatting."""
        mock_search.return_value = SearchResponse(
            query="test",
            results=[SearchResult(
                memory_id="id1",
                content="Memory content",
                metadata={"type": "test"},
                similarity=0.9,
                distance=0.1
            )],
            total_found=1,
            filtered_count=1,
            search_time_ms=5.0,
            threshold_used=0.7
        )

        context = inject_memory_context("test query")

        assert "Memory 1" in context
        assert "Memory content" in context
        mock_search.assert_called_once()

    @patch('ta_lab2.tools.ai_orchestrator.memory.injection.search_memories')
    def test_inject_passes_parameters(self, mock_search):
        """Test inject_memory_context passes parameters correctly."""
        mock_search.return_value = SearchResponse(
            query="test",
            results=[],
            total_found=0,
            filtered_count=0,
            search_time_ms=1.0,
            threshold_used=0.8
        )

        inject_memory_context(
            "test",
            max_memories=3,
            min_similarity=0.8,
            memory_type="insight"
        )

        call_kwargs = mock_search.call_args.kwargs
        assert call_kwargs["max_results"] == 3
        assert call_kwargs["min_similarity"] == 0.8
        assert call_kwargs["memory_type"] == "insight"


class TestBuildAugmentedPrompt:
    """Tests for build_augmented_prompt function."""

    @patch('ta_lab2.tools.ai_orchestrator.memory.injection.inject_memory_context')
    def test_build_returns_structured_dict(self, mock_inject):
        """Test build_augmented_prompt returns proper structure."""
        mock_inject.return_value = "# Test Context"

        result = build_augmented_prompt(
            user_query="What is EMA?",
            system_prompt="You are helpful."
        )

        assert "system" in result
        assert "context" in result
        assert "user" in result
        assert "full_prompt" in result
        assert result["user"] == "What is EMA?"
        assert "You are helpful" in result["system"]


class TestEstimateContextTokens:
    """Tests for token estimation."""

    def test_estimate_returns_reasonable_count(self):
        """Test token estimation is roughly accurate."""
        text = "This is a test sentence with about ten words."
        estimate = estimate_context_tokens(text)

        # ~4 chars per token, 46 chars / 4 = ~11 tokens
        assert 8 <= estimate <= 15


class TestIntegrationWithRealChromaDB:
    """Integration tests with real ChromaDB."""

    @pytest.fixture
    def real_client(self):
        """Get real ChromaDB client, skip if unavailable."""
        reset_memory_client()
        try:
            from ta_lab2.tools.ai_orchestrator.config import load_config
            from ta_lab2.tools.ai_orchestrator.memory import get_memory_client
            import os

            config = load_config()
            if not os.path.exists(config.chromadb_path):
                pytest.skip(f"ChromaDB not found at {config.chromadb_path}")

            client = get_memory_client(config)
            yield client
        except Exception as e:
            pytest.skip(f"ChromaDB not available: {e}")
        finally:
            reset_memory_client()

    def test_real_search_returns_results(self, real_client):
        """Test search against real ChromaDB returns results."""
        response = search_memories(
            "EMA calculation multi-timeframe",
            max_results=5,
            min_similarity=0.5,  # Lower threshold for testing
            client=real_client
        )

        print(f"\nReal search: {response}")
        for r in response.results:
            print(f"  {r.similarity:.2f}: {r.content[:60]}...")

        # Should find something related to EMAs
        assert response.total_found > 0, "Expected some results from real ChromaDB"

    def test_real_inject_context(self, real_client):
        """Test context injection with real ChromaDB."""
        context = inject_memory_context(
            "How do I calculate EMA crossovers?",
            max_memories=3,
            min_similarity=0.5,
            client=real_client
        )

        print(f"\nReal context injection:\n{context[:500]}...")

        # Should produce formatted context
        assert len(context) > 50, "Expected substantial context"

    def test_real_memory_types(self, real_client):
        """Test getting memory types from real ChromaDB."""
        types = get_memory_types(client=real_client)

        print(f"\nMemory types found: {types}")

        # Should find some types
        assert len(types) >= 0  # May or may not have types
```
  </action>
  <verify>Run `pytest tests/orchestrator/test_memory_search.py -v` - all mock tests pass, integration tests pass if ChromaDB available</verify>
  <done>20+ tests covering search, filtering, formatting, injection, and real ChromaDB integration</done>
</task>

</tasks>

<verification>
1. `python -c "from ta_lab2.tools.ai_orchestrator.memory import search_memories; r = search_memories('EMA'); print(r)"` - finds memories
2. `python -c "from ta_lab2.tools.ai_orchestrator.memory import inject_memory_context; print(inject_memory_context('backtest')[:300])"` - formats context
3. `pytest tests/orchestrator/test_memory_search.py -v` - all tests pass
</verification>

<success_criteria>
- search_memories() returns results with similarity scores
- Results filtered by 0.7 threshold (MEMO-02 satisfied)
- inject_memory_context() formats memories for AI prompts (MEMO-03 satisfied)
- Metadata filtering works (filter by type)
- 20+ tests pass covering all search and injection scenarios
</success_criteria>

<output>
After completion, create `.planning/phases/02-memory-core-chromadb-integration/02-02-SUMMARY.md`
</output>
