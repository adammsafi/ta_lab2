---
phase: 09-integration-observability
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ta_lab2/observability/__init__.py
  - src/ta_lab2/observability/tracing.py
  - src/ta_lab2/observability/metrics.py
  - src/ta_lab2/observability/health.py
  - src/ta_lab2/observability/storage.py
  - sql/ddl/create_observability_schema.sql
autonomous: true

must_haves:
  truths:
    - "OpenTelemetry tracing generates correlation IDs for cross-system requests"
    - "Metrics can be recorded and queried from PostgreSQL"
    - "Health checks report liveness, readiness, and startup status"
  artifacts:
    - path: "src/ta_lab2/observability/tracing.py"
      provides: "OpenTelemetry tracing setup with PostgreSQL exporter"
      exports: ["setup_tracing", "get_tracer", "TracingContext"]
    - path: "src/ta_lab2/observability/metrics.py"
      provides: "Metrics collection with PostgreSQL storage"
      exports: ["MetricsCollector", "Metric", "counter", "gauge", "histogram"]
    - path: "src/ta_lab2/observability/health.py"
      provides: "Health check implementation"
      exports: ["HealthChecker", "HealthStatus", "liveness", "readiness", "startup"]
    - path: "src/ta_lab2/observability/storage.py"
      provides: "Observability database tables and queries"
      exports: ["WorkflowStateTracker", "ensure_observability_tables"]
    - path: "sql/ddl/create_observability_schema.sql"
      provides: "DDL for observability schema"
      contains: "CREATE SCHEMA IF NOT EXISTS observability"
  key_links:
    - from: "src/ta_lab2/observability/tracing.py"
      to: "src/ta_lab2/observability/storage.py"
      via: "PostgreSQLSpanExporter stores spans"
      pattern: "PostgreSQLSpanExporter|storage"
    - from: "src/ta_lab2/observability/metrics.py"
      to: "src/ta_lab2/observability/storage.py"
      via: "MetricsCollector writes to observability.metrics"
      pattern: "observability\\.metrics"
---

<objective>
Create observability infrastructure module with OpenTelemetry tracing, metrics collection, and health checks.

Purpose: Foundation for cross-system monitoring - all integration tests and production monitoring depend on this infrastructure.

Output: src/ta_lab2/observability/ module with tracing, metrics, health checks, and PostgreSQL storage.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-integration-observability/09-CONTEXT.md
@.planning/phases/09-integration-observability/09-RESEARCH.md

# Existing patterns to follow
@src/ta_lab2/notifications/telegram.py
@src/ta_lab2/scripts/features/validate_features.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create observability schema and storage module</name>
  <files>
    sql/ddl/create_observability_schema.sql
    src/ta_lab2/observability/__init__.py
    src/ta_lab2/observability/storage.py
  </files>
  <action>
    Create observability PostgreSQL schema and Python storage module.

    1. Create sql/ddl/create_observability_schema.sql:
       - CREATE SCHEMA IF NOT EXISTS observability
       - workflow_state table: workflow_id UUID PK, correlation_id VARCHAR(64), workflow_type, current_phase, status, created_at, updated_at, metadata JSONB
       - metrics table: id BIGSERIAL, metric_name, metric_value, metric_type, recorded_at TIMESTAMPTZ, labels JSONB - partitioned by month
       - spans table: trace_id VARCHAR(32), span_id VARCHAR(16), parent_span_id, operation_name, service_name, start_time, end_time, attributes JSONB, status
       - alerts table: id SERIAL, alert_type, severity, title, message, triggered_at, acknowledged_at, metadata JSONB
       - Indexes on correlation_id, metric_name+time, trace_id, alert severity+time

    2. Create src/ta_lab2/observability/__init__.py:
       - Export main classes: TracingContext, MetricsCollector, HealthChecker, WorkflowStateTracker, HealthStatus

    3. Create src/ta_lab2/observability/storage.py:
       - ensure_observability_tables(engine) - creates schema and tables idempotently
       - WorkflowStateTracker class:
         - __init__(engine)
         - create_workflow(workflow_id, correlation_id, workflow_type) -> inserts initial state
         - transition(workflow_id, new_phase, status, metadata=None) -> updates state
         - get_workflow(workflow_id) -> returns current state
         - list_workflows(status=None, limit=100) -> returns recent workflows
       - PostgreSQLSpanExporter class for OpenTelemetry spans
       - Use SQLAlchemy text() for queries, follow existing patterns from validate_features.py
  </action>
  <verify>
    python -c "from ta_lab2.observability.storage import ensure_observability_tables, WorkflowStateTracker; print('Storage module OK')"
  </verify>
  <done>
    Observability schema SQL exists with all tables. WorkflowStateTracker can create/transition/query workflows.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create OpenTelemetry tracing module</name>
  <files>
    src/ta_lab2/observability/tracing.py
  </files>
  <action>
    Create OpenTelemetry tracing integration.

    1. Create src/ta_lab2/observability/tracing.py:
       - setup_tracing(service_name: str, engine=None) -> Tracer
         - Creates TracerProvider with Resource(service.name)
         - If engine provided, add PostgreSQLSpanExporter from storage.py
         - Sets global tracer provider
         - Returns tracer for service

       - get_tracer(name: str = __name__) -> Tracer
         - Returns tracer from current provider

       - TracingContext class (context manager):
         - __init__(operation_name, attributes=None)
         - __enter__ -> starts span, returns self
         - __exit__ -> ends span, records exceptions if any
         - trace_id property -> returns 32-char hex trace ID
         - span_id property -> returns 16-char hex span ID
         - add_event(name, attributes=None)
         - set_attribute(key, value)

       - generate_correlation_id() -> returns 32-char hex string (uses OpenTelemetry trace context if available, else UUID)

       - Graceful degradation: If opentelemetry-api not installed, TracingContext becomes no-op wrapper

    2. Follow patterns from existing code:
       - Lazy imports inside functions where appropriate
       - Try/except ImportError for optional dependencies
       - Logging via logger = logging.getLogger(__name__)
  </action>
  <verify>
    python -c "from ta_lab2.observability.tracing import setup_tracing, TracingContext, generate_correlation_id; print(f'Correlation ID: {generate_correlation_id()}')"
  </verify>
  <done>
    TracingContext creates spans with trace_id propagation. generate_correlation_id returns valid 32-char hex.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create metrics and health check modules</name>
  <files>
    src/ta_lab2/observability/metrics.py
    src/ta_lab2/observability/health.py
  </files>
  <action>
    Create metrics collection and health check modules.

    1. Create src/ta_lab2/observability/metrics.py:
       - Metric dataclass: name, value, metric_type (counter/gauge/histogram), timestamp, labels dict

       - MetricsCollector class:
         - __init__(engine) -> stores engine, calls _ensure_table()
         - record(metric: Metric) -> inserts into observability.metrics
         - counter(name, value=1, **labels) -> records counter
         - gauge(name, value, **labels) -> records gauge
         - histogram(name, value, **labels) -> records histogram with bucket calculation
         - query(name, start_time=None, end_time=None, labels=None) -> returns metrics
         - get_percentile(name, percentile=0.5, hours=24) -> returns p50/p95/etc for recent data

    2. Create src/ta_lab2/observability/health.py:
       - HealthStatus dataclass: healthy (bool), message (str), checked_at (datetime), details (dict optional)

       - HealthChecker class:
         - __init__(engine, memory_client=None, config=None)
         - startup_complete property (bool, tracks if startup probe passed)

         - liveness() -> HealthStatus
           - Simple: returns healthy=True if process responds
           - DO NOT check dependencies (database, memory) - just process liveness

         - readiness() -> HealthStatus
           - Check database: SELECT 1
           - Check memory service if configured: memory_client.health_check()
           - Returns healthy only if ALL checks pass
           - Details dict includes each check result

         - startup() -> HealthStatus
           - Check if initial data loaded (e.g., dim_timeframe has rows)
           - Sets startup_complete=True once successful
           - Returns healthy=False until startup complete

       - Follow Kubernetes probe patterns from RESEARCH.md
  </action>
  <verify>
    python -c "from ta_lab2.observability.metrics import MetricsCollector, Metric; from ta_lab2.observability.health import HealthChecker, HealthStatus; print('Metrics and Health modules OK')"
  </verify>
  <done>
    MetricsCollector can record counter/gauge/histogram to database. HealthChecker implements liveness/readiness/startup probes.
  </done>
</task>

</tasks>

<verification>
```bash
# Verify module structure
python -c "
from ta_lab2.observability import TracingContext, MetricsCollector, HealthChecker, WorkflowStateTracker, HealthStatus
print('All observability exports available')
"

# Verify tracing generates correlation IDs
python -c "
from ta_lab2.observability.tracing import generate_correlation_id
cid = generate_correlation_id()
assert len(cid) == 32, f'Expected 32-char correlation ID, got {len(cid)}'
assert all(c in '0123456789abcdef' for c in cid), 'Expected hex string'
print(f'Valid correlation ID: {cid}')
"

# Verify SQL file exists
test -f sql/ddl/create_observability_schema.sql && echo "Schema SQL exists" || echo "Schema SQL MISSING"
```
</verification>

<success_criteria>
- src/ta_lab2/observability/ module exists with __init__.py, tracing.py, metrics.py, health.py, storage.py
- sql/ddl/create_observability_schema.sql creates observability schema with workflow_state, metrics, spans, alerts tables
- TracingContext generates 32-char hex trace IDs
- MetricsCollector has counter(), gauge(), histogram() methods
- HealthChecker implements liveness(), readiness(), startup() probes
- WorkflowStateTracker can create_workflow(), transition(), get_workflow()
- All imports work without errors
</success_criteria>

<output>
After completion, create `.planning/phases/09-integration-observability/09-01-SUMMARY.md`
</output>
