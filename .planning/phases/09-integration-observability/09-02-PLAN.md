---
phase: 09-integration-observability
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/conftest.py
  - tests/integration/__init__.py
  - tests/integration/conftest.py
  - tests/observability/__init__.py
  - tests/observability/conftest.py
  - tests/validation/__init__.py
  - tests/validation/conftest.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Tests can run in three infrastructure tiers: full infrastructure, mixed (real DB/mocked AI), or fully mocked"
    - "Integration tests skip gracefully when real infrastructure unavailable"
    - "Shared fixtures provide database, memory client, and test data across test tiers"
  artifacts:
    - path: "tests/conftest.py"
      provides: "Root conftest with shared fixtures and markers"
      contains: "pytest.mark.real_deps"
    - path: "tests/integration/conftest.py"
      provides: "Integration test fixtures"
      exports: ["real_database", "mock_database", "test_orchestrator"]
    - path: "tests/observability/conftest.py"
      provides: "Observability test fixtures"
      exports: ["metrics_collector", "health_checker"]
    - path: "tests/validation/conftest.py"
      provides: "Validation test fixtures"
      exports: ["test_assets", "expected_dates"]
  key_links:
    - from: "tests/integration/conftest.py"
      to: "tests/conftest.py"
      via: "fixture inheritance"
      pattern: "pytest.fixture"
    - from: "pyproject.toml"
      to: "tests/**/conftest.py"
      via: "marker registration"
      pattern: "markers.*=.*real_deps|mixed_deps|mocked_deps"
---

<objective>
Create integration test infrastructure with three-tier pytest markers and shared fixtures.

Purpose: Enable flexible test execution - real infrastructure tests for validation, mocked tests for CI/CD, mixed for development. The three tiers allow tests to run appropriately based on available infrastructure.

Output: Test directory structure with conftest.py files, pytest markers, and shared fixtures.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-integration-observability/09-CONTEXT.md
@.planning/phases/09-integration-observability/09-RESEARCH.md

# Existing test patterns
@tests/features/test_validate_features.py
@tests/features/test_feature_pipeline_integration.py
@tests/orchestrator/test_execution.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Register pytest markers and update root conftest</name>
  <files>
    pyproject.toml
    tests/conftest.py
  </files>
  <action>
    Register three-tier test markers and create root conftest with shared fixtures.

    1. Update pyproject.toml [tool.pytest.ini_options]:
       Add markers section:
       ```
       markers = [
           "real_deps: Tests requiring real infrastructure (database, Qdrant, OpenAI)",
           "mixed_deps: Tests with real database/Qdrant, mocked AI APIs",
           "mocked_deps: Tests with all external dependencies mocked (CI/CD)",
           "integration: Integration tests (cross-component)",
           "observability: Observability infrastructure tests",
           "validation: Gap and alignment validation tests",
           "slow: Tests that take >10 seconds",
       ]
       ```

    2. Update tests/conftest.py (append to existing or create):
       - Import pytest, os
       - Add fixture: database_url() -> returns TARGET_DB_URL or skips
       - Add fixture: skip_without_database() -> skips test if no database
       - Add fixture: skip_without_qdrant() -> skips test if Qdrant not running
       - Add pytest hook: pytest_configure(config) -> validate markers registered
       - Add auto-skip decorator helper for real_deps tests

    Example fixtures pattern:
    ```python
    @pytest.fixture(scope="session")
    def database_url():
        url = os.environ.get("TARGET_DB_URL") or os.environ.get("DATABASE_URL")
        if not url:
            pytest.skip("No database URL configured")
        return url

    @pytest.fixture(scope="session")
    def database_engine(database_url):
        from sqlalchemy import create_engine
        engine = create_engine(database_url)
        yield engine
        engine.dispose()
    ```
  </action>
  <verify>
    pytest --markers | grep -E "real_deps|mixed_deps|mocked_deps"
  </verify>
  <done>
    pytest --markers shows real_deps, mixed_deps, mocked_deps, integration, observability, validation markers.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create test directory structure with conftest files</name>
  <files>
    tests/integration/__init__.py
    tests/integration/conftest.py
    tests/observability/__init__.py
    tests/observability/conftest.py
    tests/validation/__init__.py
    tests/validation/conftest.py
  </files>
  <action>
    Create test directories with specialized fixtures for each test type.

    1. Create tests/integration/__init__.py:
       - Empty file for package

    2. Create tests/integration/conftest.py:
       - Fixtures for integration tests

       ```python
       @pytest.fixture(scope="function")
       def clean_database(database_engine):
           """Transaction rollback for test isolation."""
           connection = database_engine.connect()
           transaction = connection.begin()
           yield connection
           transaction.rollback()
           connection.close()

       @pytest.fixture
       def mock_orchestrator(mocker):
           """Mocked orchestrator for testing without real AI APIs."""
           from ta_lab2.tools.ai_orchestrator.execution import AsyncOrchestrator
           mock = mocker.MagicMock(spec=AsyncOrchestrator)
           mock.execute_single = mocker.AsyncMock(return_value=Result(...))
           return mock

       @pytest.fixture
       def mock_memory_client(mocker):
           """Mocked memory client."""
           mock = mocker.MagicMock()
           mock.search.return_value = {"results": [{"content": "test memory"}]}
           return mock

       @pytest.fixture
       def test_task():
           """Sample task for testing."""
           from ta_lab2.tools.ai_orchestrator.core import Task, TaskType
           return Task(type=TaskType.CODE_ANALYSIS, prompt="Test task")
       ```

    3. Create tests/observability/__init__.py and conftest.py:
       ```python
       @pytest.fixture
       def metrics_collector(mocker):
           """MetricsCollector with mocked database."""
           from ta_lab2.observability.metrics import MetricsCollector
           mock_engine = mocker.MagicMock()
           return MetricsCollector(mock_engine)

       @pytest.fixture
       def health_checker(mocker):
           """HealthChecker with mocked dependencies."""
           from ta_lab2.observability.health import HealthChecker
           mock_engine = mocker.MagicMock()
           return HealthChecker(mock_engine)

       @pytest.fixture
       def tracing_context():
           """TracingContext for testing spans."""
           from ta_lab2.observability.tracing import TracingContext
           return TracingContext("test_operation")
       ```

    4. Create tests/validation/__init__.py and conftest.py:
       ```python
       @pytest.fixture
       def test_assets():
           """Sample asset IDs for validation tests."""
           return [1, 52]  # BTC, ETH commonly used

       @pytest.fixture
       def expected_dates():
           """Generate expected date range for gap testing."""
           from datetime import datetime, timedelta
           end = datetime.now().date()
           start = end - timedelta(days=30)
           return pd.date_range(start, end, freq='D')

       @pytest.fixture
       def mock_dim_sessions():
           """Mock trading session data."""
           return {
               'crypto': {'session': 'CRYPTO', 'daily': True},
               'equity': {'session': 'NYSE', 'daily': False, 'trading_days': [0,1,2,3,4]},
           }
       ```
  </action>
  <verify>
    python -c "
import sys
sys.path.insert(0, 'tests')
from integration.conftest import *
from observability.conftest import *
from validation.conftest import *
print('All conftest imports successful')
"
  </verify>
  <done>
    tests/integration/, tests/observability/, tests/validation/ directories exist with __init__.py and conftest.py files.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create sample tests demonstrating three-tier pattern</name>
  <files>
    tests/integration/test_tier_demo.py
  </files>
  <action>
    Create demonstration tests showing how three-tier markers work.

    Create tests/integration/test_tier_demo.py:

    ```python
    """
    Demonstration of three-tier test dependency pattern.

    Run different tiers:
      pytest -m real_deps      # Requires real DB, Qdrant, OpenAI
      pytest -m mixed_deps     # Real DB, mocked AI
      pytest -m mocked_deps    # All mocked (CI/CD)
      pytest -m "not real_deps"  # Skip slow infrastructure tests
    """

    import pytest
    from unittest.mock import MagicMock


    @pytest.mark.real_deps
    @pytest.mark.integration
    class TestRealDependencies:
        """Tests that require full infrastructure."""

        def test_database_connection(self, database_engine):
            """Verify real database is accessible."""
            from sqlalchemy import text
            with database_engine.connect() as conn:
                result = conn.execute(text("SELECT 1")).scalar()
                assert result == 1

        @pytest.mark.skipif(
            not os.environ.get("QDRANT_HOST"),
            reason="Qdrant not configured"
        )
        def test_qdrant_connection(self):
            """Verify Qdrant memory store is accessible."""
            # Would test real Qdrant connection
            pass


    @pytest.mark.mixed_deps
    @pytest.mark.integration
    class TestMixedDependencies:
        """Tests with real DB, mocked AI."""

        def test_workflow_with_mocked_ai(self, clean_database, mock_orchestrator):
            """Test workflow using real DB but mocked AI."""
            # Real database operations
            # Mocked AI calls
            assert mock_orchestrator is not None

        def test_memory_search_with_mocked_api(self, clean_database, mock_memory_client):
            """Test memory integration with mocked embedding API."""
            result = mock_memory_client.search("test query")
            assert "results" in result


    @pytest.mark.mocked_deps
    @pytest.mark.integration
    class TestMockedDependencies:
        """Tests with all dependencies mocked - for CI/CD."""

        def test_orchestrator_routing_logic(self, mock_orchestrator):
            """Test routing logic without any real services."""
            # Pure logic testing
            assert mock_orchestrator is not None

        def test_workflow_state_machine(self, mocker):
            """Test workflow state transitions without DB."""
            from ta_lab2.observability.storage import WorkflowStateTracker

            mock_engine = mocker.MagicMock()
            tracker = WorkflowStateTracker(mock_engine)

            # Test state transition logic
            # (mocked database calls)
            assert tracker is not None
    ```
  </action>
  <verify>
    # Verify mocked tests run without real infrastructure
    pytest tests/integration/test_tier_demo.py -m mocked_deps -v --collect-only 2>/dev/null | grep "test_" || echo "Tests collected"
  </verify>
  <done>
    tests/integration/test_tier_demo.py demonstrates @real_deps, @mixed_deps, @mocked_deps patterns. pytest -m mocked_deps collects mocked tests.
  </done>
</task>

</tasks>

<verification>
```bash
# Verify markers are registered
pytest --markers | grep -E "(real_deps|mixed_deps|mocked_deps|integration|observability|validation)"

# Verify test collection works for each tier
pytest --collect-only -m real_deps 2>/dev/null | head -20
pytest --collect-only -m mocked_deps 2>/dev/null | head -20

# Verify directories exist
ls -la tests/integration/ tests/observability/ tests/validation/

# Run mocked tests (should work without infrastructure)
pytest tests/integration/test_tier_demo.py -m mocked_deps -v
```
</verification>

<success_criteria>
- pyproject.toml has markers section with real_deps, mixed_deps, mocked_deps, integration, observability, validation
- tests/conftest.py has database_url, database_engine, skip helpers
- tests/integration/conftest.py has clean_database, mock_orchestrator, mock_memory_client, test_task fixtures
- tests/observability/conftest.py has metrics_collector, health_checker, tracing_context fixtures
- tests/validation/conftest.py has test_assets, expected_dates, mock_dim_sessions fixtures
- pytest --markers shows all registered markers
- pytest -m mocked_deps collects and runs tests without requiring real infrastructure
</success_criteria>

<output>
After completion, create `.planning/phases/09-integration-observability/09-02-SUMMARY.md`
</output>
