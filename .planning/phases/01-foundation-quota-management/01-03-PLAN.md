---
phase: 01-foundation-quota-management
plan: 03
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - src/ta_lab2/tools/ai_orchestrator/validation.py
  - src/ta_lab2/tools/ai_orchestrator/adapters.py
  - src/ta_lab2/tools/ai_orchestrator/routing.py
  - src/ta_lab2/tools/ai_orchestrator/core.py
  - src/ta_lab2/tools/ai_orchestrator/__init__.py
  - tests/orchestrator/test_validation.py
  - tests/orchestrator/test_smoke.py
  - .planning/PARALLEL-TRACKS.md
autonomous: true

must_haves:
  truths:
    - "Pre-flight validation prevents routing to unimplemented adapters"
    - "Each adapter reports its implementation status (stub vs working)"
    - "Validation runs during routing (excludes stubs) AND before execution (safety check)"
    - "Smoke tests verify one end-to-end task executes successfully"
    - "All three parallel tracks (memory, orchestrator, ta_lab2) can develop independently"
    - "Memory, orchestrator, and ta_lab2 have documented interface boundaries"
    - "Each track has stub implementations for testing without other tracks"
  artifacts:
    - path: "src/ta_lab2/tools/ai_orchestrator/validation.py"
      provides: "Pre-flight adapter validation (double-check: routing + execution)"
      exports: ["AdapterValidator", "ValidationResult", "validate_adapters", "pre_flight_check"]
      min_lines: 100
    - path: "src/ta_lab2/tools/ai_orchestrator/adapters.py"
      provides: "Updated adapters with is_implemented property"
      contains: "is_implemented"
    - path: "tests/orchestrator/test_validation.py"
      provides: "Validation tests for ORCH-11 (double validation)"
      min_lines: 80
    - path: "tests/orchestrator/test_smoke.py"
      provides: "End-to-end smoke tests"
      min_lines: 40
    - path: ".planning/PARALLEL-TRACKS.md"
      provides: "Documentation of memory/orchestrator/ta_lab2 track boundaries and interfaces"
      contains: "interface contracts"
      min_lines: 60
  key_links:
    - from: "src/ta_lab2/tools/ai_orchestrator/routing.py"
      to: "src/ta_lab2/tools/ai_orchestrator/validation.py"
      via: "filter available platforms during routing (first validation)"
      pattern: "get_available_platforms|filter.*implemented"
      mechanism: "TaskRouter.route() calls validator.get_available_platforms() to exclude stubs"
    - from: "src/ta_lab2/tools/ai_orchestrator/core.py"
      to: "src/ta_lab2/tools/ai_orchestrator/validation.py"
      via: "pre-flight safety check before execute (second validation)"
      pattern: "pre_flight_check"
      mechanism: "Orchestrator.execute(pre_flight=True) calls pre_flight_check(task, validator) and returns error Result if validation fails"
    - from: "src/ta_lab2/tools/ai_orchestrator/adapters.py"
      to: "is_implemented property"
      via: "adapter status reporting"
      pattern: "is_implemented"
---

<objective>
Implement double-validation system (ORCH-11), document parallel track interfaces, and create comprehensive tests for Phase 1 infrastructure.

Purpose: Double validation (routing excludes stubs, execution safety-checks) prevents tasks from reaching unimplemented adapters. Parallel track documentation with stubs enables memory, orchestrator, and ta_lab2 to develop independently. Tests prove the entire foundation works.

Output:
- validation.py with double-check pattern (routing filter + execution safety)
- Updated adapters with is_implemented property
- routing.py filters unavailable platforms
- core.py safety-checks before execution
- PARALLEL-TRACKS.md with interface contracts and test stubs
- Comprehensive validation and smoke tests
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-quota-management/01-CONTEXT.md
@.planning/phases/01-foundation-quota-management/01-01-SUMMARY.md
@.planning/phases/01-foundation-quota-management/01-02-SUMMARY.md
@src/ta_lab2/tools/ai_orchestrator/adapters.py
@src/ta_lab2/tools/ai_orchestrator/core.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add is_implemented property to adapters</name>
  <files>src/ta_lab2/tools/ai_orchestrator/adapters.py</files>
  <action>
Update adapters.py to add implementation status to each adapter:

1. Add to BasePlatformAdapter ABC:
   - @property @abstractmethod is_implemented(self) -> bool
   - @property @abstractmethod implementation_status(self) -> str

2. Update ClaudeCodeAdapter:
   - is_implemented -> True (Claude Code CLI works via current session)
   - implementation_status -> "working" or "partial" based on capabilities
   - Add _check_implementation() method that verifies CLI is available

3. Update ChatGPTAdapter:
   - is_implemented -> False (still a stub)
   - implementation_status -> "stub"
   - When is_implemented is False, execute() should raise NotImplementedError with helpful message

4. Update GeminiAdapter:
   - is_implemented -> depends on _check_gcloud_available()
   - implementation_status -> "working" if gcloud available, "unavailable" if not
   - Add _check_gcloud_available() that runs `gcloud --version`

5. Add get_adapter_status() method to each adapter:
   - Returns dict with: name, is_implemented, status, capabilities, requirements
   - capabilities: list of what the adapter can do
   - requirements: list of what needs to be set up (env vars, CLI tools)

Preserve all existing functionality - only add the new properties/methods.
  </action>
  <verify>
Run: `python -c "from ta_lab2.tools.ai_orchestrator.adapters import ClaudeCodeAdapter, ChatGPTAdapter, GeminiAdapter; print(ClaudeCodeAdapter().is_implemented, ChatGPTAdapter().is_implemented, GeminiAdapter().is_implemented)"`
  </verify>
  <done>Each adapter reports accurate implementation status</done>
</task>

<task type="auto">
  <name>Task 2: Create validation module with double-check pattern</name>
  <files>src/ta_lab2/tools/ai_orchestrator/validation.py</files>
  <action>
Create validation.py with pre-flight validation (double-check: routing filter + execution safety):

1. Define ValidationResult dataclass:
   - adapter_name: str
   - is_valid: bool
   - is_implemented: bool
   - status: str ("working", "partial", "stub", "unavailable", "error")
   - message: str
   - requirements_met: dict[str, bool]
   - timestamp: datetime

2. Define AdapterValidator class:
   - __init__(self, adapters: dict[Platform, BasePlatformAdapter])
   - validate_adapter(self, platform: Platform) -> ValidationResult
   - validate_all(self) -> dict[Platform, ValidationResult]
   - get_available_platforms(self) -> list[Platform]: Only implemented adapters (for routing filter)
   - get_unavailable_platforms(self) -> list[Platform]: Stub or broken adapters
   - is_platform_available(self, platform: Platform) -> bool: Quick check for specific platform

3. Implement validate_adapter():
   - Check is_implemented property
   - Check environment variables (API keys set?)
   - Check CLI tools available (gcloud for Gemini)
   - Return comprehensive ValidationResult

4. Implement pre_flight_check(task: Task, validator: AdapterValidator) -> tuple[bool, str]:
   - SECOND validation checkpoint (execution-time safety check)
   - Given a task, check if ANY valid adapter exists
   - If platform_hint set, validate that specific adapter
   - Return (can_execute, reason_if_not)
   - Include list of available platforms in error message

5. Add module-level validate_adapters(orchestrator) -> dict function for easy use

Export: AdapterValidator, ValidationResult, validate_adapters, pre_flight_check
  </action>
  <verify>
Run: `python -c "from ta_lab2.tools.ai_orchestrator.validation import AdapterValidator, pre_flight_check; print('Validation module OK')"`
  </verify>
  <done>Validation module implements double-check pattern: routing filter + execution safety</done>
</task>

<task type="auto">
  <name>Task 3: Integrate validation into routing (first checkpoint)</name>
  <files>
    src/ta_lab2/tools/ai_orchestrator/routing.py
    src/ta_lab2/tools/ai_orchestrator/core.py
  </files>
  <action>
Part A - Update routing.py TaskRouter:

1. Add validator parameter to TaskRouter.__init__():
   - Store AdapterValidator as self.validator

2. Modify route() method to filter unavailable platforms:
   - Before routing logic, call: available = self.validator.get_available_platforms()
   - Only consider platforms in available list
   - If no platforms available, raise RuntimeError with helpful message listing what's needed
   - Update routing logic to skip stub adapters

Part B - Update core.py Orchestrator:

1. Add validation to Orchestrator.__init__():
   - Import and create AdapterValidator
   - Store as self.validator
   - Pass validator to TaskRouter: self.router = TaskRouter(validator=self.validator)

2. Add pre_flight parameter to execute() (second checkpoint):
   - execute(self, task: Task, pre_flight: bool = True) -> Result
   - After routing, if pre_flight=True:
     a. Call: can_execute, reason = pre_flight_check(task, self.validator)
     b. If not can_execute:
        - Get available: self.validator.get_available_platforms()
        - Return Result(success=False, output=f"Cannot execute: {reason}. Available platforms: {[p.value for p in available]}")
   - Only execute if validation passes

3. Add validate_environment() method:
   - Returns full validation status for all adapters
   - Useful for CLI status display

This implements defense-in-depth: routing excludes stubs (first check), execute verifies again (second check).
  </action>
  <verify>
Run: `python -c "from ta_lab2.tools.ai_orchestrator import Orchestrator; o = Orchestrator(); print('Double validation integrated')"`
  </verify>
  <done>Double validation integrated: routing filters stubs, execute() safety-checks before execution</done>
</task>

<task type="auto">
  <name>Task 4: Create validation tests</name>
  <files>tests/orchestrator/test_validation.py</files>
  <action>
Create test_validation.py with comprehensive validation tests:

1. test_stub_adapter_blocked_at_routing():
   - Configure only ChatGPT (stub) adapter
   - Attempt to route task
   - Assert routing raises error or returns no platforms

2. test_stub_adapter_blocked_at_execution():
   - Mock routing to return ChatGPT platform
   - Call execute() with pre_flight=True
   - Assert returns Result with success=False and helpful error

3. test_implemented_adapter_passes():
   - Claude Code should pass both routing and execution validation
   - Verify no errors raised

4. test_validation_reports_requirements():
   - Check validation results include missing env vars
   - Verify helpful messages about what needs to be installed

5. test_pre_flight_can_be_skipped():
   - pre_flight=False bypasses execution checkpoint
   - Routing checkpoint still applies

6. test_helpful_error_on_no_adapters():
   - All adapters are stubs
   - Error message lists requirements to enable each platform

7. test_double_validation_catches_race_condition():
   - Mock scenario where adapter becomes unavailable between routing and execution
   - Execution checkpoint should catch it

8. test_get_available_platforms():
   - Verify only implemented adapters returned
   - Stub adapters excluded

All tests verify the double-check pattern works correctly.
  </action>
  <verify>
Run: `pytest tests/orchestrator/test_validation.py -v` - all 8 tests pass
  </verify>
  <done>Validation tests prove double-check pattern catches stubs at both routing and execution</done>
</task>

<task type="auto">
  <name>Task 5: Create smoke tests and update exports</name>
  <files>
    tests/orchestrator/test_smoke.py
    src/ta_lab2/tools/ai_orchestrator/__init__.py
  </files>
  <action>
Part A - Create test_smoke.py (integration tests):

1. test_smoke_quota_persistence():
   - Write quota state to persistence
   - Reload from disk
   - Verify values match

2. test_smoke_config_loading():
   - Load config from .env.example
   - Verify required fields present

3. test_smoke_orchestrator_init():
   - Create Orchestrator()
   - Verify router, quota_tracker, validator, adapters all initialized
   - No errors during init

4. test_smoke_validation_runs():
   - Call orchestrator.validate_environment()
   - Verify returns status dict
   - Check Claude Code shows as available

5. test_smoke_task_execution_claude():
   - Create simple Task(type=CODE_GENERATION, prompt="test")
   - Execute via Claude Code adapter
   - Verify Result returned (may contain instructions since subprocess not implemented)
   - Verify quota tracking records usage

These prove the complete Phase 1 infrastructure works end-to-end.

Part B - Update __init__.py exports:
   - Add to __all__: AdapterValidator, ValidationResult, validate_adapters, pre_flight_check
   - Ensure all public APIs are exported
  </action>
  <verify>
Run: `pytest tests/orchestrator/test_smoke.py -v` - all 5 tests pass
  </verify>
  <done>Smoke tests prove end-to-end infrastructure works; all exports available</done>
</task>

<task type="auto">
  <name>Task 6: Document parallel track interfaces</name>
  <files>.planning/PARALLEL-TRACKS.md</files>
  <action>
Create .planning/PARALLEL-TRACKS.md documenting the three parallel development tracks:

Structure:

# Parallel Development Tracks

## Overview
Explains how memory, orchestrator, and ta_lab2 can develop independently.

## Track 1: Memory Infrastructure
- **Location:** src/ta_lab2/tools/memory/
- **Interface:** Memory API
  - add(content, metadata) -> memory_id
  - search(query, threshold=0.7) -> List[Memory]
  - get_context(memory_ids) -> str
- **Dependencies:** Mem0, Vertex AI Memory Bank
- **Stub for testing:** tests/stubs/memory_stub.py with in-memory dict
- **Integration point:** Phase 9 (orchestrator calls memory.add/search)

## Track 2: Orchestrator
- **Location:** src/ta_lab2/tools/ai_orchestrator/
- **Interface:** Task/Result API
  - Orchestrator.execute(task) -> Result
  - Task(type, prompt, context) - input format
  - Result(output, success, cost) - output format
- **Dependencies:** Claude/ChatGPT/Gemini SDKs
- **Stub for testing:** tests/stubs/orchestrator_stub.py with mock executor
- **Integration point:** Phase 9 (uses memory for context injection)

## Track 3: ta_lab2 Features
- **Location:** src/ta_lab2/features/, src/ta_lab2/scripts/
- **Interface:** SQL + Python API
  - Refresh scripts: refresh_ema_daily.py → writes to cmc_ema_daily table
  - Query interface: get_ema(symbol, date) -> DataFrame
- **Dependencies:** PostgreSQL, Polars
- **Stub for testing:** tests/stubs/db_stub.py with in-memory Polars DataFrames
- **Integration point:** Phase 9 (orchestrator automates refresh scripts)

## Development Independence Verification

### Track 1 (Memory) can develop without Track 2/3:
- Tests use memory_stub for orchestrator calls
- No dependency on ta_lab2 tables

### Track 2 (Orchestrator) can develop without Track 1/3:
- Tests use orchestrator_stub for memory operations
- Task routing works without ta_lab2 scripts

### Track 3 (ta_lab2) can develop without Track 1/2:
- Manual script execution doesn't need orchestrator
- Operates on PostgreSQL, no memory layer needed

## Integration Phase (Phase 9)
When tracks converge:
1. Remove stubs, use real implementations
2. Test end-to-end: orchestrator → memory → ta_lab2
3. Verify: user submits task → routed → memory provides context → ta_lab2 executes → result stored
  </action>
  <verify>
Run: `cat .planning/PARALLEL-TRACKS.md | grep "Track 1\\|Track 2\\|Track 3"` shows all three tracks documented
  </verify>
  <done>Parallel track interfaces documented with stubs for independent development</done>
</task>

</tasks>

<verification>
1. `from ta_lab2.tools.ai_orchestrator import AdapterValidator, pre_flight_check` works
2. ChatGPT adapter (stub) blocked at routing (first checkpoint)
3. If routing bypassed, stub blocked at execution (second checkpoint)
4. Claude Code adapter passes both validation checkpoints
5. TaskRouter.route() only considers available platforms
6. Orchestrator.execute(pre_flight=True) safety-checks before execution
7. `pytest tests/orchestrator/test_validation.py -v` - all 8 tests pass
8. `pytest tests/orchestrator/test_smoke.py -v` - all 5 tests pass
9. `.planning/PARALLEL-TRACKS.md` documents all three track interfaces
10. Error messages explain what's available and what's needed
</verification>

<success_criteria>
- ORCH-11 fully implemented: Double validation (routing + execution) prevents routing to unimplemented adapters
- Each adapter accurately reports its implementation status
- Validation provides helpful feedback about requirements
- Smoke tests prove end-to-end infrastructure works
- Parallel track interfaces documented with stubs for independent development
- All Phase 1 success criteria verified by tests
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-quota-management/01-03-SUMMARY.md`
</output>
