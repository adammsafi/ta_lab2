---
phase: 01-foundation-quota-management
plan: 03
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - src/ta_lab2/tools/ai_orchestrator/validation.py
  - src/ta_lab2/tools/ai_orchestrator/adapters.py
  - src/ta_lab2/tools/ai_orchestrator/__init__.py
  - tests/orchestrator/test_validation.py
  - tests/orchestrator/test_smoke.py
autonomous: true

must_haves:
  truths:
    - "Pre-flight validation prevents routing to unimplemented adapters"
    - "Each adapter reports its implementation status (stub vs working)"
    - "Validation runs before task execution and blocks stub adapters"
    - "Smoke tests verify one end-to-end task executes successfully"
    - "All three parallel tracks (memory, orchestrator, ta_lab2) can develop independently"
  artifacts:
    - path: "src/ta_lab2/tools/ai_orchestrator/validation.py"
      provides: "Pre-flight adapter validation"
      exports: ["AdapterValidator", "ValidationResult", "validate_adapters"]
      min_lines: 80
    - path: "src/ta_lab2/tools/ai_orchestrator/adapters.py"
      provides: "Updated adapters with is_implemented property"
      contains: "is_implemented"
    - path: "tests/orchestrator/test_validation.py"
      provides: "Validation tests for ORCH-11"
      min_lines: 60
    - path: "tests/orchestrator/test_smoke.py"
      provides: "End-to-end smoke tests"
      min_lines: 40
  key_links:
    - from: "src/ta_lab2/tools/ai_orchestrator/core.py"
      to: "src/ta_lab2/tools/ai_orchestrator/validation.py"
      via: "pre-flight check before execute"
      pattern: "validate|pre_flight"
    - from: "src/ta_lab2/tools/ai_orchestrator/adapters.py"
      to: "is_implemented property"
      via: "adapter status reporting"
      pattern: "is_implemented"
---

<objective>
Implement pre-flight adapter validation (ORCH-11) and create smoke tests that verify the complete Phase 1 infrastructure.

Purpose: Pre-flight validation prevents the orchestrator from routing tasks to stub adapters, ensuring users get clear feedback about what's available. Smoke tests prove the entire foundation works end-to-end.

Output:
- validation.py with AdapterValidator
- Updated adapters with is_implemented property
- Passing validation and smoke tests
- Documentation of parallel track interfaces
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-quota-management/01-CONTEXT.md
@.planning/phases/01-foundation-quota-management/01-01-SUMMARY.md
@.planning/phases/01-foundation-quota-management/01-02-SUMMARY.md
@src/ta_lab2/tools/ai_orchestrator/adapters.py
@src/ta_lab2/tools/ai_orchestrator/core.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add is_implemented property to adapters</name>
  <files>src/ta_lab2/tools/ai_orchestrator/adapters.py</files>
  <action>
Update adapters.py to add implementation status to each adapter:

1. Add to BasePlatformAdapter ABC:
   - @property @abstractmethod is_implemented(self) -> bool
   - @property @abstractmethod implementation_status(self) -> str

2. Update ClaudeCodeAdapter:
   - is_implemented -> True (Claude Code CLI works via current session)
   - implementation_status -> "working" or "partial" based on capabilities
   - Add _check_implementation() method that verifies CLI is available

3. Update ChatGPTAdapter:
   - is_implemented -> False (still a stub)
   - implementation_status -> "stub"
   - When is_implemented is False, execute() should raise NotImplementedError with helpful message

4. Update GeminiAdapter:
   - is_implemented -> depends on _check_gcloud_available()
   - implementation_status -> "working" if gcloud available, "unavailable" if not
   - Add _check_gcloud_available() that runs `gcloud --version`

5. Add get_adapter_status() method to each adapter:
   - Returns dict with: name, is_implemented, status, capabilities, requirements
   - capabilities: list of what the adapter can do
   - requirements: list of what needs to be set up (env vars, CLI tools)

Preserve all existing functionality - only add the new properties/methods.
  </action>
  <verify>
Run: `python -c "from ta_lab2.tools.ai_orchestrator.adapters import ClaudeCodeAdapter, ChatGPTAdapter, GeminiAdapter; print(ClaudeCodeAdapter().is_implemented, ChatGPTAdapter().is_implemented, GeminiAdapter().is_implemented)"`
  </verify>
  <done>Each adapter reports accurate implementation status</done>
</task>

<task type="auto">
  <name>Task 2: Create validation module</name>
  <files>src/ta_lab2/tools/ai_orchestrator/validation.py</files>
  <action>
Create validation.py with pre-flight validation:

1. Define ValidationResult dataclass:
   - adapter_name: str
   - is_valid: bool
   - is_implemented: bool
   - status: str ("working", "partial", "stub", "unavailable", "error")
   - message: str
   - requirements_met: dict[str, bool]
   - timestamp: datetime

2. Define AdapterValidator class:
   - __init__(self, adapters: dict[Platform, BasePlatformAdapter])
   - validate_adapter(self, platform: Platform) -> ValidationResult
   - validate_all(self) -> dict[Platform, ValidationResult]
   - get_available_platforms(self) -> list[Platform]: Only implemented adapters
   - get_unavailable_platforms(self) -> list[Platform]: Stub or broken adapters

3. Implement validate_adapter():
   - Check is_implemented property
   - Check environment variables (API keys set?)
   - Check CLI tools available (gcloud for Gemini)
   - Return comprehensive ValidationResult

4. Implement pre_flight_check(task: Task, validator: AdapterValidator) -> tuple[bool, str]:
   - Given a task, check if ANY valid adapter exists
   - If platform_hint set, validate that specific adapter
   - Return (can_execute, reason_if_not)

5. Add module-level validate_adapters(orchestrator) -> dict function for easy use

Export: AdapterValidator, ValidationResult, validate_adapters, pre_flight_check
  </action>
  <verify>
Run: `python -c "from ta_lab2.tools.ai_orchestrator.validation import AdapterValidator; print('Validation module OK')"`
  </verify>
  <done>Pre-flight validation module checks adapter implementation before routing</done>
</task>

<task type="auto">
  <name>Task 3: Integrate validation into Orchestrator and create tests</name>
  <files>
    src/ta_lab2/tools/ai_orchestrator/core.py
    src/ta_lab2/tools/ai_orchestrator/__init__.py
    tests/orchestrator/test_validation.py
    tests/orchestrator/test_smoke.py
  </files>
  <action>
Part A - Update core.py Orchestrator:

1. Add validation to Orchestrator.__init__():
   - Import and create AdapterValidator
   - Store as self.validator

2. Add pre_flight parameter to execute():
   - execute(self, task: Task, pre_flight: bool = True) -> Result
   - If pre_flight=True, run validation before routing
   - If validation fails, return Result with success=False and helpful error message
   - Error message should explain which adapters are available

3. Add validate_environment() method:
   - Returns full validation status for all adapters
   - Useful for CLI status display

Part B - Update __init__.py exports:
   - Add AdapterValidator, ValidationResult, validate_adapters to __all__

Part C - Create test_validation.py:

1. test_stub_adapter_blocked(): ChatGPT stub should fail pre-flight
2. test_implemented_adapter_passes(): Claude Code should pass pre-flight
3. test_validation_reports_requirements(): Missing env vars reported
4. test_pre_flight_can_be_skipped(): pre_flight=False bypasses check
5. test_helpful_error_on_no_adapters(): Clear message when nothing available

Part D - Create test_smoke.py:

1. test_smoke_quota_persistence(): Write quota, restart, verify persisted
2. test_smoke_config_loading(): Load config from .env.example
3. test_smoke_orchestrator_init(): Orchestrator creates without error
4. test_smoke_validation_runs(): validate_environment() returns status
5. test_smoke_task_execution_claude(): Execute simple task via Claude Code adapter (current session returns instructions)

These are integration smoke tests - they verify components work together.
  </action>
  <verify>
Run: `pytest tests/orchestrator/ -v` - all tests pass
  </verify>
  <done>Pre-flight validation integrated into orchestrator, blocks stub adapters, all tests pass</done>
</task>

</tasks>

<verification>
1. `from ta_lab2.tools.ai_orchestrator import AdapterValidator` works
2. ChatGPT adapter (stub) fails pre-flight validation with helpful message
3. Claude Code adapter passes pre-flight validation
4. Orchestrator.execute() with pre_flight=True blocks stub adapters
5. `pytest tests/orchestrator/ -v` passes all validation and smoke tests
6. Error messages explain what's available and what's needed
</verification>

<success_criteria>
- ORCH-11 fully implemented: Pre-flight validation prevents routing to unimplemented adapters
- Each adapter accurately reports its implementation status
- Validation provides helpful feedback about requirements
- Smoke tests prove end-to-end infrastructure works
- All Phase 1 success criteria verified by tests
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-quota-management/01-03-SUMMARY.md`
</output>
