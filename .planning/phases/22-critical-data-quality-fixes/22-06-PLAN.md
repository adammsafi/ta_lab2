---
phase: 22-critical-data-quality-fixes
plan: 06
type: execute
wave: 2
depends_on: ["22-01", "22-02"]
files_modified:
  - tests/test_bar_validation.py
  - tests/test_ema_validation.py
  - tests/conftest.py
  - .github/workflows/test.yml
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Automated tests verify OHLC invariant enforcement"
    - "Automated tests verify EMA output validation"
    - "Automated tests verify backfill detection"
    - "CI runs tests on every PR and blocks merge on failure"
  artifacts:
    - path: "tests/test_bar_validation.py"
      provides: "Bar validation test suite"
      contains: "test_ohlc_invariants"
    - path: "tests/test_ema_validation.py"
      provides: "EMA validation test suite"
      contains: "test_ema_bounds_validation"
    - path: ".github/workflows/test.yml"
      provides: "CI workflow configuration"
      contains: "pytest"
  key_links:
    - from: "CI workflow"
      to: "pytest"
      via: "test execution"
      pattern: "pytest.*tests/"
---

<objective>
Create automated validation test suite (GAP-C04).

Purpose: GAP-C04 identified that all validation is manual - no automated tests to catch regressions in OHLC checks, EMA validation, or backfill detection. This plan creates a comprehensive test suite that runs in CI.

Output: Test files for bar validation, EMA validation, and backfill detection + CI integration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/22-critical-data-quality-fixes/22-CONTEXT.md

# Existing test files
@tests/test_bar_contract.py
@tests/test_bar_ohlc_correctness.py
@tests/conftest.py

# Features implemented in earlier plans
@.planning/phases/22-critical-data-quality-fixes/22-01-SUMMARY.md (reject tables)
@.planning/phases/22-critical-data-quality-fixes/22-02-SUMMARY.md (EMA validation)

# Phase 21 analysis
@.planning/phases/21-comprehensive-review/deliverables/gap-analysis.md (GAP-C04 details)
@.planning/phases/21-comprehensive-review/deliverables/validation-points.md (validation test cases)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create bar validation test suite</name>
  <files>tests/test_bar_validation.py</files>
  <action>
Create comprehensive bar validation tests:

```python
"""
Bar validation test suite for Phase 22 data quality features.

Tests:
- OHLC invariant enforcement (high >= low, high >= max(open, close), etc.)
- NULL rejection (OHLCV columns must not be NULL)
- Quality flags (is_partial_end, is_missing_days set correctly)
- Backfill detection (rebuild triggered when historical data appears)
- Reject table logging (violations logged before repair)

Test data strategy: Hybrid
- Unit tests: Small generated fixtures with targeted scenarios
- Integration tests: Skip if database not available
"""

import pytest
import pandas as pd
import numpy as np
from unittest.mock import Mock, patch

# Mark tests that need database
DB_AVAILABLE = False  # Updated by conftest.py

# =============================================================================
# OHLC Invariant Tests
# =============================================================================

class TestOHLCInvariants:
    """Test OHLC invariant enforcement."""

    def test_high_greater_equal_low(self):
        """High must be >= low. If violated, swap occurs."""
        # Create row with high < low
        row = {"high": 100, "low": 150, "open": 120, "close": 130}
        # Call detect_ohlc_violations
        # Assert 'high_lt_low' violation detected
        pass

    def test_high_greater_equal_oc_max(self):
        """High must be >= max(open, close)."""
        # Create row with high < max(open, close)
        row = {"high": 100, "low": 80, "open": 120, "close": 90}
        # Assert 'high_lt_oc_max' violation detected
        pass

    def test_low_less_equal_oc_min(self):
        """Low must be <= min(open, close)."""
        # Create row with low > min(open, close)
        row = {"high": 150, "low": 120, "open": 100, "close": 130}
        # Assert 'low_gt_oc_min' violation detected
        pass

    def test_valid_ohlc_no_violations(self):
        """Valid OHLC should have no violations."""
        row = {"high": 150, "low": 80, "open": 100, "close": 120}
        # Assert no violations detected
        pass

    def test_multiple_violations_detected(self):
        """Multiple violations in same row should all be detected."""
        row = {"high": 50, "low": 200, "open": 100, "close": 150}
        # Assert multiple violations
        pass


class TestNULLRejection:
    """Test NULL value rejection."""

    def test_null_open_rejected(self):
        """NULL open value should be rejected."""
        pass

    def test_null_high_rejected(self):
        """NULL high value should be rejected."""
        pass

    def test_null_low_rejected(self):
        """NULL low value should be rejected."""
        pass

    def test_null_close_rejected(self):
        """NULL close value should be rejected."""
        pass

    def test_null_volume_rejected(self):
        """NULL volume value should be rejected."""
        pass


class TestRejectTableLogging:
    """Test that violations are logged to reject tables."""

    def test_violations_logged_before_repair(self):
        """Violations should be logged BEFORE enforce_ohlc_sanity repairs them."""
        # This tests the integration added in 22-01
        pass

    def test_reject_record_has_original_values(self):
        """Reject records should contain original (pre-repair) values."""
        pass

    def test_reject_record_has_violation_and_repair(self):
        """Reject records should have both violation_type and repair_action."""
        pass


class TestQualityFlags:
    """Test quality flag assignment."""

    def test_is_partial_end_set_for_incomplete_period(self):
        """is_partial_end=TRUE for periods with missing end days."""
        pass

    def test_is_missing_days_set_for_gaps(self):
        """is_missing_days=TRUE when source has gaps in date sequence."""
        pass

    def test_count_days_matches_actual(self):
        """count_days should match actual number of source days."""
        pass


class TestBackfillDetection:
    """Test backfill detection logic."""

    def test_backfill_detected_when_min_before_state(self):
        """Backfill detected when source MIN(ts) < daily_min_seen."""
        # This tests the logic added in 22-03
        pass

    def test_no_backfill_when_min_after_state(self):
        """No backfill when source MIN(ts) >= daily_min_seen."""
        pass

    def test_backfill_triggers_rebuild(self):
        """Backfill detection should trigger DELETE + rebuild."""
        pass


# =============================================================================
# Integration Tests (skip if no DB)
# =============================================================================

@pytest.mark.skipif(not DB_AVAILABLE, reason="Database not available")
class TestBarValidationIntegration:
    """Integration tests with real database."""

    def test_1d_builder_rejects_invalid_ohlc(self):
        """End-to-end: Run 1D builder with invalid data, verify rejects."""
        pass

    def test_multi_tf_builder_logs_repairs(self):
        """End-to-end: Run multi-TF builder with --keep-rejects, verify logging."""
        pass
```

Follow existing test patterns in tests/test_bar_contract.py.
Use pytest fixtures for test data.
Mock database connections for unit tests.
  </action>
  <verify>
```bash
pytest tests/test_bar_validation.py -v --collect-only
# Should show all test classes and methods

pytest tests/test_bar_validation.py -v -k "OHLC" --tb=short
# Run OHLC tests specifically
```
  </verify>
  <done>
- TestOHLCInvariants class tests all 3 invariant types
- TestNULLRejection class tests all 5 OHLCV NULL cases
- TestRejectTableLogging class tests reject table integration
- TestQualityFlags class tests flag assignment
- TestBackfillDetection class tests backfill logic
- Integration tests skip gracefully without database
  </done>
</task>

<task type="auto">
  <name>Task 2: Create EMA validation test suite</name>
  <files>tests/test_ema_validation.py</files>
  <action>
Create comprehensive EMA validation tests:

```python
"""
EMA validation test suite for Phase 22 EMA output validation.

Tests:
- Price bounds validation (0.5x to 2x recent min/max)
- Statistical bounds validation (mean +/- 3 std dev)
- NaN/infinity rejection
- Negative value detection
- Reject table logging
- Bounds computation

Test data strategy: Hybrid
- Unit tests: Generated EMA values with known bounds
- Integration tests: Skip if database not available
"""

import pytest
import pandas as pd
import numpy as np
from unittest.mock import Mock, patch

DB_AVAILABLE = False  # Updated by conftest.py


# =============================================================================
# EMA Bounds Computation Tests
# =============================================================================

class TestPriceBounds:
    """Test price-based bounds computation."""

    def test_price_bounds_multipliers(self):
        """Bounds should be 0.5x min and 2x max of recent prices."""
        # Given: recent prices [100, 150, 80, 120]
        # Expected: bounds = (0.5 * 80, 2 * 150) = (40, 300)
        pass

    def test_price_bounds_with_lookback(self):
        """Bounds should use specified lookback window."""
        pass

    def test_price_bounds_empty_source(self):
        """Should handle empty source data gracefully."""
        pass


class TestStatisticalBounds:
    """Test statistical bounds computation."""

    def test_statistical_bounds_3_std(self):
        """Bounds should be mean +/- 3 standard deviations."""
        # Given: historical EMAs with known mean=100, std=10
        # Expected: bounds = (70, 130)
        pass

    def test_statistical_bounds_insufficient_history(self):
        """Should return None for bounds with <10 historical values."""
        pass

    def test_statistical_bounds_lookback_count(self):
        """Bounds should use specified lookback count."""
        pass


# =============================================================================
# EMA Validation Tests
# =============================================================================

class TestEMAValidation:
    """Test EMA output validation logic."""

    def test_nan_detected(self):
        """NaN EMA values should be detected as violations."""
        df = pd.DataFrame({"ema": [100, np.nan, 150]})
        # Assert 'nan' violation for row 1
        pass

    def test_infinity_detected(self):
        """Infinity EMA values should be detected as violations."""
        df = pd.DataFrame({"ema": [100, np.inf, 150]})
        # Assert 'infinity' violation for row 1
        pass

    def test_negative_detected(self):
        """Negative EMA values should be detected as violations."""
        df = pd.DataFrame({"ema": [100, -50, 150]})
        # Assert 'negative' violation for row 1
        pass

    def test_out_of_price_bounds_detected(self):
        """EMAs outside price bounds should be detected."""
        # Given: price bounds (50, 200), EMA=300
        # Assert 'out_of_price_bounds' violation
        pass

    def test_out_of_statistical_bounds_detected(self):
        """EMAs outside statistical bounds should be detected."""
        # Given: statistical bounds (70, 130), EMA=200
        # Assert 'out_of_statistical_bounds' violation
        pass

    def test_valid_ema_no_violations(self):
        """Valid EMAs should have no violations."""
        # Given: EMA within both bounds
        # Assert no violations
        pass

    def test_all_emas_written_despite_violations(self):
        """Warn-and-continue: All EMAs written even if invalid."""
        # Verify validate_ema_output returns ALL rows (not filtered)
        pass


class TestEMARejectLogging:
    """Test EMA reject table logging."""

    def test_violations_logged_to_table(self):
        """Violations should be inserted to ema_rejects table."""
        pass

    def test_violations_logged_to_app_logs(self):
        """Violations should also be logged at WARNING level."""
        pass

    def test_reject_record_has_bounds_info(self):
        """Reject records should include bounds_info JSON."""
        pass


# =============================================================================
# Integration Tests
# =============================================================================

@pytest.mark.skipif(not DB_AVAILABLE, reason="Database not available")
class TestEMAValidationIntegration:
    """Integration tests with real database."""

    def test_ema_refresher_validates_output(self):
        """End-to-end: Run EMA refresher, verify validation runs."""
        pass

    def test_ema_rejects_table_populated(self):
        """End-to-end: Verify ema_rejects table has violations (if any)."""
        pass
```

Import validation functions from base_ema_refresher.py.
Use pytest parametrize for multiple test cases.
  </action>
  <verify>
```bash
pytest tests/test_ema_validation.py -v --collect-only
# Should show all test classes and methods

pytest tests/test_ema_validation.py -v -k "bounds" --tb=short
# Run bounds tests specifically
```
  </verify>
  <done>
- TestPriceBounds class tests price-based bounds computation
- TestStatisticalBounds class tests statistical bounds computation
- TestEMAValidation class tests all violation detection types
- TestEMARejectLogging class tests reject table integration
- Integration tests skip gracefully without database
  </done>
</task>

<task type="auto">
  <name>Task 3: Add CI integration and fixtures</name>
  <files>
tests/conftest.py
.github/workflows/test.yml
  </files>
  <action>
Update test configuration for CI integration:

1. Update conftest.py with database availability check:
```python
import pytest
import os

# Database availability check
DB_AVAILABLE = False

def pytest_configure(config):
    """Check database availability at test session start."""
    global DB_AVAILABLE
    db_url = os.environ.get("DATABASE_URL")
    if db_url:
        try:
            from sqlalchemy import create_engine, text
            engine = create_engine(db_url)
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            DB_AVAILABLE = True
        except Exception:
            DB_AVAILABLE = False

    # Make available to test modules
    import tests.test_bar_validation as tbv
    import tests.test_ema_validation as tev
    tbv.DB_AVAILABLE = DB_AVAILABLE
    tev.DB_AVAILABLE = DB_AVAILABLE


@pytest.fixture
def sample_ohlc_data():
    """Sample OHLC data for bar validation tests."""
    return [
        {"id": 1, "timestamp": "2024-01-01", "open": 100, "high": 150, "low": 80, "close": 120, "volume": 1000},
        {"id": 1, "timestamp": "2024-01-02", "open": 120, "high": 160, "low": 100, "close": 140, "volume": 1200},
        # Add more rows as needed
    ]


@pytest.fixture
def sample_ema_data():
    """Sample EMA data for validation tests."""
    return [
        {"id": 1, "tf": "1D", "period": 10, "timestamp": "2024-01-01", "ema": 100.5, "close": 100},
        {"id": 1, "tf": "1D", "period": 10, "timestamp": "2024-01-02", "ema": 102.3, "close": 105},
        # Add more rows as needed
    ]


@pytest.fixture
def mock_engine():
    """Mock SQLAlchemy engine for unit tests."""
    from unittest.mock import Mock
    engine = Mock()
    return engine
```

2. Update or create .github/workflows/test.yml:
```yaml
name: Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install -e ".[dev]"

      - name: Run unit tests
        run: |
          pytest tests/ -v --tb=short -m "not integration" --ignore=tests/test_connectivity.py

      - name: Run validation tests (no DB)
        run: |
          pytest tests/test_bar_validation.py tests/test_ema_validation.py -v --tb=short

      # Optional: Run integration tests with test database
      # - name: Run integration tests
      #   env:
      #     DATABASE_URL: ${{ secrets.TEST_DATABASE_URL }}
      #   run: |
      #     pytest tests/ -v --tb=short -m "integration"
```

3. Add pytest markers to pyproject.toml or pytest.ini:
```ini
[tool.pytest.ini_options]
markers = [
    "integration: marks tests as integration tests (requires database)",
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
]
```
  </action>
  <verify>
```bash
# Verify conftest.py has fixtures
grep -E "def sample_ohlc_data|def sample_ema_data|DB_AVAILABLE" tests/conftest.py

# Verify CI workflow exists
cat .github/workflows/test.yml | grep -E "pytest|test_bar_validation|test_ema_validation"

# Run tests locally
pytest tests/test_bar_validation.py tests/test_ema_validation.py -v --tb=short
```
  </verify>
  <done>
- conftest.py has DB_AVAILABLE check for integration test skipping
- conftest.py provides sample_ohlc_data and sample_ema_data fixtures
- conftest.py provides mock_engine fixture for unit tests
- CI workflow runs unit tests on every PR
- CI workflow runs validation tests
- Integration tests skip gracefully when DATABASE_URL not set
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Test discovery:
```bash
pytest tests/test_bar_validation.py tests/test_ema_validation.py --collect-only
# Should list all test classes and methods
```

2. Unit tests pass without database:
```bash
# Unset DATABASE_URL to simulate CI without DB
unset DATABASE_URL
pytest tests/test_bar_validation.py tests/test_ema_validation.py -v --tb=short
# Integration tests should skip, unit tests should pass
```

3. CI workflow validation:
```bash
# Validate YAML syntax
python -c "import yaml; yaml.safe_load(open('.github/workflows/test.yml'))"
```

4. Test coverage check (optional):
```bash
pytest tests/test_bar_validation.py tests/test_ema_validation.py --cov=ta_lab2.scripts.bars --cov=ta_lab2.scripts.emas --cov-report=term-missing
```
</verification>

<success_criteria>
1. test_bar_validation.py tests OHLC invariants, NULL rejection, quality flags, backfill, rejects
2. test_ema_validation.py tests price bounds, statistical bounds, NaN/infinity, rejects
3. conftest.py provides fixtures and DB availability check
4. CI workflow runs tests on every PR
5. Unit tests pass without database access
6. Integration tests skip gracefully when database not available
</success_criteria>

<output>
After completion, create `.planning/phases/22-critical-data-quality-fixes/22-06-SUMMARY.md`
</output>
