---
phase: 22-critical-data-quality-fixes
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ta_lab2/scripts/bars/refresh_cmc_price_bars_1d.py
  - sql/ddl/create_cmc_price_bars_1d_state.sql
autonomous: true
user_setup: []

must_haves:
  truths:
    - "1D builder detects backfilled historical data (before first processed date)"
    - "Backfill triggers full rebuild for affected asset ID"
    - "daily_min_seen column tracks earliest timestamp ever seen"
    - "No data corruption from undetected backfills"
  artifacts:
    - path: "src/ta_lab2/scripts/bars/refresh_cmc_price_bars_1d.py"
      provides: "Backfill detection logic"
      contains: "daily_min_seen"
    - path: "sql/ddl/create_cmc_price_bars_1d_state.sql"
      provides: "State table DDL with daily_min_seen column"
      contains: "daily_min_seen"
  key_links:
    - from: "refresh_cmc_price_bars_1d.py"
      to: "cmc_price_bars_1d_state"
      via: "daily_min_seen comparison"
      pattern: "daily_min_ts.*<.*daily_min_seen"
---

<objective>
Add backfill detection to 1D bar builder (GAP-C03 Part 1: Simple fix).

Purpose: GAP-C03 identified that 1D builder has no backfill detection. If price_histories7 backfills historical data before the first processed date, 1D bars never rebuild, causing bar_seq corruption. Multi-TF builders already have this via daily_min_seen - 1D needs parity.

Output: daily_min_seen column in state table + backfill detection logic in 1D builder.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/22-critical-data-quality-fixes/22-CONTEXT.md

# Key source files
@src/ta_lab2/scripts/bars/refresh_cmc_price_bars_1d.py
@src/ta_lab2/scripts/bars/refresh_cmc_price_bars_multi_tf.py (reference for existing backfill detection)

# Phase 21 analysis
@.planning/phases/21-comprehensive-review/deliverables/gap-analysis.md (GAP-C03 details)
@.planning/phases/21-comprehensive-review/deliverables/incremental-refresh.md (backfill detection mechanics)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add daily_min_seen column to 1D state table</name>
  <files>sql/ddl/create_cmc_price_bars_1d_state.sql</files>
  <action>
Update the 1D state table DDL to include daily_min_seen column:

1. Find or create the DDL file for cmc_price_bars_1d_state table.

2. Add daily_min_seen column:
```sql
ALTER TABLE public.cmc_price_bars_1d_state
ADD COLUMN IF NOT EXISTS daily_min_seen TIMESTAMPTZ;

-- Backfill existing rows: set daily_min_seen = last_src_ts for safety
-- (conservative: assumes no historical data before what we've processed)
UPDATE public.cmc_price_bars_1d_state
SET daily_min_seen = last_src_ts
WHERE daily_min_seen IS NULL;

-- Comment explaining purpose
COMMENT ON COLUMN public.cmc_price_bars_1d_state.daily_min_seen IS
'Earliest timestamp ever seen in price_histories7 for this ID.
Used for backfill detection: if MIN(timestamp) < daily_min_seen, historical data
was backfilled and full rebuild is required to maintain bar_seq integrity.';
```

3. If DDL file doesn't exist, create it with full CREATE TABLE statement including:
- Existing columns (id, last_src_ts, updated_at, etc.)
- New daily_min_seen column
- Primary key and constraints

Note: The actual column addition should happen via the script at runtime (see Task 2).
This DDL file documents the schema for reference and can be used for fresh installs.
  </action>
  <verify>
```bash
# Check DDL file exists and contains daily_min_seen
cat sql/ddl/create_cmc_price_bars_1d_state.sql | grep -i "daily_min_seen"
```
  </verify>
  <done>
- DDL file documents cmc_price_bars_1d_state schema with daily_min_seen column
- Column purpose documented in comment
- Backfill migration query provided for existing data
  </done>
</task>

<task type="auto">
  <name>Task 2: Add backfill detection logic to 1D builder</name>
  <files>src/ta_lab2/scripts/bars/refresh_cmc_price_bars_1d.py</files>
  <action>
Modify refresh_cmc_price_bars_1d.py to detect and handle backfills:

1. Add schema migration at startup (auto-add column if missing):
```python
def ensure_state_schema(engine: Engine):
    """Ensure state table has daily_min_seen column."""
    with engine.begin() as conn:
        # Check if column exists
        result = conn.execute(text("""
            SELECT column_name FROM information_schema.columns
            WHERE table_name = 'cmc_price_bars_1d_state'
            AND column_name = 'daily_min_seen'
        """))
        if not result.fetchone():
            # Add column
            conn.execute(text("""
                ALTER TABLE public.cmc_price_bars_1d_state
                ADD COLUMN daily_min_seen TIMESTAMPTZ;
                UPDATE public.cmc_price_bars_1d_state
                SET daily_min_seen = last_src_ts
                WHERE daily_min_seen IS NULL;
            """))
            logger.info("Added daily_min_seen column to cmc_price_bars_1d_state")
```

2. Modify state loading to include daily_min_seen:
```python
# In load_state or equivalent
state = {
    'id': row.id,
    'last_src_ts': row.last_src_ts,
    'daily_min_seen': row.daily_min_seen,  # NEW
    ...
}
```

3. Add backfill detection before processing:
```python
def check_for_backfill(engine: Engine, id: int, state: dict) -> bool:
    """
    Check if historical data was backfilled before first processed date.

    Returns True if backfill detected (rebuild required).
    """
    if state is None or state.get('daily_min_seen') is None:
        return False  # No state = first run, not a backfill

    # Query earliest timestamp in source
    with engine.connect() as conn:
        result = conn.execute(text("""
            SELECT MIN(timestamp) as daily_min_ts
            FROM price_histories7
            WHERE id = :id
        """), {"id": id})
        row = result.fetchone()
        if row and row.daily_min_ts:
            if row.daily_min_ts < state['daily_min_seen']:
                logger.warning(
                    f"Backfill detected for id={id}: "
                    f"source min={row.daily_min_ts}, state min={state['daily_min_seen']}"
                )
                return True
    return False
```

4. Handle backfill with full rebuild:
```python
def handle_backfill(engine: Engine, id: int):
    """Delete bars and reset state for full rebuild."""
    with engine.begin() as conn:
        # Delete all bars for this ID
        conn.execute(text("""
            DELETE FROM public.cmc_price_bars_1d WHERE id = :id
        """), {"id": id})
        # Delete state (will be recreated during processing)
        conn.execute(text("""
            DELETE FROM public.cmc_price_bars_1d_state WHERE id = :id
        """), {"id": id})
    logger.info(f"Backfill rebuild: deleted bars and state for id={id}")
```

5. Update state save to include daily_min_seen:
```python
# After processing, update state with earliest timestamp seen
daily_min_seen = min(state.get('daily_min_seen', new_min_ts), new_min_ts)
# Include in INSERT/UPDATE state query
```

6. Add CLI argument for forced rebuild:
```python
parser.add_argument("--rebuild-if-backfill", action="store_true", default=True,
    help="Trigger full rebuild if backfill detected (default: True)")
parser.add_argument("--no-rebuild-if-backfill", action="store_false", dest="rebuild_if_backfill",
    help="Skip backfill detection and rebuild")
```

Integration point: Insert backfill check BEFORE the main processing loop, after state loading.
  </action>
  <verify>
```bash
# Verify backfill detection functions exist
grep -n "check_for_backfill\|handle_backfill\|daily_min_seen" src/ta_lab2/scripts/bars/refresh_cmc_price_bars_1d.py

# Verify CLI argument
python src/ta_lab2/scripts/bars/refresh_cmc_price_bars_1d.py --help | grep -E "rebuild-if-backfill"
```
  </verify>
  <done>
- ensure_state_schema adds daily_min_seen column if missing (auto-migration)
- check_for_backfill queries source MIN(timestamp) and compares to state
- handle_backfill deletes bars and state for full rebuild
- State save includes daily_min_seen (earliest timestamp ever processed)
- --rebuild-if-backfill CLI argument (default True)
  </done>
</task>

<task type="auto">
  <name>Task 3: Test backfill detection</name>
  <files>tests/test_bar_contract.py</files>
  <action>
Add test cases for backfill detection to existing test file:

1. Add test for backfill detection logic:
```python
def test_check_for_backfill_detects_historical_data():
    """
    Test that check_for_backfill returns True when source has data
    earlier than daily_min_seen in state.
    """
    # This can be a unit test with mocked state and source query
    pass

def test_handle_backfill_deletes_bars_and_state():
    """
    Test that handle_backfill properly clears bars and state for rebuild.
    """
    # This can be a unit test verifying DELETE queries are issued
    pass

def test_daily_min_seen_updated_after_processing():
    """
    Test that daily_min_seen is updated in state after processing new data.
    """
    pass
```

2. Add integration test if database is available:
```python
@pytest.mark.skipif(not DB_AVAILABLE, reason="Database not available")
def test_1d_builder_backfill_integration():
    """
    Integration test: Insert historical data, verify rebuild triggered.

    Steps:
    1. Run 1D builder for test ID
    2. Insert historical data before first processed date
    3. Run 1D builder again
    4. Verify backfill detected and rebuild happened
    """
    pass
```

3. Use existing test patterns in test_bar_contract.py for consistency.

Note: Tests should be skippable if database is not available (CI friendliness).
  </action>
  <verify>
```bash
# Run the new tests
pytest tests/test_bar_contract.py -v -k "backfill" --tb=short

# If no database, verify tests are skipped gracefully
pytest tests/test_bar_contract.py -v -k "backfill" --collect-only
```
  </verify>
  <done>
- Unit tests for check_for_backfill and handle_backfill functions
- Integration test for full backfill detection flow (skippable without DB)
- Tests follow existing patterns in test_bar_contract.py
- All tests pass (or skip gracefully without database)
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Schema verification:
```sql
-- Check daily_min_seen column exists
SELECT column_name, data_type
FROM information_schema.columns
WHERE table_name = 'cmc_price_bars_1d_state'
AND column_name = 'daily_min_seen';
```

2. Functional test (manual or automated):
```bash
# Run 1D builder for a test ID
python src/ta_lab2/scripts/bars/refresh_cmc_price_bars_1d.py --ids 1

# Check state includes daily_min_seen
psql -c "SELECT id, last_src_ts, daily_min_seen FROM cmc_price_bars_1d_state WHERE id = 1;"
```

3. Backfill simulation (if test data available):
```sql
-- Simulate backfill by inserting old data (in test environment only)
INSERT INTO price_histories7 (id, timestamp, ...) VALUES (1, '2020-01-01', ...);

-- Then run builder and verify rebuild
-- Builder should log: "Backfill detected for id=1"
```
</verification>

<success_criteria>
1. daily_min_seen column exists in cmc_price_bars_1d_state table
2. Auto-migration adds column to existing installations (ensure_state_schema)
3. Backfill detected when source MIN(timestamp) < daily_min_seen
4. Backfill triggers DELETE + full rebuild (handle_backfill)
5. State includes daily_min_seen after processing
6. Tests verify detection and handling logic
</success_criteria>

<output>
After completion, create `.planning/phases/22-critical-data-quality-fixes/22-03-SUMMARY.md`
</output>
