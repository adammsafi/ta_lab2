---
phase: 15-economic-data-strategy
plan: 04
type: execute
wave: 2
depends_on: ["15-03"]
files_modified:
  - src/ta_lab2/integrations/economic/rate_limiter.py
  - src/ta_lab2/integrations/economic/cache.py
  - src/ta_lab2/integrations/economic/circuit_breaker.py
  - src/ta_lab2/integrations/economic/quality.py
  - src/ta_lab2/integrations/economic/fred_provider.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Rate limiter prevents exceeding FRED API limits (120 calls/min)"
    - "TTL cache avoids repeated queries for same data"
    - "Circuit breaker stops retrying when service is down"
    - "Data quality framework validates FRED data"
  artifacts:
    - path: "src/ta_lab2/integrations/economic/rate_limiter.py"
      provides: "Rate limiting for API calls"
      contains: "class RateLimiter"
    - path: "src/ta_lab2/integrations/economic/cache.py"
      provides: "TTL caching for economic data"
      contains: "class EconomicDataCache"
    - path: "src/ta_lab2/integrations/economic/circuit_breaker.py"
      provides: "Circuit breaker for API resilience"
      contains: "class CircuitBreaker"
    - path: "src/ta_lab2/integrations/economic/quality.py"
      provides: "Data quality validation framework"
      contains: "class QualityValidator"
  key_links:
    - from: "fred_provider.py"
      to: "rate_limiter.py, cache.py, circuit_breaker.py"
      via: "composition"
      pattern: "self._rate_limiter"
---

<objective>
Add rate limiting, caching, circuit breaker, and data quality framework

Purpose: Implement production-ready reliability features per context requirements. All features must be working implementations (not stubs): rate limiter (120 calls/min), TTL cache, circuit breaker (exponential backoff), and comprehensive data quality validation.

Output: Four new modules with working implementations, FredProvider updated to use them.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-economic-data-strategy/15-CONTEXT.md

# Existing cache pattern
@src/ta_lab2/utils/cache.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create rate_limiter.py with working rate limiting</name>
  <files>src/ta_lab2/integrations/economic/rate_limiter.py</files>
  <action>
Create rate_limiter.py with working rate limiting (not stub):

```python
"""Rate limiting for economic data API calls.

Provides token bucket rate limiting to prevent exceeding API limits.
FRED API allows 120 requests per minute by default.
"""
import threading
import time
from typing import Optional


class RateLimiter:
    """Token bucket rate limiter for API calls.

    Implements a token bucket algorithm that refills tokens at a steady
    rate. Callers can acquire tokens (blocking or non-blocking) before
    making API requests.

    Attributes:
        max_tokens: Maximum tokens in bucket (burst capacity)
        refill_rate: Tokens added per second
        tokens: Current available tokens

    Example:
        >>> limiter = RateLimiter(max_tokens=120, refill_period=60)
        >>> limiter.acquire()  # Blocks if rate limit exceeded
        >>> # Make API call...
    """

    def __init__(
        self,
        max_tokens: int = 120,
        refill_period: float = 60.0,
    ):
        """Initialize rate limiter.

        Args:
            max_tokens: Maximum tokens (requests) allowed in the bucket.
                        Default 120 for FRED API.
            refill_period: Seconds to refill all tokens. Default 60 seconds.
        """
        self.max_tokens = max_tokens
        self.refill_rate = max_tokens / refill_period  # tokens per second
        self._tokens = float(max_tokens)
        self._last_refill = time.monotonic()
        self._lock = threading.Lock()

    def _refill(self) -> None:
        """Refill tokens based on elapsed time."""
        now = time.monotonic()
        elapsed = now - self._last_refill
        self._tokens = min(
            self.max_tokens,
            self._tokens + elapsed * self.refill_rate
        )
        self._last_refill = now

    def acquire(self, tokens: int = 1, blocking: bool = True, timeout: Optional[float] = None) -> bool:
        """Acquire tokens from the bucket.

        Args:
            tokens: Number of tokens to acquire (usually 1 per API call)
            blocking: If True, wait for tokens to become available
            timeout: Maximum seconds to wait (None = wait forever)

        Returns:
            True if tokens acquired, False if timed out or non-blocking fail

        Example:
            >>> if limiter.acquire(blocking=False):
            ...     # Make API call
            ... else:
            ...     print("Rate limited, try later")
        """
        deadline = None if timeout is None else time.monotonic() + timeout

        while True:
            with self._lock:
                self._refill()

                if self._tokens >= tokens:
                    self._tokens -= tokens
                    return True

                if not blocking:
                    return False

                # Calculate wait time for enough tokens
                tokens_needed = tokens - self._tokens
                wait_time = tokens_needed / self.refill_rate

            # Check timeout
            if deadline is not None:
                remaining = deadline - time.monotonic()
                if remaining <= 0:
                    return False
                wait_time = min(wait_time, remaining)

            time.sleep(min(wait_time, 0.1))  # Sleep in small increments

    @property
    def available_tokens(self) -> float:
        """Current available tokens."""
        with self._lock:
            self._refill()
            return self._tokens

    def reset(self) -> None:
        """Reset to full capacity."""
        with self._lock:
            self._tokens = float(self.max_tokens)
            self._last_refill = time.monotonic()


# Default FRED rate limiter (120 requests per minute)
_fred_limiter: Optional[RateLimiter] = None


def get_fred_rate_limiter() -> RateLimiter:
    """Get or create the global FRED rate limiter.

    Returns a singleton rate limiter configured for FRED API limits
    (120 requests per minute).

    Returns:
        RateLimiter instance
    """
    global _fred_limiter
    if _fred_limiter is None:
        _fred_limiter = RateLimiter(max_tokens=120, refill_period=60.0)
    return _fred_limiter
```
  </action>
  <verify>
Run: `python -c "from ta_lab2.integrations.economic.rate_limiter import RateLimiter; r=RateLimiter(10, 1); print(f'Acquired: {r.acquire()}'); print(f'Available: {r.available_tokens}')"` shows rate limiter working
  </verify>
  <done>rate_limiter.py created with working token bucket implementation</done>
</task>

<task type="auto">
  <name>Task 2: Create cache.py with TTL caching</name>
  <files>src/ta_lab2/integrations/economic/cache.py</files>
  <action>
Create cache.py with working TTL cache (not stub):

```python
"""TTL caching for economic data.

Provides in-memory caching with time-to-live expiration to avoid
repeated API calls for the same data.
"""
import hashlib
import json
import threading
import time
from dataclasses import dataclass
from typing import Optional, Any, Dict


@dataclass
class CacheEntry:
    """Cache entry with value and expiration time."""
    value: Any
    expires_at: float
    created_at: float

    @property
    def is_expired(self) -> bool:
        """Check if entry has expired."""
        return time.time() > self.expires_at


class EconomicDataCache:
    """In-memory TTL cache for economic data.

    Provides thread-safe caching with configurable TTL (time-to-live).
    Cache keys are generated from series ID and parameters.

    Attributes:
        default_ttl: Default time-to-live in seconds
        max_size: Maximum cache entries (LRU eviction)

    Example:
        >>> cache = EconomicDataCache(default_ttl=3600)  # 1 hour TTL
        >>> cache.set("FEDFUNDS", data)
        >>> result = cache.get("FEDFUNDS")
        >>> if result is not None:
        ...     print("Cache hit!")
    """

    def __init__(
        self,
        default_ttl: float = 3600.0,
        max_size: int = 1000,
    ):
        """Initialize cache.

        Args:
            default_ttl: Default TTL in seconds (1 hour default)
            max_size: Maximum entries before LRU eviction
        """
        self.default_ttl = default_ttl
        self.max_size = max_size
        self._cache: Dict[str, CacheEntry] = {}
        self._access_order: list = []  # For LRU tracking
        self._lock = threading.Lock()

    @staticmethod
    def _make_key(series_id: str, **params) -> str:
        """Generate cache key from series ID and parameters."""
        key_data = {"series_id": series_id, **params}
        key_json = json.dumps(key_data, sort_keys=True, default=str)
        return hashlib.md5(key_json.encode()).hexdigest()

    def get(self, series_id: str, **params) -> Optional[Any]:
        """Get value from cache.

        Args:
            series_id: Series identifier
            **params: Additional parameters (start_date, end_date, etc.)

        Returns:
            Cached value if present and not expired, None otherwise
        """
        key = self._make_key(series_id, **params)

        with self._lock:
            entry = self._cache.get(key)
            if entry is None:
                return None

            if entry.is_expired:
                del self._cache[key]
                if key in self._access_order:
                    self._access_order.remove(key)
                return None

            # Update LRU order
            if key in self._access_order:
                self._access_order.remove(key)
            self._access_order.append(key)

            return entry.value

    def set(
        self,
        series_id: str,
        value: Any,
        ttl: Optional[float] = None,
        **params
    ) -> None:
        """Set value in cache.

        Args:
            series_id: Series identifier
            value: Value to cache
            ttl: Optional TTL override in seconds
            **params: Additional parameters for cache key
        """
        key = self._make_key(series_id, **params)
        ttl = ttl if ttl is not None else self.default_ttl
        now = time.time()

        entry = CacheEntry(
            value=value,
            expires_at=now + ttl,
            created_at=now
        )

        with self._lock:
            # Evict if at capacity
            while len(self._cache) >= self.max_size and self._access_order:
                oldest_key = self._access_order.pop(0)
                self._cache.pop(oldest_key, None)

            self._cache[key] = entry

            # Update LRU order
            if key in self._access_order:
                self._access_order.remove(key)
            self._access_order.append(key)

    def invalidate(self, series_id: str, **params) -> bool:
        """Invalidate a specific cache entry.

        Args:
            series_id: Series identifier
            **params: Additional parameters

        Returns:
            True if entry was removed, False if not found
        """
        key = self._make_key(series_id, **params)

        with self._lock:
            if key in self._cache:
                del self._cache[key]
                if key in self._access_order:
                    self._access_order.remove(key)
                return True
            return False

    def clear(self) -> int:
        """Clear all cache entries.

        Returns:
            Number of entries cleared
        """
        with self._lock:
            count = len(self._cache)
            self._cache.clear()
            self._access_order.clear()
            return count

    def cleanup_expired(self) -> int:
        """Remove all expired entries.

        Returns:
            Number of entries removed
        """
        with self._lock:
            expired_keys = [
                k for k, v in self._cache.items()
                if v.is_expired
            ]
            for key in expired_keys:
                del self._cache[key]
                if key in self._access_order:
                    self._access_order.remove(key)
            return len(expired_keys)

    @property
    def size(self) -> int:
        """Current number of cached entries."""
        return len(self._cache)

    def stats(self) -> Dict[str, Any]:
        """Get cache statistics.

        Returns:
            Dict with size, max_size, default_ttl
        """
        return {
            "size": self.size,
            "max_size": self.max_size,
            "default_ttl": self.default_ttl,
        }


# Global cache instance
_economic_cache: Optional[EconomicDataCache] = None


def get_economic_cache() -> EconomicDataCache:
    """Get or create the global economic data cache.

    Returns:
        EconomicDataCache instance with 1-hour default TTL
    """
    global _economic_cache
    if _economic_cache is None:
        _economic_cache = EconomicDataCache(default_ttl=3600.0, max_size=1000)
    return _economic_cache
```
  </action>
  <verify>
Run: `python -c "from ta_lab2.integrations.economic.cache import EconomicDataCache; c=EconomicDataCache(ttl=1); c.set('TEST', 'value'); print(f'Got: {c.get(\"TEST\")}'); print(f'Size: {c.size}')"` shows cache working
  </verify>
  <done>cache.py created with working TTL cache implementation with LRU eviction</done>
</task>

<task type="auto">
  <name>Task 3: Create circuit_breaker.py and quality.py</name>
  <files>
src/ta_lab2/integrations/economic/circuit_breaker.py
src/ta_lab2/integrations/economic/quality.py
  </files>
  <action>
Create circuit_breaker.py with working circuit breaker:

```python
"""Circuit breaker for API resilience.

Implements the circuit breaker pattern to prevent cascade failures
when external APIs are down or overloaded.
"""
import threading
import time
from enum import Enum
from typing import Optional, Callable, Any
from dataclasses import dataclass


class CircuitState(Enum):
    """Circuit breaker states."""
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failing, rejecting requests
    HALF_OPEN = "half_open"  # Testing if service recovered


@dataclass
class CircuitStats:
    """Circuit breaker statistics."""
    state: CircuitState
    failure_count: int
    success_count: int
    last_failure_time: Optional[float]
    last_success_time: Optional[float]


class CircuitBreaker:
    """Circuit breaker for external service calls.

    Prevents cascade failures by tracking failures and temporarily
    stopping requests when a service is down.

    States:
        CLOSED: Normal operation, requests pass through
        OPEN: Service failing, requests rejected immediately
        HALF_OPEN: Testing if service recovered

    Attributes:
        failure_threshold: Failures before opening circuit
        recovery_timeout: Seconds before trying half-open
        success_threshold: Successes in half-open to close

    Example:
        >>> breaker = CircuitBreaker(failure_threshold=5)
        >>> def call_api():
        ...     # Make API call
        ...     pass
        >>> result = breaker.call(call_api)
    """

    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: float = 60.0,
        success_threshold: int = 2,
    ):
        """Initialize circuit breaker.

        Args:
            failure_threshold: Failures before opening (default 5)
            recovery_timeout: Seconds before half-open (default 60)
            success_threshold: Successes to close from half-open (default 2)
        """
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.success_threshold = success_threshold

        self._state = CircuitState.CLOSED
        self._failure_count = 0
        self._success_count = 0
        self._last_failure_time: Optional[float] = None
        self._last_success_time: Optional[float] = None
        self._lock = threading.Lock()

    @property
    def state(self) -> CircuitState:
        """Current circuit state."""
        with self._lock:
            self._check_state_transition()
            return self._state

    def _check_state_transition(self) -> None:
        """Check if state should transition based on time."""
        if self._state == CircuitState.OPEN:
            if self._last_failure_time is not None:
                elapsed = time.time() - self._last_failure_time
                if elapsed >= self.recovery_timeout:
                    self._state = CircuitState.HALF_OPEN
                    self._success_count = 0

    def record_success(self) -> None:
        """Record a successful call."""
        with self._lock:
            self._last_success_time = time.time()

            if self._state == CircuitState.HALF_OPEN:
                self._success_count += 1
                if self._success_count >= self.success_threshold:
                    self._state = CircuitState.CLOSED
                    self._failure_count = 0
            elif self._state == CircuitState.CLOSED:
                # Reset failure count on success
                self._failure_count = max(0, self._failure_count - 1)

    def record_failure(self) -> None:
        """Record a failed call."""
        with self._lock:
            self._failure_count += 1
            self._last_failure_time = time.time()

            if self._state == CircuitState.HALF_OPEN:
                # Any failure in half-open goes back to open
                self._state = CircuitState.OPEN
            elif self._state == CircuitState.CLOSED:
                if self._failure_count >= self.failure_threshold:
                    self._state = CircuitState.OPEN

    def is_call_permitted(self) -> bool:
        """Check if a call is currently permitted.

        Returns:
            True if call should proceed, False if circuit is open
        """
        with self._lock:
            self._check_state_transition()
            return self._state != CircuitState.OPEN

    def call(
        self,
        func: Callable[..., Any],
        *args,
        fallback: Optional[Callable[..., Any]] = None,
        **kwargs
    ) -> Any:
        """Execute a function through the circuit breaker.

        Args:
            func: Function to call
            *args: Positional arguments for func
            fallback: Optional fallback function if circuit is open
            **kwargs: Keyword arguments for func

        Returns:
            Result of func or fallback

        Raises:
            CircuitOpenError: If circuit is open and no fallback provided
        """
        if not self.is_call_permitted():
            if fallback is not None:
                return fallback(*args, **kwargs)
            raise CircuitOpenError(
                f"Circuit is open. Service failing. "
                f"Recovery in {self._time_until_half_open():.1f}s"
            )

        try:
            result = func(*args, **kwargs)
            self.record_success()
            return result
        except Exception as e:
            self.record_failure()
            raise

    def _time_until_half_open(self) -> float:
        """Time in seconds until circuit goes to half-open."""
        if self._last_failure_time is None:
            return 0.0
        elapsed = time.time() - self._last_failure_time
        return max(0.0, self.recovery_timeout - elapsed)

    def reset(self) -> None:
        """Reset circuit breaker to closed state."""
        with self._lock:
            self._state = CircuitState.CLOSED
            self._failure_count = 0
            self._success_count = 0
            self._last_failure_time = None

    def stats(self) -> CircuitStats:
        """Get circuit breaker statistics."""
        with self._lock:
            self._check_state_transition()
            return CircuitStats(
                state=self._state,
                failure_count=self._failure_count,
                success_count=self._success_count,
                last_failure_time=self._last_failure_time,
                last_success_time=self._last_success_time,
            )


class CircuitOpenError(Exception):
    """Raised when circuit breaker is open."""
    pass
```

Create quality.py with data quality validation:

```python
"""Data quality validation for economic data.

Provides comprehensive validation including null checks, type validation,
statistical outlier detection, and range validation based on historical norms.
"""
import logging
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional, List, Dict, Any

import numpy as np
import pandas as pd

from ta_lab2.integrations.economic.types import EconomicSeries

logger = logging.getLogger(__name__)


@dataclass
class QualityIssue:
    """A single data quality issue."""
    severity: str  # "error", "warning", "info"
    category: str  # "null", "type", "range", "outlier", "gap"
    message: str
    affected_dates: List[datetime] = field(default_factory=list)
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class QualityReport:
    """Quality validation report for a series."""
    series_id: str
    is_valid: bool
    issues: List[QualityIssue] = field(default_factory=list)
    stats: Dict[str, Any] = field(default_factory=dict)
    validated_at: datetime = field(default_factory=datetime.now)

    @property
    def error_count(self) -> int:
        return sum(1 for i in self.issues if i.severity == "error")

    @property
    def warning_count(self) -> int:
        return sum(1 for i in self.issues if i.severity == "warning")


class QualityValidator:
    """Validator for economic time series data.

    Performs comprehensive quality checks:
    - Null/NaN value detection
    - Type validation (numeric)
    - Statistical outlier detection (IQR-based)
    - Range validation (configurable bounds)
    - Gap detection (missing dates)

    Attributes:
        null_threshold: Max proportion of nulls (default 0.05 = 5%)
        outlier_iqr_multiplier: IQR multiplier for outlier detection (default 3.0)

    Example:
        >>> validator = QualityValidator()
        >>> report = validator.validate(series)
        >>> if not report.is_valid:
        ...     for issue in report.issues:
        ...         print(f"{issue.severity}: {issue.message}")
    """

    # Known reasonable ranges for common economic series
    KNOWN_RANGES = {
        "FEDFUNDS": (-1.0, 25.0),      # Fed funds rate (can go slightly negative)
        "UNRATE": (0.0, 30.0),          # Unemployment rate
        "CPIAUCSL": (0.0, 500.0),       # CPI index
        "DGS10": (-2.0, 20.0),          # 10-year treasury
        "DGS2": (-2.0, 20.0),           # 2-year treasury
    }

    def __init__(
        self,
        null_threshold: float = 0.05,
        outlier_iqr_multiplier: float = 3.0,
        check_gaps: bool = True,
    ):
        """Initialize validator.

        Args:
            null_threshold: Max allowed null proportion (default 5%)
            outlier_iqr_multiplier: IQR multiplier for outliers (default 3.0)
            check_gaps: Whether to check for date gaps
        """
        self.null_threshold = null_threshold
        self.outlier_iqr_multiplier = outlier_iqr_multiplier
        self.check_gaps = check_gaps

    def validate(self, series: EconomicSeries) -> QualityReport:
        """Validate an economic series.

        Args:
            series: EconomicSeries to validate

        Returns:
            QualityReport with issues and statistics
        """
        issues: List[QualityIssue] = []
        data = series.data

        # Calculate basic stats
        stats = self._calculate_stats(data)

        # Check for empty data
        if len(data) == 0:
            issues.append(QualityIssue(
                severity="error",
                category="null",
                message="Series has no data",
            ))
            return QualityReport(
                series_id=series.series_id,
                is_valid=False,
                issues=issues,
                stats=stats,
            )

        # Null checks
        null_issues = self._check_nulls(data)
        issues.extend(null_issues)

        # Type validation
        type_issues = self._check_types(data)
        issues.extend(type_issues)

        # Statistical outliers
        outlier_issues = self._check_outliers(data, series.series_id)
        issues.extend(outlier_issues)

        # Range validation (if known range exists)
        range_issues = self._check_range(data, series.series_id)
        issues.extend(range_issues)

        # Gap detection
        if self.check_gaps:
            gap_issues = self._check_gaps(data, series.frequency)
            issues.extend(gap_issues)

        # Determine overall validity (no errors)
        is_valid = not any(i.severity == "error" for i in issues)

        return QualityReport(
            series_id=series.series_id,
            is_valid=is_valid,
            issues=issues,
            stats=stats,
        )

    def _calculate_stats(self, data: pd.Series) -> Dict[str, Any]:
        """Calculate descriptive statistics."""
        if len(data) == 0:
            return {"count": 0}

        numeric_data = pd.to_numeric(data, errors="coerce")
        return {
            "count": len(data),
            "null_count": data.isna().sum(),
            "null_pct": data.isna().mean(),
            "min": numeric_data.min() if len(numeric_data.dropna()) > 0 else None,
            "max": numeric_data.max() if len(numeric_data.dropna()) > 0 else None,
            "mean": numeric_data.mean() if len(numeric_data.dropna()) > 0 else None,
            "std": numeric_data.std() if len(numeric_data.dropna()) > 0 else None,
        }

    def _check_nulls(self, data: pd.Series) -> List[QualityIssue]:
        """Check for null/NaN values."""
        issues = []
        null_pct = data.isna().mean()

        if null_pct > 0:
            null_dates = data.index[data.isna()].tolist()
            severity = "error" if null_pct > self.null_threshold else "warning"
            issues.append(QualityIssue(
                severity=severity,
                category="null",
                message=f"{null_pct*100:.1f}% null values ({data.isna().sum()} of {len(data)})",
                affected_dates=null_dates[:10],  # Limit to first 10
                details={"null_percentage": null_pct}
            ))

        return issues

    def _check_types(self, data: pd.Series) -> List[QualityIssue]:
        """Check that values are numeric."""
        issues = []

        try:
            pd.to_numeric(data, errors="raise")
        except (ValueError, TypeError):
            # Find non-numeric values
            non_numeric = data[pd.to_numeric(data, errors="coerce").isna() & data.notna()]
            if len(non_numeric) > 0:
                issues.append(QualityIssue(
                    severity="error",
                    category="type",
                    message=f"{len(non_numeric)} non-numeric values found",
                    affected_dates=non_numeric.index.tolist()[:10],
                    details={"sample_values": non_numeric.head().tolist()}
                ))

        return issues

    def _check_outliers(self, data: pd.Series, series_id: str) -> List[QualityIssue]:
        """Detect statistical outliers using IQR method."""
        issues = []
        numeric_data = pd.to_numeric(data, errors="coerce").dropna()

        if len(numeric_data) < 10:
            return issues

        q1 = numeric_data.quantile(0.25)
        q3 = numeric_data.quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - self.outlier_iqr_multiplier * iqr
        upper_bound = q3 + self.outlier_iqr_multiplier * iqr

        outliers = numeric_data[(numeric_data < lower_bound) | (numeric_data > upper_bound)]

        if len(outliers) > 0:
            issues.append(QualityIssue(
                severity="warning",
                category="outlier",
                message=f"{len(outliers)} statistical outliers detected (IQR method)",
                affected_dates=outliers.index.tolist()[:10],
                details={
                    "lower_bound": lower_bound,
                    "upper_bound": upper_bound,
                    "outlier_values": outliers.head().tolist()
                }
            ))

        return issues

    def _check_range(self, data: pd.Series, series_id: str) -> List[QualityIssue]:
        """Check values against known reasonable ranges."""
        issues = []

        if series_id not in self.KNOWN_RANGES:
            return issues

        min_val, max_val = self.KNOWN_RANGES[series_id]
        numeric_data = pd.to_numeric(data, errors="coerce").dropna()

        out_of_range = numeric_data[(numeric_data < min_val) | (numeric_data > max_val)]

        if len(out_of_range) > 0:
            issues.append(QualityIssue(
                severity="error",
                category="range",
                message=f"{len(out_of_range)} values outside expected range [{min_val}, {max_val}]",
                affected_dates=out_of_range.index.tolist()[:10],
                details={
                    "expected_min": min_val,
                    "expected_max": max_val,
                    "actual_values": out_of_range.head().tolist()
                }
            ))

        return issues

    def _check_gaps(self, data: pd.Series, frequency: str) -> List[QualityIssue]:
        """Check for unexpected gaps in the time series."""
        issues = []

        if len(data) < 2:
            return issues

        # Determine expected frequency
        freq_map = {
            "Daily": "D",
            "Weekly": "W",
            "Monthly": "MS",
            "Quarterly": "QS",
            "Annual": "YS",
        }
        expected_freq = freq_map.get(frequency, "D")

        try:
            expected_range = pd.date_range(
                start=data.index.min(),
                end=data.index.max(),
                freq=expected_freq
            )
            missing_dates = expected_range.difference(data.index)

            if len(missing_dates) > 0:
                issues.append(QualityIssue(
                    severity="warning",
                    category="gap",
                    message=f"{len(missing_dates)} expected dates missing",
                    affected_dates=missing_dates.tolist()[:10],
                    details={"expected_frequency": frequency}
                ))
        except Exception as e:
            logger.warning(f"Gap check failed: {e}")

        return issues
```
  </action>
  <verify>
Run: `python -c "from ta_lab2.integrations.economic.circuit_breaker import CircuitBreaker, CircuitState; cb=CircuitBreaker(2); cb.record_failure(); cb.record_failure(); print(f'State: {cb.state}')"` shows OPEN state
Run: `python -c "from ta_lab2.integrations.economic.quality import QualityValidator; print('Quality validator importable')"` succeeds
  </verify>
  <done>circuit_breaker.py and quality.py created with working implementations</done>
</task>

</tasks>

<verification>
1. rate_limiter.py implements token bucket algorithm with 120 calls/min default
2. cache.py implements TTL cache with LRU eviction
3. circuit_breaker.py implements circuit breaker pattern with exponential backoff
4. quality.py implements comprehensive validation (null, type, outlier, range, gap)
5. All modules thread-safe with proper locking
6. All features are working implementations (not stubs)
7. Global singleton getters provided for convenience
</verification>

<success_criteria>
- Rate limiter prevents exceeding FRED 120 calls/min limit
- Cache avoids repeated queries with configurable TTL
- Circuit breaker stops retrying when service down (5 failures default)
- Quality validator detects nulls, outliers, and range violations
- All modules have proper docstrings and type hints
- Production-ready code (not stubs) per context requirements
</success_criteria>

<output>
After completion, create `.planning/phases/15-economic-data-strategy/15-04-SUMMARY.md`
</output>
