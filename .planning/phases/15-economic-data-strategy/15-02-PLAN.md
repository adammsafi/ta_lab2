---
phase: 15-economic-data-strategy
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ta_lab2/utils/economic/__init__.py
  - src/ta_lab2/utils/economic/consolidation.py
  - src/ta_lab2/utils/economic/io_helpers.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Valuable functions from fedtools2 extracted and cleaned up"
    - "Functions have proper docstrings and type hints"
    - "Module importable without external dependencies"
  artifacts:
    - path: "src/ta_lab2/utils/economic/__init__.py"
      provides: "Package exports and documentation"
      exports: ["combine_timeframes", "missing_ranges", "read_csv", "ensure_dir"]
    - path: "src/ta_lab2/utils/economic/consolidation.py"
      provides: "Time series consolidation utilities"
      contains: "combine_timeframes"
    - path: "src/ta_lab2/utils/economic/io_helpers.py"
      provides: "I/O utilities for economic data"
      contains: "read_csv"
  key_links:
    - from: "src/ta_lab2/utils/economic/__init__.py"
      to: "consolidation.py, io_helpers.py"
      via: "imports"
      pattern: "from .consolidation import"
---

<objective>
Extract valuable utilities from fedtools2 to ta_lab2.utils.economic

Purpose: Preserve reusable logic from fedtools2 (combine_timeframes, missing_ranges, I/O helpers) in a clean, well-documented form within ta_lab2. These utilities have value beyond the original package context. Following context decision to "extract unique parts, archive the rest."

Output: New ta_lab2.utils.economic module with cleaned-up, documented versions of valuable functions.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-economic-data-strategy/15-CONTEXT.md

# Existing utils pattern
@src/ta_lab2/utils/cache.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create economic utils package structure</name>
  <files>
src/ta_lab2/utils/economic/__init__.py
  </files>
  <action>
Create the economic utils package with proper __init__.py:

```python
"""Economic data utilities extracted from fedtools2.

This module provides time series consolidation and I/O utilities
that were extracted from the archived fedtools2 package. These
utilities have general applicability for economic and financial
time series work.

Extracted from: fedtools2 (archived 2026-02-03)
Original purpose: ETL consolidation of Federal Reserve policy target datasets
Archive location: .archive/external-packages/2026-02-03/fedtools2/

Functions:
    combine_timeframes: Merge multiple time series with coverage tracking
    missing_ranges: Detect gaps in boolean mask series
    read_csv: Read CSV with standardized DataFrame output
    ensure_dir: Create directory with parents if needed

Example:
    >>> from ta_lab2.utils.economic import combine_timeframes, missing_ranges
    >>> merged = combine_timeframes([df1, df2], ["series1", "series2"])
    >>> gaps = missing_ranges(merged["has_series1"] == False)
"""

from ta_lab2.utils.economic.consolidation import combine_timeframes, missing_ranges
from ta_lab2.utils.economic.io_helpers import read_csv, ensure_dir

__all__ = [
    "combine_timeframes",
    "missing_ranges",
    "read_csv",
    "ensure_dir",
]
```

Create the utils/economic/ directory if it doesn't exist.
  </action>
  <verify>
Run: `python -c "from ta_lab2.utils.economic import combine_timeframes, missing_ranges; print('Import OK')"` succeeds (after all files created)
  </verify>
  <done>economic utils package structure created with proper __init__.py and exports</done>
</task>

<task type="auto">
  <name>Task 2: Create consolidation.py with cleaned-up functions</name>
  <files>src/ta_lab2/utils/economic/consolidation.py</files>
  <action>
Create consolidation.py with cleaned-up versions of fedtools2 utilities:

```python
"""Time series consolidation utilities.

Extracted from fedtools2.utils.consolidation and cleaned up with:
- Full type hints
- Comprehensive docstrings
- Removed S#/V# comment style
- Improved variable names

Original source: .archive/external-packages/2026-02-03/fedtools2/src/fedtools2/utils/consolidation.py
"""
from __future__ import annotations

from functools import reduce
from typing import Optional

import pandas as pd


def _prepare_dataframe(df: pd.DataFrame, name: str) -> pd.DataFrame:
    """Prepare a DataFrame for time series merging.

    Normalizes the first column to 'date', converts to DatetimeIndex,
    prefixes all data columns with the series name, and adds a coverage flag.

    Args:
        df: Input DataFrame with date in first column
        name: Series name for column prefixing

    Returns:
        DataFrame with DatetimeIndex and prefixed columns
    """
    df = df.copy()

    # Normalize first column to 'date'
    df.rename(columns={df.columns[0]: "date"}, inplace=True)

    # Convert to sorted DatetimeIndex
    df["date"] = pd.to_datetime(df["date"])
    df = df.set_index("date").sort_index()

    # Prefix data columns and add coverage flag
    df.columns = [f"{name}_{col}" for col in df.columns]
    df[f"has_{name}"] = True

    return df


def combine_timeframes(
    dfs: list[pd.DataFrame],
    names: list[str],
    persist: bool = True,
    limit: Optional[int] = None,
) -> pd.DataFrame:
    """Merge multiple time series DataFrames with coverage tracking.

    Performs an outer join on all DataFrames by date, prefixes columns
    with series names to avoid collisions, and tracks which series have
    data at each date via has_{name} boolean columns.

    Args:
        dfs: List of DataFrames to merge (each with date in first column)
        names: List of names for each DataFrame (used for column prefixes)
        persist: If True, forward-fill missing values after merge
        limit: Maximum number of consecutive NaN values to forward-fill

    Returns:
        Merged DataFrame with:
        - DatetimeIndex
        - Columns prefixed with series names: {name}_{original_col}
        - Coverage flags: has_{name} (True where series has data)

    Raises:
        AssertionError: If dfs and names have different lengths

    Example:
        >>> df1 = pd.DataFrame({"date": ["2024-01-01"], "value": [100]})
        >>> df2 = pd.DataFrame({"date": ["2024-01-01", "2024-01-02"], "value": [200, 201]})
        >>> merged = combine_timeframes([df1, df2], ["series1", "series2"])
        >>> merged.columns.tolist()
        ['series1_value', 'has_series1', 'series2_value', 'has_series2']
    """
    assert len(dfs) == len(names), "dfs and names must have the same length"

    # Prepare all DataFrames
    prepared = [_prepare_dataframe(df, name) for df, name in zip(dfs, names)]

    # Outer join all on date index
    merged = reduce(lambda left, right: left.join(right, how="outer"), prepared)

    # Fill coverage flags to False where missing
    for name in names:
        merged[f"has_{name}"] = merged[f"has_{name}"].fillna(False)

    # Forward-fill values if requested
    if persist:
        value_cols = [col for col in merged.columns if not col.startswith("has_")]
        merged[value_cols] = merged[value_cols].ffill(limit=limit)

    return merged


def missing_ranges(mask: pd.Series) -> list[tuple[pd.Timestamp, pd.Timestamp]]:
    """Detect contiguous ranges where a boolean mask is True.

    Useful for finding gaps in time series data by passing a mask
    like `series.isna()` or `~has_data`.

    Args:
        mask: Boolean Series with DatetimeIndex

    Returns:
        List of (start, end) timestamp tuples for each contiguous True range

    Example:
        >>> dates = pd.date_range("2024-01-01", periods=5)
        >>> mask = pd.Series([False, True, True, False, True], index=dates)
        >>> gaps = missing_ranges(mask)
        >>> len(gaps)
        2
        >>> gaps[0]  # First gap: Jan 2-3
        (Timestamp('2024-01-02'), Timestamp('2024-01-03'))
    """
    if mask.empty:
        return []

    # Normalize to pandas nullable boolean
    boolean_mask = mask.astype("boolean").fillna(False)

    # Detect transitions: False->True (start) and True->False (end)
    starts = (~boolean_mask.shift(1, fill_value=False)) & boolean_mask
    ends = boolean_mask & (~boolean_mask.shift(-1, fill_value=False))

    # Pair into intervals
    return list(zip(boolean_mask.index[starts], boolean_mask.index[ends]))
```
  </action>
  <verify>
Run: `python -c "from ta_lab2.utils.economic.consolidation import combine_timeframes, missing_ranges; print('Functions importable')"` succeeds
  </verify>
  <done>consolidation.py created with cleaned-up combine_timeframes and missing_ranges functions</done>
</task>

<task type="auto">
  <name>Task 3: Create io_helpers.py with cleaned-up I/O utilities</name>
  <files>src/ta_lab2/utils/economic/io_helpers.py</files>
  <action>
Create io_helpers.py with cleaned-up I/O utilities:

```python
"""I/O utilities for economic data.

Extracted from fedtools2.utils.io and cleaned up with:
- Full type hints
- Comprehensive docstrings
- Removed environment-specific paths

Original source: .archive/external-packages/2026-02-03/fedtools2/src/fedtools2/utils/io.py
"""
from pathlib import Path
from typing import Union

import pandas as pd


def read_csv(path: Union[str, Path]) -> pd.DataFrame:
    """Read a CSV file into a DataFrame.

    Simple wrapper around pd.read_csv for consistency in economic data
    loading. Handles both string and Path inputs.

    Args:
        path: Path to the CSV file

    Returns:
        DataFrame with CSV contents

    Example:
        >>> df = read_csv("data/FEDFUNDS.csv")
        >>> df.columns.tolist()
        ['observation_date', 'FEDFUNDS']
    """
    return pd.read_csv(path)


def ensure_dir(path: Union[str, Path]) -> Path:
    """Create a directory with parents if it doesn't exist.

    Thread-safe directory creation that handles race conditions
    via exist_ok=True.

    Args:
        path: Directory path to create

    Returns:
        Path object for the directory

    Example:
        >>> output_dir = ensure_dir("output/economic")
        >>> output_dir.exists()
        True
    """
    path = Path(path)
    path.mkdir(parents=True, exist_ok=True)
    return path
```
  </action>
  <verify>
Run: `python -c "from ta_lab2.utils.economic.io_helpers import read_csv, ensure_dir; print('I/O helpers importable')"` succeeds
  </verify>
  <done>io_helpers.py created with cleaned-up read_csv and ensure_dir functions</done>
</task>

</tasks>

<verification>
1. Directory exists: src/ta_lab2/utils/economic/
2. __init__.py exports all 4 functions
3. consolidation.py contains combine_timeframes and missing_ranges with full docstrings
4. io_helpers.py contains read_csv and ensure_dir with type hints
5. All functions importable: `from ta_lab2.utils.economic import combine_timeframes, missing_ranges, read_csv, ensure_dir`
6. No environment-specific paths or hardcoded values
7. Original S#/V# comment style replaced with standard docstrings
</verification>

<success_criteria>
- ta_lab2.utils.economic module created with 4 exported functions
- Functions have comprehensive docstrings and type hints
- Code cleaned up from original fedtools2 (no cryptic comments, no hardcoded paths)
- Module importable: `from ta_lab2.utils.economic import combine_timeframes`
- Extraction follows context decision: "extract unique parts, archive the rest"
</success_criteria>

<output>
After completion, create `.planning/phases/15-economic-data-strategy/15-02-SUMMARY.md`
</output>
