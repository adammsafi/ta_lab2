---
phase: 12-archive-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ta_lab2/tools/archive/__init__.py
  - src/ta_lab2/tools/archive/types.py
  - src/ta_lab2/tools/archive/manifest.py
autonomous: true

must_haves:
  truths:
    - "FileEntry dataclass captures all required manifest fields (original_path, archive_path, action, timestamp, sha256_checksum, size_bytes)"
    - "ArchiveResult dataclass tracks operation outcomes (total, archived, skipped, errors)"
    - "ValidationSnapshot dataclass captures filesystem state for pre/post comparison"
    - "create_manifest() produces versioned JSON with $schema field"
    - "compute_file_checksum() uses hashlib.file_digest() for efficient SHA256"
    - "validate_manifest() verifies structure and checksums"
  artifacts:
    - path: "src/ta_lab2/tools/archive/__init__.py"
      provides: "Module exports for archive tooling"
      exports: ["FileEntry", "ArchiveResult", "ValidationSnapshot", "create_manifest", "save_manifest", "validate_manifest", "compute_file_checksum"]
    - path: "src/ta_lab2/tools/archive/types.py"
      provides: "Dataclass definitions for archive operations"
      min_lines: 80
      contains: "@dataclass"
    - path: "src/ta_lab2/tools/archive/manifest.py"
      provides: "Manifest creation and validation functions"
      min_lines: 120
      contains: "file_digest"
  key_links:
    - from: "src/ta_lab2/tools/archive/manifest.py"
      to: "src/ta_lab2/tools/archive/types.py"
      via: "import FileEntry, ValidationSnapshot"
      pattern: "from.*types import"
    - from: "src/ta_lab2/tools/archive/__init__.py"
      to: "src/ta_lab2/tools/archive/types.py"
      via: "re-exports"
      pattern: "from.*types import"
---

<objective>
Create the archive tooling foundation with dataclasses and manifest functions.

Purpose: Provide reusable Python utilities for archive operations that follow proven patterns from memory/migration.py. These tools will be used by subsequent phases for all file archiving operations.

Output:
- src/ta_lab2/tools/archive/ module with types.py and manifest.py
- Dataclasses: FileEntry, ArchiveResult, ValidationSnapshot
- Functions: create_manifest, save_manifest, validate_manifest, compute_file_checksum
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-archive-foundation/12-RESEARCH.md

# Proven patterns to follow
@src/ta_lab2/tools/ai_orchestrator/memory/migration.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create types.py with dataclasses</name>
  <files>
    src/ta_lab2/tools/archive/__init__.py
    src/ta_lab2/tools/archive/types.py
  </files>
  <action>
Create src/ta_lab2/tools/archive/ directory and types.py with dataclasses following the MigrationResult pattern from memory/migration.py:

1. Create src/ta_lab2/tools/archive/ directory

2. Create types.py with three dataclasses:

```python
"""Type definitions for archive operations.

Provides dataclasses for tracking archive operations, file entries,
and validation snapshots. Follows patterns from memory/migration.py.
"""
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional


@dataclass
class FileEntry:
    """Entry for a single archived file in manifest.

    Tracks the complete history of a file's archive operation,
    including original location, archive destination, and integrity data.

    Attributes:
        original_path: Original file path (relative to project root)
        archive_path: New path in .archive/ (relative to project root)
        action: Archive action type (deprecated, refactored, migrated)
        timestamp: ISO 8601 timestamp of archive operation
        sha256_checksum: SHA256 hash of file content for integrity verification
        size_bytes: File size in bytes

    Example:
        >>> entry = FileEntry(
        ...     original_path="src/old_module.py",
        ...     archive_path=".archive/2026-02-02/deprecated/old_module.py",
        ...     action="deprecated",
        ...     timestamp="2026-02-02T10:30:00",
        ...     sha256_checksum="abc123...",
        ...     size_bytes=1024
        ... )
    """
    original_path: str
    archive_path: str
    action: str  # deprecated, refactored, migrated
    timestamp: str  # ISO 8601
    sha256_checksum: str
    size_bytes: int


@dataclass
class ArchiveResult:
    """Result of batch archive operation.

    Tracks outcomes for batch archiving operations, following the
    MigrationResult pattern from memory/migration.py.

    Attributes:
        total: Total files in operation
        archived: Successfully archived files
        skipped: Files skipped (already archived or not found)
        errors: Files that failed during archive
        error_paths: List of paths that failed (for debugging)

    Example:
        >>> result = ArchiveResult(total=10, archived=8, skipped=1, errors=1)
        >>> print(result)
        Archive Result:
          Total: 10
          Archived: 8
          Skipped: 1 (already archived)
          Errors: 1
          Success Rate: 90.0%
    """
    total: int
    archived: int
    skipped: int
    errors: int
    error_paths: list[str] = field(default_factory=list)

    def __str__(self) -> str:
        """Human-readable summary."""
        success_rate = (self.archived + self.skipped) / self.total * 100 if self.total > 0 else 0
        return (
            f"Archive Result:\n"
            f"  Total: {self.total}\n"
            f"  Archived: {self.archived}\n"
            f"  Skipped: {self.skipped} (already archived)\n"
            f"  Errors: {self.errors}\n"
            f"  Success Rate: {success_rate:.1f}%"
        )


@dataclass
class ValidationSnapshot:
    """Filesystem state snapshot for zero data loss validation.

    Captures the state of files at a point in time, enabling
    pre/post comparison to verify no data was lost during archiving.

    Attributes:
        root: Root directory that was snapshotted
        pattern: Glob pattern used to find files
        timestamp: ISO 8601 timestamp of snapshot
        total_files: Number of files found
        total_size_bytes: Total size of all files
        file_checksums: Dict mapping relative path to SHA256 checksum

    Example:
        >>> snapshot = ValidationSnapshot(
        ...     root=Path("src"),
        ...     pattern="**/*.py",
        ...     timestamp="2026-02-02T10:00:00",
        ...     total_files=100,
        ...     total_size_bytes=500000,
        ...     file_checksums={"module.py": "abc123..."}
        ... )
        >>> print(snapshot)
        Snapshot(src, pattern=**/*.py):
          Files: 100
          Size: 500,000 bytes
          Coverage: 100 checksums
    """
    root: Path
    pattern: str
    timestamp: str
    total_files: int
    total_size_bytes: int
    file_checksums: dict[str, str] = field(default_factory=dict)

    def __str__(self) -> str:
        """Human-readable summary."""
        return (
            f"Snapshot({self.root}, pattern={self.pattern}):\n"
            f"  Files: {self.total_files}\n"
            f"  Size: {self.total_size_bytes:,} bytes\n"
            f"  Coverage: {len(self.file_checksums)} checksums"
        )


__all__ = ["FileEntry", "ArchiveResult", "ValidationSnapshot"]
```

3. Create __init__.py that re-exports from types.py (will be extended in Task 2):

```python
"""Archive tooling for v0.5.0 reorganization.

Provides utilities for archiving files with git history preservation,
manifest tracking, and zero data loss validation.

Example:
    >>> from ta_lab2.tools.archive import FileEntry, create_manifest
    >>> entry = FileEntry(...)
    >>> manifest = create_manifest([entry], "2026-02-02", "deprecated")
"""
from ta_lab2.tools.archive.types import (
    FileEntry,
    ArchiveResult,
    ValidationSnapshot,
)

__all__ = [
    "FileEntry",
    "ArchiveResult",
    "ValidationSnapshot",
]
```
  </action>
  <verify>
Run: `python -c "from ta_lab2.tools.archive import FileEntry, ArchiveResult, ValidationSnapshot; print('Types imported successfully')"`
Expected: "Types imported successfully"

Run: `python -c "from ta_lab2.tools.archive import ArchiveResult; r = ArchiveResult(10, 8, 1, 1); print(r)"`
Expected: Formatted result string with "Success Rate: 90.0%"
  </verify>
  <done>
- src/ta_lab2/tools/archive/ directory created
- types.py contains FileEntry, ArchiveResult, ValidationSnapshot dataclasses
- __init__.py re-exports all dataclasses
- All dataclasses importable from ta_lab2.tools.archive
- ArchiveResult.__str__() produces human-readable output
  </done>
</task>

<task type="auto">
  <name>Task 2: Create manifest.py with manifest functions</name>
  <files>
    src/ta_lab2/tools/archive/manifest.py
    src/ta_lab2/tools/archive/__init__.py
  </files>
  <action>
Create manifest.py with functions for creating, saving, and validating archive manifests:

1. Create manifest.py:

```python
"""Manifest creation and validation for archive operations.

Provides functions for creating versioned JSON manifests that track
archived files with checksums and metadata. Uses hashlib.file_digest()
(Python 3.11+) for efficient checksum computation.
"""
import hashlib
import json
import logging
from dataclasses import asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

from ta_lab2.tools.archive.types import FileEntry

logger = logging.getLogger(__name__)

# Manifest schema version
MANIFEST_SCHEMA = "https://ta_lab2.local/schemas/archive-manifest/v1.0.0"
MANIFEST_VERSION = "1.0.0"

# Valid archive actions
VALID_ACTIONS = {"deprecated", "refactored", "migrated"}


def compute_file_checksum(file_path: Path) -> str:
    """Compute SHA256 checksum using hashlib.file_digest() (Python 3.11+).

    Uses optimized I/O that bypasses Python buffers for large files.

    Args:
        file_path: Path to file to checksum

    Returns:
        Lowercase hexadecimal SHA256 checksum (64 characters)

    Raises:
        FileNotFoundError: If file does not exist
        PermissionError: If file cannot be read

    Example:
        >>> checksum = compute_file_checksum(Path("README.md"))
        >>> len(checksum)
        64
    """
    with file_path.open("rb") as f:
        digest = hashlib.file_digest(f, "sha256")
    return digest.hexdigest()


def create_file_entry(
    original_path: Path,
    archive_path: Path,
    action: str,
    project_root: Optional[Path] = None
) -> FileEntry:
    """Create a FileEntry for a file that has been archived.

    Computes checksum and size from the archived file (not original).

    Args:
        original_path: Original file path (relative or absolute)
        archive_path: Archive destination path (must exist)
        action: Archive action (deprecated, refactored, migrated)
        project_root: Project root for relative paths (default: cwd)

    Returns:
        FileEntry with computed checksum and size

    Raises:
        ValueError: If action is not valid
        FileNotFoundError: If archive_path does not exist

    Example:
        >>> entry = create_file_entry(
        ...     Path("src/old.py"),
        ...     Path(".archive/2026-02-02/deprecated/old.py"),
        ...     "deprecated"
        ... )
    """
    if action not in VALID_ACTIONS:
        raise ValueError(f"Invalid action '{action}'. Must be one of: {VALID_ACTIONS}")

    if project_root is None:
        project_root = Path.cwd()

    # Resolve paths relative to project root
    if original_path.is_absolute():
        original_rel = original_path.relative_to(project_root)
    else:
        original_rel = original_path

    if archive_path.is_absolute():
        archive_abs = archive_path
        archive_rel = archive_path.relative_to(project_root)
    else:
        archive_abs = project_root / archive_path
        archive_rel = archive_path

    if not archive_abs.exists():
        raise FileNotFoundError(f"Archive path does not exist: {archive_abs}")

    return FileEntry(
        original_path=str(original_rel),
        archive_path=str(archive_rel),
        action=action,
        timestamp=datetime.now(timezone.utc).isoformat(),
        sha256_checksum=compute_file_checksum(archive_abs),
        size_bytes=archive_abs.stat().st_size
    )


def create_manifest(
    entries: list[FileEntry],
    archive_date: str,
    category: str
) -> dict:
    """Create versioned manifest for archive category.

    Creates a JSON-serializable manifest following the archive-manifest
    schema with $schema versioning for future compatibility.

    Args:
        entries: List of FileEntry objects for archived files
        archive_date: ISO 8601 date (YYYY-MM-DD)
        category: Archive category (deprecated, refactored, migrated)

    Returns:
        Dict ready for JSON serialization with schema and version info

    Example:
        >>> entries = [entry1, entry2]
        >>> manifest = create_manifest(entries, "2026-02-02", "deprecated")
        >>> manifest["$schema"]
        'https://ta_lab2.local/schemas/archive-manifest/v1.0.0'
    """
    return {
        "$schema": MANIFEST_SCHEMA,
        "version": MANIFEST_VERSION,
        "archive_date": archive_date,
        "category": category,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "total_files": len(entries),
        "total_size_bytes": sum(e.size_bytes for e in entries),
        "files": [asdict(e) for e in entries]
    }


def save_manifest(manifest: dict, manifest_path: Path) -> None:
    """Save manifest to JSON file with readable formatting.

    Creates parent directories if needed. Uses 2-space indentation
    and sorted keys for readable git diffs.

    Args:
        manifest: Manifest dict from create_manifest()
        manifest_path: Path to write manifest file

    Example:
        >>> manifest = create_manifest(entries, "2026-02-02", "deprecated")
        >>> save_manifest(manifest, Path(".archive/2026-02-02/deprecated/manifest.json"))
    """
    manifest_path.parent.mkdir(parents=True, exist_ok=True)
    manifest_path.write_text(
        json.dumps(manifest, indent=2, sort_keys=True) + "\n"
    )
    logger.info(f"Saved manifest to {manifest_path}")


def validate_manifest(manifest_path: Path) -> tuple[bool, list[str]]:
    """Validate manifest structure and file checksums.

    Checks:
    1. Valid JSON structure
    2. Required fields present ($schema, version, archive_date, files)
    3. All file entries have required fields
    4. All archived files exist at archive_path
    5. All checksums match current file content

    Args:
        manifest_path: Path to manifest.json file

    Returns:
        Tuple of (is_valid, issues) where issues is empty list if valid

    Example:
        >>> valid, issues = validate_manifest(Path(".archive/2026-02-02/manifest.json"))
        >>> if not valid:
        ...     print("Issues:", issues)
    """
    issues = []

    # Check manifest exists
    if not manifest_path.exists():
        return False, [f"Manifest not found: {manifest_path}"]

    # Parse JSON
    try:
        manifest = json.loads(manifest_path.read_text())
    except json.JSONDecodeError as e:
        return False, [f"Invalid JSON: {e}"]

    # Check required top-level fields
    required_fields = ["$schema", "version", "archive_date", "files"]
    for field in required_fields:
        if field not in manifest:
            issues.append(f"Missing required field: {field}")

    if issues:
        return False, issues

    # Validate each file entry
    entry_required = ["original_path", "archive_path", "action", "sha256_checksum", "size_bytes"]

    for idx, entry in enumerate(manifest.get("files", [])):
        # Check required entry fields
        for field in entry_required:
            if field not in entry:
                issues.append(f"File entry {idx} missing field: {field}")
                continue

        # Verify archive_path exists
        archive_path = Path(entry.get("archive_path", ""))
        if not archive_path.exists():
            issues.append(f"Archived file not found: {archive_path}")
            continue

        # Verify checksum matches
        try:
            actual_checksum = compute_file_checksum(archive_path)
            expected_checksum = entry.get("sha256_checksum", "")
            if actual_checksum != expected_checksum:
                issues.append(
                    f"Checksum mismatch for {archive_path}: "
                    f"expected {expected_checksum[:16]}..., got {actual_checksum[:16]}..."
                )
        except Exception as e:
            issues.append(f"Failed to verify checksum for {archive_path}: {e}")

        # Verify size matches
        actual_size = archive_path.stat().st_size
        expected_size = entry.get("size_bytes", 0)
        if actual_size != expected_size:
            issues.append(
                f"Size mismatch for {archive_path}: "
                f"expected {expected_size}, got {actual_size}"
            )

    return len(issues) == 0, issues


def load_manifest(manifest_path: Path) -> Optional[dict]:
    """Load and parse a manifest file.

    Args:
        manifest_path: Path to manifest.json file

    Returns:
        Parsed manifest dict, or None if file doesn't exist or is invalid

    Example:
        >>> manifest = load_manifest(Path(".archive/2026-02-02/manifest.json"))
        >>> if manifest:
        ...     print(f"Contains {manifest['total_files']} files")
    """
    if not manifest_path.exists():
        logger.warning(f"Manifest not found: {manifest_path}")
        return None

    try:
        return json.loads(manifest_path.read_text())
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse manifest {manifest_path}: {e}")
        return None


__all__ = [
    "MANIFEST_SCHEMA",
    "MANIFEST_VERSION",
    "VALID_ACTIONS",
    "compute_file_checksum",
    "create_file_entry",
    "create_manifest",
    "save_manifest",
    "validate_manifest",
    "load_manifest",
]
```

2. Update __init__.py to include manifest exports:

```python
"""Archive tooling for v0.5.0 reorganization.

Provides utilities for archiving files with git history preservation,
manifest tracking, and zero data loss validation.

Example:
    >>> from ta_lab2.tools.archive import FileEntry, create_manifest
    >>> entry = FileEntry(...)
    >>> manifest = create_manifest([entry], "2026-02-02", "deprecated")
"""
from ta_lab2.tools.archive.types import (
    FileEntry,
    ArchiveResult,
    ValidationSnapshot,
)
from ta_lab2.tools.archive.manifest import (
    MANIFEST_SCHEMA,
    MANIFEST_VERSION,
    VALID_ACTIONS,
    compute_file_checksum,
    create_file_entry,
    create_manifest,
    save_manifest,
    validate_manifest,
    load_manifest,
)

__all__ = [
    # Types
    "FileEntry",
    "ArchiveResult",
    "ValidationSnapshot",
    # Manifest functions
    "MANIFEST_SCHEMA",
    "MANIFEST_VERSION",
    "VALID_ACTIONS",
    "compute_file_checksum",
    "create_file_entry",
    "create_manifest",
    "save_manifest",
    "validate_manifest",
    "load_manifest",
]
```
  </action>
  <verify>
Run: `python -c "from ta_lab2.tools.archive import compute_file_checksum; from pathlib import Path; print(len(compute_file_checksum(Path('README.md'))))"`
Expected: "64" (SHA256 hex length)

Run: `python -c "from ta_lab2.tools.archive import create_manifest, FileEntry; e = FileEntry('a.py', 'b.py', 'deprecated', '2026-02-02', 'abc', 100); m = create_manifest([e], '2026-02-02', 'deprecated'); print(m['$schema'])"`
Expected: "https://ta_lab2.local/schemas/archive-manifest/v1.0.0"

Run: `python -c "from ta_lab2.tools.archive import VALID_ACTIONS; print(VALID_ACTIONS)"`
Expected: Set containing deprecated, refactored, migrated
  </verify>
  <done>
- manifest.py created with all required functions
- compute_file_checksum() uses hashlib.file_digest()
- create_manifest() produces versioned JSON with $schema
- validate_manifest() checks structure, existence, and checksums
- All functions importable from ta_lab2.tools.archive
- __init__.py updated with complete exports
  </done>
</task>

</tasks>

<verification>
Phase 12 Plan 02 verification (run all):

1. Import verification:
   ```bash
   python -c "from ta_lab2.tools.archive import FileEntry, ArchiveResult, ValidationSnapshot, create_manifest, save_manifest, validate_manifest, compute_file_checksum; print('All imports successful')"
   ```
   Expected: "All imports successful"

2. Checksum function verification:
   ```bash
   python -c "from ta_lab2.tools.archive import compute_file_checksum; from pathlib import Path; c = compute_file_checksum(Path('pyproject.toml')); print(f'Checksum: {c[:16]}... ({len(c)} chars)')"
   ```
   Expected: 64-character checksum

3. Manifest creation verification:
   ```bash
   python -c "
from ta_lab2.tools.archive import FileEntry, create_manifest
entry = FileEntry('test.py', '.archive/test.py', 'deprecated', '2026-02-02T10:00:00', 'abc123', 100)
manifest = create_manifest([entry], '2026-02-02', 'deprecated')
assert manifest['\$schema'].startswith('https://ta_lab2')
assert manifest['version'] == '1.0.0'
assert len(manifest['files']) == 1
print('Manifest creation: OK')
"
   ```
   Expected: "Manifest creation: OK"

4. File line counts:
   ```bash
   wc -l src/ta_lab2/tools/archive/types.py src/ta_lab2/tools/archive/manifest.py
   ```
   Expected: types.py >= 80 lines, manifest.py >= 120 lines
</verification>

<success_criteria>
1. src/ta_lab2/tools/archive/ module exists with __init__.py, types.py, manifest.py
2. FileEntry, ArchiveResult, ValidationSnapshot dataclasses importable
3. compute_file_checksum() returns 64-char SHA256 using file_digest()
4. create_manifest() produces versioned JSON with $schema field
5. validate_manifest() checks structure and checksums
6. All functions documented with docstrings and examples
7. All files committed to git
</success_criteria>

<output>
After completion, create `.planning/phases/12-archive-foundation/12-02-SUMMARY.md`
</output>
