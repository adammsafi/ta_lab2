---
phase: 12-archive-foundation
plan: 03
type: execute
wave: 2
depends_on: [12-02]
files_modified:
  - src/ta_lab2/tools/archive/validate.py
  - src/ta_lab2/tools/archive/__init__.py
  - .planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json
autonomous: true

must_haves:
  truths:
    - "create_snapshot() captures file counts, sizes, and checksums for any directory"
    - "validate_no_data_loss() compares pre/post snapshots and reports missing files"
    - "Pre-reorganization baseline exists with exact file counts for src/, tests/, docs/"
    - "Baseline snapshot stored in .planning/phases/12-archive-foundation/baseline/"
    - "Snapshot includes total files, total size, and per-file SHA256 checksums"
  artifacts:
    - path: "src/ta_lab2/tools/archive/validate.py"
      provides: "Snapshot creation and validation functions"
      min_lines: 120
      exports: ["create_snapshot", "validate_no_data_loss", "save_snapshot", "load_snapshot"]
    - path: ".planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json"
      provides: "Pre-reorganization file counts and checksums"
      contains: "total_files"
  key_links:
    - from: "src/ta_lab2/tools/archive/validate.py"
      to: "src/ta_lab2/tools/archive/types.py"
      via: "import ValidationSnapshot"
      pattern: "from.*types import.*ValidationSnapshot"
    - from: "src/ta_lab2/tools/archive/validate.py"
      to: "src/ta_lab2/tools/archive/manifest.py"
      via: "import compute_file_checksum"
      pattern: "from.*manifest import.*compute_file_checksum"
---

<objective>
Create validation tooling and capture pre-reorganization baseline snapshot.

Purpose: Establish the validation infrastructure and baseline file counts that will be used to verify zero data loss after all v0.5.0 file moves. This snapshot provides the "before" state for comparison.

Output:
- src/ta_lab2/tools/archive/validate.py with snapshot and validation functions
- Pre-reorganization baseline snapshot (JSON) with file counts and checksums
- Documented baseline statistics for audit trail
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-archive-foundation/12-RESEARCH.md

# Plan 02 provides types and manifest functions
@.planning/phases/12-archive-foundation/12-02-PLAN.md

# Proven patterns
@src/ta_lab2/tools/ai_orchestrator/memory/migration.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create validate.py with snapshot functions</name>
  <files>
    src/ta_lab2/tools/archive/validate.py
    src/ta_lab2/tools/archive/__init__.py
  </files>
  <action>
Create validate.py with functions for creating filesystem snapshots and validating zero data loss:

1. Create validate.py:

```python
"""Validation tools for zero data loss during archive operations.

Provides functions for creating filesystem snapshots before operations
and validating that no data was lost after operations complete.
"""
import json
import logging
from dataclasses import asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

from ta_lab2.tools.archive.types import ValidationSnapshot
from ta_lab2.tools.archive.manifest import compute_file_checksum

logger = logging.getLogger(__name__)

# Default patterns for different file types
DEFAULT_PATTERNS = {
    "python": "**/*.py",
    "all_code": "**/*.py",
    "documentation": "**/*.md",
    "configs": "**/*.{json,yaml,yml,toml}",
}

# Directories to exclude from checksumming
EXCLUDE_DIRS = {
    "__pycache__",
    ".git",
    ".venv",
    "venv",
    "node_modules",
    ".pytest_cache",
    ".mypy_cache",
    ".ruff_cache",
    "*.egg-info",
}


def should_exclude(path: Path) -> bool:
    """Check if a path should be excluded from snapshot.

    Args:
        path: Path to check

    Returns:
        True if path should be excluded
    """
    for part in path.parts:
        if part in EXCLUDE_DIRS or part.endswith(".egg-info"):
            return True
    return False


def create_snapshot(
    root: Path,
    pattern: str = "**/*.py",
    compute_checksums: bool = True,
    progress_every: int = 100
) -> ValidationSnapshot:
    """Create filesystem state snapshot for validation.

    Captures file counts, sizes, and optionally checksums for all files
    matching the pattern under root directory.

    Args:
        root: Root directory to snapshot
        pattern: Glob pattern for files to include (default: **/*.py)
        compute_checksums: If True, compute SHA256 for each file (slower but complete)
        progress_every: Log progress every N files (default: 100)

    Returns:
        ValidationSnapshot with file counts, sizes, and checksums

    Example:
        >>> snapshot = create_snapshot(Path("src"), pattern="**/*.py")
        >>> print(f"Found {snapshot.total_files} Python files")
    """
    root = Path(root).resolve()
    logger.info(f"Creating snapshot of {root} with pattern '{pattern}'")

    files = [
        f for f in root.glob(pattern)
        if f.is_file() and not should_exclude(f)
    ]

    total_size = 0
    checksums: dict[str, str] = {}
    processed = 0

    for file_path in files:
        try:
            stat = file_path.stat()
            total_size += stat.st_size

            if compute_checksums:
                rel_path = str(file_path.relative_to(root))
                checksums[rel_path] = compute_file_checksum(file_path)

            processed += 1
            if processed % progress_every == 0:
                logger.info(f"Progress: {processed}/{len(files)} files processed")

        except (PermissionError, OSError) as e:
            logger.warning(f"Skipping {file_path}: {e}")

    snapshot = ValidationSnapshot(
        root=root,
        pattern=pattern,
        timestamp=datetime.now(timezone.utc).isoformat(),
        total_files=len(files),
        total_size_bytes=total_size,
        file_checksums=checksums
    )

    logger.info(f"Snapshot complete: {snapshot.total_files} files, {snapshot.total_size_bytes:,} bytes")
    return snapshot


def validate_no_data_loss(
    pre: ValidationSnapshot,
    post: ValidationSnapshot,
    strict: bool = False
) -> tuple[bool, list[str]]:
    """Validate that no data was lost between snapshots.

    Compares pre and post snapshots to ensure all files from pre-snapshot
    exist in post-snapshot (possibly at different paths). Uses checksums
    to track files that moved.

    Args:
        pre: Snapshot before archive operation
        post: Snapshot after archive operation (should cover wider scope)
        strict: If True, require exact file count match (no additions)

    Returns:
        Tuple of (success, issues) where issues lists any problems

    Example:
        >>> pre = create_snapshot(Path("src"))
        >>> # ... perform archiving ...
        >>> post = create_snapshot(Path("."))  # Check entire project
        >>> success, issues = validate_no_data_loss(pre, post)
        >>> if not success:
        ...     print("Data loss detected!")
    """
    issues = []

    # Check file counts
    if post.total_files < pre.total_files:
        missing = pre.total_files - post.total_files
        issues.append(
            f"File count decreased by {missing} "
            f"({pre.total_files} -> {post.total_files})"
        )
    elif strict and post.total_files != pre.total_files:
        diff = post.total_files - pre.total_files
        issues.append(
            f"File count changed by {diff:+d} "
            f"({pre.total_files} -> {post.total_files})"
        )

    # Check total size
    if post.total_size_bytes < pre.total_size_bytes:
        lost_bytes = pre.total_size_bytes - post.total_size_bytes
        issues.append(
            f"Total size decreased by {lost_bytes:,} bytes "
            f"({pre.total_size_bytes:,} -> {post.total_size_bytes:,})"
        )

    # Check that all pre-checksums exist in post (files may have moved)
    if pre.file_checksums and post.file_checksums:
        pre_checksums = set(pre.file_checksums.values())
        post_checksums = set(post.file_checksums.values())
        missing_checksums = pre_checksums - post_checksums

        if missing_checksums:
            # Find which files are missing
            missing_files = [
                path for path, checksum in pre.file_checksums.items()
                if checksum in missing_checksums
            ]
            issues.append(
                f"{len(missing_files)} file(s) missing (checksum not found in post-snapshot):"
            )
            for path in missing_files[:10]:  # Show first 10
                issues.append(f"  - {path}")
            if len(missing_files) > 10:
                issues.append(f"  ... and {len(missing_files) - 10} more")

    return len(issues) == 0, issues


def save_snapshot(snapshot: ValidationSnapshot, output_path: Path) -> None:
    """Save snapshot to JSON file.

    Args:
        snapshot: ValidationSnapshot to save
        output_path: Path for output JSON file

    Example:
        >>> snapshot = create_snapshot(Path("src"))
        >>> save_snapshot(snapshot, Path("baseline.json"))
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Convert to serializable dict
    data = {
        "root": str(snapshot.root),
        "pattern": snapshot.pattern,
        "timestamp": snapshot.timestamp,
        "total_files": snapshot.total_files,
        "total_size_bytes": snapshot.total_size_bytes,
        "file_checksums": snapshot.file_checksums
    }

    output_path.write_text(
        json.dumps(data, indent=2, sort_keys=True) + "\n"
    )
    logger.info(f"Saved snapshot to {output_path}")


def load_snapshot(input_path: Path) -> Optional[ValidationSnapshot]:
    """Load snapshot from JSON file.

    Args:
        input_path: Path to JSON snapshot file

    Returns:
        ValidationSnapshot, or None if file doesn't exist or is invalid

    Example:
        >>> snapshot = load_snapshot(Path("baseline.json"))
        >>> if snapshot:
        ...     print(f"Loaded {snapshot.total_files} files")
    """
    if not input_path.exists():
        logger.warning(f"Snapshot file not found: {input_path}")
        return None

    try:
        data = json.loads(input_path.read_text())
        return ValidationSnapshot(
            root=Path(data["root"]),
            pattern=data["pattern"],
            timestamp=data["timestamp"],
            total_files=data["total_files"],
            total_size_bytes=data["total_size_bytes"],
            file_checksums=data.get("file_checksums", {})
        )
    except (json.JSONDecodeError, KeyError) as e:
        logger.error(f"Failed to load snapshot {input_path}: {e}")
        return None


def create_multi_directory_snapshot(
    directories: list[Path],
    pattern: str = "**/*.py",
    compute_checksums: bool = True
) -> dict[str, ValidationSnapshot]:
    """Create snapshots for multiple directories.

    Useful for capturing baseline of several areas at once.

    Args:
        directories: List of directories to snapshot
        pattern: Glob pattern for files
        compute_checksums: If True, compute checksums

    Returns:
        Dict mapping directory name to ValidationSnapshot

    Example:
        >>> snapshots = create_multi_directory_snapshot([Path("src"), Path("tests")])
        >>> for name, snap in snapshots.items():
        ...     print(f"{name}: {snap.total_files} files")
    """
    snapshots = {}
    for directory in directories:
        if directory.exists():
            name = directory.name
            snapshots[name] = create_snapshot(
                directory,
                pattern=pattern,
                compute_checksums=compute_checksums
            )
        else:
            logger.warning(f"Directory not found, skipping: {directory}")
    return snapshots


__all__ = [
    "DEFAULT_PATTERNS",
    "EXCLUDE_DIRS",
    "create_snapshot",
    "validate_no_data_loss",
    "save_snapshot",
    "load_snapshot",
    "create_multi_directory_snapshot",
]
```

2. Update __init__.py to include validate exports:

Add these imports and exports to the existing __init__.py:

```python
from ta_lab2.tools.archive.validate import (
    DEFAULT_PATTERNS,
    EXCLUDE_DIRS,
    create_snapshot,
    validate_no_data_loss,
    save_snapshot,
    load_snapshot,
    create_multi_directory_snapshot,
)
```

And add to __all__:
```python
    # Validation functions
    "DEFAULT_PATTERNS",
    "EXCLUDE_DIRS",
    "create_snapshot",
    "validate_no_data_loss",
    "save_snapshot",
    "load_snapshot",
    "create_multi_directory_snapshot",
```
  </action>
  <verify>
Run: `python -c "from ta_lab2.tools.archive import create_snapshot, validate_no_data_loss; print('Validation imports successful')"`
Expected: "Validation imports successful"

Run: `python -c "from ta_lab2.tools.archive import create_snapshot; from pathlib import Path; s = create_snapshot(Path('src/ta_lab2/tools/archive'), compute_checksums=False); print(f'Found {s.total_files} files')"`
Expected: Shows file count (should be 4: __init__.py, types.py, manifest.py, validate.py)
  </verify>
  <done>
- validate.py created with create_snapshot(), validate_no_data_loss(), save_snapshot(), load_snapshot()
- create_snapshot() captures file counts, sizes, and checksums
- validate_no_data_loss() compares snapshots and reports missing files
- __init__.py updated with validation exports
- All functions importable from ta_lab2.tools.archive
  </done>
</task>

<task type="auto">
  <name>Task 2: Capture pre-reorganization baseline snapshot</name>
  <files>
    .planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json
  </files>
  <action>
Execute snapshot creation to capture pre-reorganization file counts and checksums:

1. Create Python script to capture comprehensive baseline:

```python
# Run this to capture baseline
import logging
from pathlib import Path
from datetime import datetime, timezone
import json

logging.basicConfig(level=logging.INFO)

from ta_lab2.tools.archive import create_snapshot, save_snapshot

# Project root
project_root = Path(".")

# Directories to snapshot
directories = {
    "src_ta_lab2": Path("src/ta_lab2"),
    "tests": Path("tests"),
    "docs": Path("docs"),
    "planning": Path(".planning"),
}

# Create snapshots for Python files
results = {}
for name, directory in directories.items():
    if directory.exists():
        print(f"\nSnapshotting {name} ({directory})...")
        snapshot = create_snapshot(directory, pattern="**/*.py", compute_checksums=True)
        results[name] = {
            "root": str(snapshot.root),
            "pattern": snapshot.pattern,
            "timestamp": snapshot.timestamp,
            "total_files": snapshot.total_files,
            "total_size_bytes": snapshot.total_size_bytes,
            "checksum_count": len(snapshot.file_checksums)
        }
        print(f"  {snapshot.total_files} files, {snapshot.total_size_bytes:,} bytes")
    else:
        print(f"Skipping {name} (not found)")

# Create overall project snapshot
print("\nCreating overall project snapshot...")
overall = create_snapshot(project_root, pattern="**/*.py", compute_checksums=True)

# Combine into baseline document
baseline = {
    "$schema": "https://ta_lab2.local/schemas/pre-reorg-baseline/v1.0.0",
    "version": "1.0.0",
    "created_at": datetime.now(timezone.utc).isoformat(),
    "phase": "12-archive-foundation",
    "purpose": "Pre-reorganization baseline for zero data loss validation",
    "overall": {
        "total_files": overall.total_files,
        "total_size_bytes": overall.total_size_bytes,
        "checksum_count": len(overall.file_checksums)
    },
    "by_directory": results,
    "file_checksums": overall.file_checksums
}

# Save baseline
output_path = Path(".planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json")
output_path.parent.mkdir(parents=True, exist_ok=True)
output_path.write_text(json.dumps(baseline, indent=2, sort_keys=True) + "\n")
print(f"\nBaseline saved to {output_path}")

# Summary
print(f"\n=== Pre-Reorganization Baseline ===")
print(f"Total Python files: {overall.total_files}")
print(f"Total size: {overall.total_size_bytes:,} bytes")
print(f"Checksums recorded: {len(overall.file_checksums)}")
print(f"\nBy directory:")
for name, data in results.items():
    print(f"  {name}: {data['total_files']} files, {data['total_size_bytes']:,} bytes")
```

2. Run the script:
   ```bash
   cd C:/Users/asafi/Downloads/ta_lab2
   python -c "..." # (the script above)
   ```

3. Verify the baseline was created:
   ```bash
   cat .planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json | head -50
   ```

4. Document the baseline statistics in the summary:
   - Total Python files count
   - Total size in bytes
   - Per-directory breakdown (src/ta_lab2, tests, docs)
   - Checksums count for validation

IMPORTANT: The exact file counts will depend on current state. Record whatever counts are returned by the snapshot tool - these become the baseline for future validation.
  </action>
  <verify>
Run: `test -f .planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json && echo "Baseline exists" || echo "Baseline missing"`
Expected: "Baseline exists"

Run: `python -c "import json; from pathlib import Path; b = json.loads(Path('.planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json').read_text()); print(f'Total files: {b[\"overall\"][\"total_files\"]}, Checksums: {b[\"overall\"][\"checksum_count\"]}')"`
Expected: Shows total file count and checksum count

Run: `grep -c '"sha256"' .planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json || echo "No sha256 found - checking for checksums"`
Run: `python -c "import json; from pathlib import Path; b = json.loads(Path('.planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json').read_text()); print(f'Has checksums: {len(b.get(\"file_checksums\", {})) > 0}')"`
Expected: Has checksums: True
  </verify>
  <done>
- Pre-reorganization baseline captured in .planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json
- Baseline includes overall file count and size
- Per-directory breakdown (src_ta_lab2, tests, docs, planning) captured
- SHA256 checksums recorded for every Python file
- Baseline statistics documented for audit trail
  </done>
</task>

</tasks>

<verification>
Phase 12 Plan 03 verification (run all):

1. Import verification:
   ```bash
   python -c "from ta_lab2.tools.archive import create_snapshot, validate_no_data_loss, save_snapshot, load_snapshot; print('All validation imports successful')"
   ```
   Expected: "All validation imports successful"

2. Baseline file exists:
   ```bash
   ls -la .planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json
   ```
   Expected: File exists with reasonable size

3. Baseline contains required fields:
   ```bash
   python -c "
import json
from pathlib import Path
b = json.loads(Path('.planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json').read_text())
assert '\$schema' in b, 'Missing schema'
assert 'overall' in b, 'Missing overall'
assert 'file_checksums' in b, 'Missing checksums'
assert b['overall']['total_files'] > 0, 'No files captured'
print(f'Baseline valid: {b[\"overall\"][\"total_files\"]} files with {len(b[\"file_checksums\"])} checksums')
"
   ```
   Expected: Shows file count and checksum count

4. Validate round-trip:
   ```bash
   python -c "
from ta_lab2.tools.archive import create_snapshot, save_snapshot, load_snapshot
from pathlib import Path
import tempfile

# Create snapshot
snap = create_snapshot(Path('src/ta_lab2/tools/archive'), compute_checksums=True)

# Save and reload
with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as f:
    save_snapshot(snap, Path(f.name))
    loaded = load_snapshot(Path(f.name))
    assert loaded.total_files == snap.total_files
    print(f'Round-trip OK: {snap.total_files} files')
"
   ```
   Expected: "Round-trip OK: N files"
</verification>

<success_criteria>
1. src/ta_lab2/tools/archive/validate.py exists with create_snapshot(), validate_no_data_loss()
2. save_snapshot() and load_snapshot() enable JSON persistence
3. Pre-reorganization baseline exists at .planning/phases/12-archive-foundation/baseline/pre_reorg_snapshot.json
4. Baseline contains overall file count, size, and per-file checksums
5. Baseline uses versioned schema ($schema field)
6. All functions importable from ta_lab2.tools.archive
7. All files committed to git
</success_criteria>

<output>
After completion, create `.planning/phases/12-archive-foundation/12-03-SUMMARY.md`
</output>
