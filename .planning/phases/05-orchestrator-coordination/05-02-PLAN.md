---
phase: 05-orchestrator-coordination
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ta_lab2/tools/ai_orchestrator/execution.py
  - tests/orchestrator/test_execution.py
autonomous: true

must_haves:
  truths:
    - "Independent tasks execute concurrently via asyncio"
    - "Semaphore limits concurrent requests"
    - "Fail-independent semantics collect all results"
    - "Results are aggregated with success/failure counts"
  artifacts:
    - path: "src/ta_lab2/tools/ai_orchestrator/execution.py"
      provides: "AsyncOrchestrator class with execute_parallel"
      exports: ["AsyncOrchestrator", "AggregatedResult", "aggregate_results"]
    - path: "tests/orchestrator/test_execution.py"
      provides: "Execution engine test coverage"
      min_lines: 80
  key_links:
    - from: "execution.py"
      to: "adapters.py"
      via: "AsyncBasePlatformAdapter"
      pattern: "adapter.*submit_task"
    - from: "execution.py"
      to: "routing.py"
      via: "TaskRouter.route_cost_optimized"
      pattern: "router.*route"
---

<objective>
Build AsyncOrchestrator parallel execution engine with result aggregation

Purpose: Implement ORCH-06 (parallel execution) and ORCH-12 (result aggregation) - concurrent task execution via asyncio TaskGroup with semaphore-controlled concurrency and fail-independent semantics.

Output: New execution.py module with AsyncOrchestrator class and comprehensive tests
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-orchestrator-coordination/05-CONTEXT.md
@.planning/phases/05-orchestrator-coordination/05-RESEARCH.md
@src/ta_lab2/tools/ai_orchestrator/adapters.py
@src/ta_lab2/tools/ai_orchestrator/core.py
@src/ta_lab2/tools/ai_orchestrator/routing.py
@src/ta_lab2/tools/ai_orchestrator/quota.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AsyncOrchestrator class with parallel execution</name>
  <files>src/ta_lab2/tools/ai_orchestrator/execution.py</files>
  <action>
Create new execution.py module with AsyncOrchestrator:

```python
"""Async orchestrator for parallel task execution."""

from __future__ import annotations

import asyncio
from asyncio import Semaphore, TaskGroup
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Dict, List, Optional

if TYPE_CHECKING:
    from .adapters import AsyncBasePlatformAdapter
    from .core import Task, Result, Platform

from .core import Task, Result, Platform, TaskStatus, TaskType
from .routing import TaskRouter
from .quota import QuotaTracker


@dataclass
class AggregatedResult:
    """Combined results from parallel task execution."""
    results: List[Result]
    total_cost: float
    total_tokens: int
    total_duration: float
    success_count: int
    failure_count: int
    by_platform: Dict[str, List[Result]] = field(default_factory=dict)

    @property
    def success_rate(self) -> float:
        total = self.success_count + self.failure_count
        return self.success_count / total if total > 0 else 0.0

    @property
    def all_succeeded(self) -> bool:
        return self.failure_count == 0


def aggregate_results(results: List[Result]) -> AggregatedResult:
    """Aggregate results from parallel execution."""
    by_platform: Dict[str, List[Result]] = {}

    for result in results:
        platform = result.platform.value
        if platform not in by_platform:
            by_platform[platform] = []
        by_platform[platform].append(result)

    return AggregatedResult(
        results=results,
        total_cost=sum(r.cost for r in results),
        total_tokens=sum(r.tokens_used for r in results),
        total_duration=sum(r.duration_seconds for r in results),
        success_count=sum(1 for r in results if r.success),
        failure_count=sum(1 for r in results if not r.success),
        by_platform=by_platform
    )


class AsyncOrchestrator:
    """
    Async orchestrator for parallel task execution across platforms.

    Features:
    - TaskGroup-based parallel execution with fail-independent semantics
    - Semaphore-controlled concurrency to prevent quota exhaustion
    - Adaptive concurrency based on remaining quota
    - Result aggregation with success/failure tracking
    """

    def __init__(
        self,
        adapters: Dict[Platform, AsyncBasePlatformAdapter] = None,
        router: TaskRouter = None,
        quota_tracker: QuotaTracker = None,
        max_concurrent: int = 10,
    ):
        """
        Initialize orchestrator.

        Args:
            adapters: Dict of Platform -> adapter instance
            router: TaskRouter instance (created if not provided)
            quota_tracker: QuotaTracker instance (created if not provided)
            max_concurrent: Base concurrent task limit (default: 10)
        """
        self._adapters = adapters or {}
        self._router = router or TaskRouter()
        self._quota = quota_tracker or QuotaTracker()
        self._max_concurrent = max_concurrent
        self._semaphore: Optional[Semaphore] = None

    async def __aenter__(self):
        """Enter async context - initialize adapters."""
        for adapter in self._adapters.values():
            await adapter.__aenter__()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Exit async context - cleanup adapters."""
        for adapter in self._adapters.values():
            await adapter.__aexit__(exc_type, exc_val, exc_tb)

    async def execute_single(self, task: Task) -> Result:
        """Execute a single task on the best platform."""
        # Route to best platform
        platform = self._router.route_cost_optimized(task, self._quota)
        adapter = self._adapters.get(platform)

        if not adapter:
            return Result(
                task=task,
                platform=platform,
                output="",
                success=False,
                status=TaskStatus.FAILED,
                error=f"No adapter for platform: {platform.value}",
            )

        # Execute via adapter
        task_id = await adapter.submit_task(task)
        timeout = task.constraints.timeout_seconds if task.constraints else 300
        result = await adapter.get_result(task_id, timeout=timeout)

        return result

    async def execute_parallel(
        self,
        tasks: List[Task],
        max_concurrent: Optional[int] = None,
    ) -> AggregatedResult:
        """
        Execute tasks in parallel with fail-independent semantics.

        All tasks run concurrently (up to semaphore limit). Failures in one
        task do not cancel others. Results collected for all tasks.

        Args:
            tasks: List of tasks to execute
            max_concurrent: Override concurrent limit for this batch

        Returns:
            AggregatedResult with all results and aggregated metrics
        """
        limit = max_concurrent or self._max_concurrent
        semaphore = Semaphore(limit)

        results: Dict[int, Result] = {}
        errors: Dict[int, Exception] = {}

        async def execute_one(idx: int, task: Task):
            """Execute single task with semaphore control."""
            async with semaphore:
                try:
                    result = await self.execute_single(task)
                    results[idx] = result
                except Exception as e:
                    errors[idx] = e

        # Fail-independent: catch ExceptionGroup, results already collected
        try:
            async with TaskGroup() as tg:
                for idx, task in enumerate(tasks):
                    tg.create_task(execute_one(idx, task))
        except* Exception:
            # TaskGroup raises ExceptionGroup on failures
            # Results/errors already collected in dicts
            pass

        # Build ordered results list
        ordered_results = []
        for i in range(len(tasks)):
            if i in results:
                ordered_results.append(results[i])
            elif i in errors:
                ordered_results.append(self._error_result(tasks[i], errors[i]))
            else:
                ordered_results.append(self._error_result(tasks[i], RuntimeError("Unknown error")))

        return aggregate_results(ordered_results)

    def _error_result(self, task: Task, error: Exception) -> Result:
        """Create error Result from exception."""
        return Result(
            task=task,
            platform=task.platform_hint or Platform.CLAUDE_CODE,
            output="",
            success=False,
            status=TaskStatus.FAILED,
            error=str(error),
        )

    def get_adaptive_concurrency(self, platform: Platform) -> int:
        """
        Calculate adaptive concurrency limit based on remaining quota.

        Per CONTEXT.md: Scale concurrent tasks based on available quota.

        Returns:
            Recommended concurrency limit (min 1, max self._max_concurrent)
        """
        status = self._quota.get_status()
        quota_key = self._quota._platform_to_quota_key(platform.value)
        platform_status = status.get(quota_key, {})

        available = platform_status.get("available", "unlimited")

        if available == "unlimited":
            return self._max_concurrent

        # Don't exceed 50% of remaining quota in one batch
        return max(1, min(self._max_concurrent, available // 2))
```
  </action>
  <verify>
Import verification: `python -c "from ta_lab2.tools.ai_orchestrator.execution import AsyncOrchestrator, AggregatedResult, aggregate_results; print('OK')"`
  </verify>
  <done>
AsyncOrchestrator class exists with execute_parallel method using TaskGroup, semaphore control, and fail-independent semantics; AggregatedResult dataclass provides result aggregation
  </done>
</task>

<task type="auto">
  <name>Task 2: Create comprehensive tests for execution engine</name>
  <files>tests/orchestrator/test_execution.py</files>
  <action>
Create test file with comprehensive coverage:

```python
"""Tests for async execution engine."""
import asyncio
import pytest
from unittest.mock import AsyncMock, Mock, patch

from ta_lab2.tools.ai_orchestrator.execution import (
    AsyncOrchestrator,
    AggregatedResult,
    aggregate_results,
)
from ta_lab2.tools.ai_orchestrator.core import Task, Result, TaskType, Platform, TaskStatus


class TestAggregatedResult:
    """Test AggregatedResult dataclass."""

    def test_success_rate_all_successful(self):
        """100% success rate when all succeed."""

    def test_success_rate_some_failures(self):
        """Correct rate with mixed results."""

    def test_success_rate_empty(self):
        """0% when no results."""

    def test_all_succeeded_property(self):
        """all_succeeded is True when no failures."""


class TestAggregateResults:
    """Test aggregate_results function."""

    def test_aggregates_costs(self):
        """Total cost is sum of all result costs."""

    def test_aggregates_tokens(self):
        """Total tokens is sum of all tokens."""

    def test_groups_by_platform(self):
        """Results grouped by platform in by_platform dict."""


class TestAsyncOrchestrator:
    """Test AsyncOrchestrator class."""

    @pytest.mark.asyncio
    async def test_execute_single_routes_to_adapter(self):
        """execute_single uses router and adapter correctly."""

    @pytest.mark.asyncio
    async def test_execute_parallel_runs_concurrently(self):
        """Tasks run in parallel, not sequentially."""

    @pytest.mark.asyncio
    async def test_execute_parallel_fail_independent(self):
        """One failure doesn't cancel other tasks."""

    @pytest.mark.asyncio
    async def test_semaphore_limits_concurrency(self):
        """Semaphore prevents more than max_concurrent tasks."""

    @pytest.mark.asyncio
    async def test_results_in_original_order(self):
        """Results returned in same order as input tasks."""

    @pytest.mark.asyncio
    async def test_error_results_for_failed_tasks(self):
        """Failed tasks get error Result objects."""


class TestAdaptiveConcurrency:
    """Test adaptive concurrency calculation."""

    def test_unlimited_quota_uses_max(self):
        """When quota unlimited, use max_concurrent."""

    def test_limited_quota_scales_down(self):
        """When quota limited, scale to 50% of remaining."""

    def test_minimum_one_concurrent(self):
        """Never return less than 1."""
```

Tests should use AsyncMock for adapter methods and verify:
- Parallel execution (measure time vs sequential)
- Fail-independent (other tasks complete despite one failure)
- Semaphore limiting (max concurrent at any time)
- Result ordering preserved
  </action>
  <verify>
Run tests: `pytest tests/orchestrator/test_execution.py -v`
  </verify>
  <done>
All execution engine tests pass, covering parallel execution, fail-independent semantics, semaphore control, and result aggregation
  </done>
</task>

</tasks>

<verification>
1. Import verification: `python -c "from ta_lab2.tools.ai_orchestrator.execution import AsyncOrchestrator, AggregatedResult"`
2. Test suite: `pytest tests/orchestrator/test_execution.py -v`
3. All orchestrator tests pass: `pytest tests/orchestrator/ -v`
</verification>

<success_criteria>
- AsyncOrchestrator.execute_parallel runs tasks concurrently via TaskGroup
- Semaphore limits concurrent tasks (default 10, configurable)
- Fail-independent: one task failure doesn't cancel others (per CONTEXT.md decision)
- AggregatedResult provides success_count, failure_count, total_cost, by_platform
- Tests verify concurrent execution and fail-independent behavior
</success_criteria>

<output>
After completion, create `.planning/phases/05-orchestrator-coordination/05-02-SUMMARY.md`
</output>
