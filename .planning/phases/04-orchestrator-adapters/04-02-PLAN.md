---
phase: 04-orchestrator-adapters
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/ta_lab2/tools/ai_orchestrator/adapters.py
  - src/ta_lab2/tools/ai_orchestrator/retry.py
  - tests/orchestrator/test_chatgpt_adapter.py
autonomous: true
user_setup:
  - service: openai
    why: "ChatGPT API access for task execution"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Platform -> API Keys -> Create new secret key"
    dashboard_config: []

must_haves:
  truths:
    - "ChatGPT adapter can submit tasks via OpenAI API and return task_id"
    - "ChatGPT adapter can stream responses via async generator"
    - "ChatGPT adapter tracks token usage from API response"
    - "ChatGPT adapter retries on rate limit errors with exponential backoff"
    - "ChatGPT adapter can be cancelled mid-stream"
  artifacts:
    - path: "src/ta_lab2/tools/ai_orchestrator/adapters.py"
      provides: "AsyncChatGPTAdapter implementation"
      contains: "class AsyncChatGPTAdapter"
    - path: "src/ta_lab2/tools/ai_orchestrator/retry.py"
      provides: "Retry decorators with exponential backoff"
      exports: ["retry_on_rate_limit"]
    - path: "tests/orchestrator/test_chatgpt_adapter.py"
      provides: "ChatGPT adapter tests with mocked OpenAI client"
      min_lines: 80
  key_links:
    - from: "AsyncChatGPTAdapter"
      to: "openai.AsyncOpenAI"
      via: "client initialization"
      pattern: "AsyncOpenAI"
    - from: "AsyncChatGPTAdapter.stream_result"
      to: "Result.metadata"
      via: "token tracking"
      pattern: "tokens_used.*usage"
---

<objective>
Implement ChatGPT async adapter with OpenAI API integration

Purpose: Enable task execution via OpenAI's GPT-4 API with streaming support, proper token tracking for cost management, and retry logic for rate limits per ORCH-02 requirement.

Output: Working AsyncChatGPTAdapter that implements all lifecycle methods, retry.py module with tenacity decorators, and comprehensive tests with mocked OpenAI client.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-orchestrator-adapters/04-CONTEXT.md
@.planning/phases/04-orchestrator-adapters/04-RESEARCH.md
@src/ta_lab2/tools/ai_orchestrator/adapters.py
@src/ta_lab2/tools/ai_orchestrator/core.py
@src/ta_lab2/tools/ai_orchestrator/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create retry.py module with exponential backoff decorator</name>
  <files>src/ta_lab2/tools/ai_orchestrator/retry.py</files>
  <action>
Create retry.py with tenacity-based retry decorators:

```python
"""Retry logic with exponential backoff for API calls."""
from __future__ import annotations

import logging
from functools import wraps
from typing import Callable, TypeVar

from tenacity import (
    retry,
    retry_if_exception_type,
    wait_exponential_jitter,
    stop_after_attempt,
    before_sleep_log,
)

logger = logging.getLogger(__name__)

# Type variable for generic decorator
F = TypeVar('F', bound=Callable)


def retry_on_rate_limit(
    max_attempts: int = 5,
    initial_wait: float = 1.0,
    max_wait: float = 32.0,
    jitter: float = 3.0,
) -> Callable[[F], F]:
    """
    Decorator for retrying API calls on rate limit errors.

    Uses exponential backoff with jitter per AWS/OpenAI best practices.
    Handles OpenAI RateLimitError and APIError.

    Args:
        max_attempts: Maximum retry attempts (default: 5)
        initial_wait: Initial wait in seconds (default: 1.0)
        max_wait: Maximum wait in seconds (default: 32.0)
        jitter: Random jitter range in seconds (default: 3.0)

    Example:
        @retry_on_rate_limit()
        async def call_api():
            return await client.chat.completions.create(...)
    """
    # Import here to avoid hard dependency if openai not installed
    try:
        import openai
        retry_exceptions = (openai.RateLimitError, openai.APIError)
    except ImportError:
        # Fallback to generic exceptions if openai not installed
        retry_exceptions = (Exception,)
        logger.warning("openai not installed, retry decorator using generic Exception")

    return retry(
        retry=retry_if_exception_type(retry_exceptions),
        wait=wait_exponential_jitter(initial=initial_wait, max=max_wait, jitter=jitter),
        stop=stop_after_attempt(max_attempts),
        before_sleep=before_sleep_log(logger, logging.WARNING),
        reraise=True,
    )


def retry_on_transient(
    max_attempts: int = 3,
    initial_wait: float = 0.5,
    max_wait: float = 10.0,
) -> Callable[[F], F]:
    """
    Decorator for retrying on transient network errors.

    Lighter retry policy for connection issues.

    Args:
        max_attempts: Maximum retry attempts (default: 3)
        initial_wait: Initial wait in seconds (default: 0.5)
        max_wait: Maximum wait in seconds (default: 10.0)
    """
    import aiohttp

    return retry(
        retry=retry_if_exception_type((
            aiohttp.ClientError,
            ConnectionError,
            TimeoutError,
        )),
        wait=wait_exponential_jitter(initial=initial_wait, max=max_wait, jitter=1.0),
        stop=stop_after_attempt(max_attempts),
        before_sleep=before_sleep_log(logger, logging.INFO),
        reraise=True,
    )


__all__ = ['retry_on_rate_limit', 'retry_on_transient']
```

Note: This requires tenacity package. Add to pyproject.toml if not present:
```bash
pip install tenacity
```
  </action>
  <verify>
```bash
python -c "from ta_lab2.tools.ai_orchestrator.retry import retry_on_rate_limit; print('retry module works')"
```
  </verify>
  <done>
- retry.py exists with retry_on_rate_limit and retry_on_transient decorators
- Uses tenacity library for robust retry logic
- Handles OpenAI-specific exceptions (RateLimitError, APIError)
- Logs retry attempts via before_sleep_log
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement AsyncChatGPTAdapter in adapters.py</name>
  <files>src/ta_lab2/tools/ai_orchestrator/adapters.py</files>
  <action>
Add AsyncChatGPTAdapter class implementing AsyncBasePlatformAdapter:

1. Add imports at top:
```python
from typing import AsyncIterator
import asyncio
import os
from openai import AsyncOpenAI
```

2. Implement AsyncChatGPTAdapter:

```python
class AsyncChatGPTAdapter(AsyncBasePlatformAdapter):
    """
    Async adapter for ChatGPT via OpenAI API.

    Features:
    - Async API calls with openai.AsyncOpenAI
    - Streaming support via async generators
    - Token tracking from API responses
    - Retry on rate limits with exponential backoff
    """

    def __init__(
        self,
        api_key: str | None = None,
        model: str = "gpt-4o-mini",
        timeout: float = 60.0,
    ):
        """
        Initialize ChatGPT adapter.

        Args:
            api_key: OpenAI API key (defaults to OPENAI_API_KEY env var)
            model: Default model to use (default: gpt-4o-mini for cost efficiency)
            timeout: Default timeout for API calls in seconds
        """
        self._api_key = api_key or os.environ.get("OPENAI_API_KEY")
        self._model = model
        self._timeout = timeout
        self._client: AsyncOpenAI | None = None
        self._pending_tasks: dict[str, asyncio.Task] = {}

    async def __aenter__(self):
        """Initialize async client."""
        if not self._api_key:
            raise ValueError("OPENAI_API_KEY not set")
        self._client = AsyncOpenAI(api_key=self._api_key, timeout=self._timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Cleanup client."""
        if self._client:
            await self._client.close()
            self._client = None

    @property
    def is_implemented(self) -> bool:
        """Check if adapter is usable."""
        return bool(self._api_key)

    @property
    def implementation_status(self) -> str:
        """Return implementation status."""
        if not self._api_key:
            return "unavailable"
        return "working"

    def get_adapter_status(self) -> dict:
        """Return comprehensive adapter status."""
        return {
            "name": "ChatGPT (Async)",
            "is_implemented": self.is_implemented,
            "status": self.implementation_status,
            "model": self._model,
            "capabilities": [
                "OpenAI API integration",
                "Streaming responses",
                "Token tracking",
                "Retry on rate limits",
            ],
            "requirements": [
                f"OPENAI_API_KEY {'(set)' if self._api_key else '(missing)'}"
            ],
        }

    async def submit_task(self, task: Task) -> str:
        """Submit task and return task_id."""
        from .core import TaskStatus

        task_id = self._generate_task_id(task)
        task.task_id = task_id

        # Create background task for execution
        self._pending_tasks[task_id] = asyncio.create_task(
            self._execute_internal(task)
        )

        return task_id

    async def get_status(self, task_id: str) -> TaskStatus:
        """Get task status."""
        from .core import TaskStatus

        if task_id not in self._pending_tasks:
            return TaskStatus.UNKNOWN

        task_obj = self._pending_tasks[task_id]
        if task_obj.done():
            if task_obj.cancelled():
                return TaskStatus.CANCELLED
            if task_obj.exception():
                return TaskStatus.FAILED
            return TaskStatus.COMPLETED
        return TaskStatus.RUNNING

    async def get_result(self, task_id: str, timeout: float = 300) -> Result:
        """Get complete result, waiting if necessary."""
        from .core import TaskStatus, Platform

        if task_id not in self._pending_tasks:
            # Return error result for unknown task
            return Result(
                task=Task(type=TaskType.CODE_GENERATION, prompt=""),
                platform=Platform.CHATGPT,
                output="",
                success=False,
                status=TaskStatus.UNKNOWN,
                error=f"Unknown task_id: {task_id}",
            )

        try:
            result = await asyncio.wait_for(
                self._pending_tasks[task_id],
                timeout=timeout
            )
            return result
        except asyncio.TimeoutError:
            return Result(
                task=Task(type=TaskType.CODE_GENERATION, prompt=""),
                platform=Platform.CHATGPT,
                output="",
                success=False,
                status=TaskStatus.FAILED,
                error=f"Task timed out after {timeout}s",
            )
        except asyncio.CancelledError:
            raise  # Re-raise cancellation

    async def stream_result(self, task_id: str) -> AsyncIterator[str]:
        """Stream result chunks."""
        # For simplicity, execute inline for streaming
        # A production implementation would track streaming tasks separately
        if task_id not in self._pending_tasks:
            return

        task_obj = self._pending_tasks.get(task_id)
        if task_obj and not task_obj.done():
            # Task still running - can't stream a background task
            # This is a limitation - streaming needs direct execution
            return

        # Fallback: yield the complete result
        result = await self.get_result(task_id)
        if result.output:
            yield result.output

    async def cancel_task(self, task_id: str) -> bool:
        """Cancel a running task."""
        if task_id not in self._pending_tasks:
            return False

        task_obj = self._pending_tasks[task_id]
        if task_obj.done():
            return False

        task_obj.cancel()
        try:
            await task_obj
        except asyncio.CancelledError:
            pass
        return True

    async def _execute_internal(self, task: Task) -> Result:
        """Internal execution with retry logic."""
        from .core import TaskStatus, Platform
        from .retry import retry_on_rate_limit
        import time

        start_time = time.time()

        if not self._client:
            return Result(
                task=task,
                platform=Platform.CHATGPT,
                output="",
                success=False,
                status=TaskStatus.FAILED,
                error="Client not initialized. Use 'async with' context manager.",
            )

        try:
            # Build messages
            messages = [{"role": "user", "content": task.prompt}]

            # Add context if provided
            if task.context:
                context_str = "\n".join(
                    f"{k}: {v}" for k, v in task.context.items()
                )
                messages.insert(0, {
                    "role": "system",
                    "content": f"Context:\n{context_str}"
                })

            # Determine model
            model = task.constraints.model if task.constraints and task.constraints.model else self._model

            # Apply retry decorator dynamically
            @retry_on_rate_limit()
            async def make_request():
                return await self._client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=task.constraints.max_tokens if task.constraints else None,
                    temperature=task.constraints.temperature if task.constraints else None,
                )

            response = await make_request()

            # Extract output and token usage
            output = response.choices[0].message.content or ""
            tokens_used = response.usage.total_tokens if response.usage else 0

            # Calculate cost (approximate for gpt-4o-mini)
            # Input: $0.15/1M tokens, Output: $0.60/1M tokens
            input_tokens = response.usage.prompt_tokens if response.usage else 0
            output_tokens = response.usage.completion_tokens if response.usage else 0
            cost = (input_tokens * 0.15 + output_tokens * 0.60) / 1_000_000

            return Result(
                task=task,
                platform=Platform.CHATGPT,
                output=output,
                success=True,
                status=TaskStatus.COMPLETED,
                tokens_used=tokens_used,
                cost=cost,
                duration_seconds=time.time() - start_time,
                metadata={
                    "model": model,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                },
            )

        except asyncio.CancelledError:
            raise  # Always re-raise CancelledError
        except Exception as e:
            return Result(
                task=task,
                platform=Platform.CHATGPT,
                output="",
                success=False,
                status=TaskStatus.FAILED,
                error=str(e),
                duration_seconds=time.time() - start_time,
            )

    async def execute_streaming(self, task: Task) -> AsyncIterator[str]:
        """Execute task with streaming response."""
        from .retry import retry_on_rate_limit

        if not self._client:
            raise RuntimeError("Client not initialized. Use 'async with' context manager.")

        # Build messages
        messages = [{"role": "user", "content": task.prompt}]
        if task.context:
            context_str = "\n".join(f"{k}: {v}" for k, v in task.context.items())
            messages.insert(0, {"role": "system", "content": f"Context:\n{context_str}"})

        model = task.constraints.model if task.constraints and task.constraints.model else self._model

        @retry_on_rate_limit()
        async def make_streaming_request():
            return await self._client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=task.constraints.max_tokens if task.constraints else None,
                temperature=task.constraints.temperature if task.constraints else None,
                stream=True,
                stream_options={"include_usage": True},
            )

        stream = await make_streaming_request()

        async for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
```

3. Add helper method _generate_task_id to AsyncBasePlatformAdapter if not already present.
  </action>
  <verify>
```bash
python -c "
from ta_lab2.tools.ai_orchestrator.adapters import AsyncChatGPTAdapter
adapter = AsyncChatGPTAdapter()
print(f'Status: {adapter.implementation_status}')
print(f'Adapter info: {adapter.get_adapter_status()}')
"
```
  </verify>
  <done>
- AsyncChatGPTAdapter class exists in adapters.py
- Implements all 5 lifecycle methods (submit_task, get_status, get_result, stream_result, cancel_task)
- Has async context manager support (__aenter__, __aexit__)
- Uses retry_on_rate_limit decorator for API calls
- Tracks tokens and calculates approximate cost
- Has execute_streaming method for direct streaming
  </done>
</task>

<task type="auto">
  <name>Task 3: Create comprehensive tests for ChatGPT adapter</name>
  <files>tests/orchestrator/test_chatgpt_adapter.py</files>
  <action>
Create test_chatgpt_adapter.py with mocked OpenAI client:

```python
"""Tests for AsyncChatGPTAdapter."""
import pytest
from unittest.mock import AsyncMock, MagicMock, patch
import asyncio

from ta_lab2.tools.ai_orchestrator.adapters import AsyncChatGPTAdapter
from ta_lab2.tools.ai_orchestrator.core import Task, TaskType, TaskStatus, TaskConstraints


@pytest.fixture
def mock_openai_response():
    """Create mock OpenAI response."""
    response = MagicMock()
    response.choices = [MagicMock()]
    response.choices[0].message.content = "Test response from ChatGPT"
    response.usage = MagicMock()
    response.usage.total_tokens = 150
    response.usage.prompt_tokens = 50
    response.usage.completion_tokens = 100
    return response


@pytest.fixture
def mock_openai_client(mock_openai_response):
    """Create mock AsyncOpenAI client."""
    client = AsyncMock()
    client.chat.completions.create = AsyncMock(return_value=mock_openai_response)
    client.close = AsyncMock()
    return client


@pytest.fixture
def sample_task():
    """Create sample task for testing."""
    return Task(
        type=TaskType.CODE_GENERATION,
        prompt="Write a hello world function in Python",
    )


class TestAsyncChatGPTAdapterInit:
    """Test adapter initialization."""

    def test_init_with_api_key(self):
        """Test initialization with explicit API key."""
        adapter = AsyncChatGPTAdapter(api_key="test-key")
        assert adapter.is_implemented is True
        assert adapter.implementation_status == "working"

    def test_init_without_api_key(self):
        """Test initialization without API key."""
        with patch.dict("os.environ", {}, clear=True):
            adapter = AsyncChatGPTAdapter(api_key=None)
            # May be True or False depending on env
            assert adapter.implementation_status in ["working", "unavailable"]

    def test_get_adapter_status(self):
        """Test adapter status reporting."""
        adapter = AsyncChatGPTAdapter(api_key="test-key")
        status = adapter.get_adapter_status()

        assert status["name"] == "ChatGPT (Async)"
        assert "capabilities" in status
        assert "Streaming responses" in status["capabilities"]


class TestAsyncChatGPTAdapterExecution:
    """Test task execution."""

    @pytest.mark.asyncio
    async def test_submit_task_returns_task_id(self, mock_openai_client, sample_task):
        """Test that submit_task returns a valid task_id."""
        adapter = AsyncChatGPTAdapter(api_key="test-key")
        adapter._client = mock_openai_client

        task_id = await adapter.submit_task(sample_task)

        assert task_id is not None
        assert "_" in task_id  # Format: platform_timestamp_uuid

    @pytest.mark.asyncio
    async def test_get_status_running(self, mock_openai_client, sample_task):
        """Test status check for running task."""
        # Make the API call hang
        never_complete = asyncio.Event()
        mock_openai_client.chat.completions.create = AsyncMock(
            side_effect=lambda **kwargs: never_complete.wait()
        )

        adapter = AsyncChatGPTAdapter(api_key="test-key")
        adapter._client = mock_openai_client

        task_id = await adapter.submit_task(sample_task)
        await asyncio.sleep(0.01)  # Let task start

        status = await adapter.get_status(task_id)
        assert status == TaskStatus.RUNNING

        # Cleanup
        await adapter.cancel_task(task_id)

    @pytest.mark.asyncio
    async def test_get_result_success(self, mock_openai_client, sample_task):
        """Test successful result retrieval."""
        adapter = AsyncChatGPTAdapter(api_key="test-key")
        adapter._client = mock_openai_client

        task_id = await adapter.submit_task(sample_task)
        result = await adapter.get_result(task_id, timeout=5.0)

        assert result.success is True
        assert result.status == TaskStatus.COMPLETED
        assert result.output == "Test response from ChatGPT"
        assert result.tokens_used == 150
        assert result.metadata["input_tokens"] == 50
        assert result.metadata["output_tokens"] == 100

    @pytest.mark.asyncio
    async def test_get_result_timeout(self, sample_task):
        """Test timeout handling."""
        # Create adapter with slow mock
        adapter = AsyncChatGPTAdapter(api_key="test-key")
        adapter._client = AsyncMock()
        adapter._client.chat.completions.create = AsyncMock(
            side_effect=lambda **kwargs: asyncio.sleep(10)
        )

        task_id = await adapter.submit_task(sample_task)
        result = await adapter.get_result(task_id, timeout=0.1)

        assert result.success is False
        assert "timed out" in result.error

    @pytest.mark.asyncio
    async def test_cancel_task(self, sample_task):
        """Test task cancellation."""
        adapter = AsyncChatGPTAdapter(api_key="test-key")
        adapter._client = AsyncMock()

        # Slow API call
        async def slow_call(**kwargs):
            await asyncio.sleep(10)
            return MagicMock()

        adapter._client.chat.completions.create = slow_call

        task_id = await adapter.submit_task(sample_task)
        await asyncio.sleep(0.01)  # Let task start

        cancelled = await adapter.cancel_task(task_id)
        assert cancelled is True

        status = await adapter.get_status(task_id)
        assert status == TaskStatus.CANCELLED


class TestAsyncChatGPTAdapterContextManager:
    """Test async context manager."""

    @pytest.mark.asyncio
    async def test_context_manager_init_close(self):
        """Test context manager initializes and closes client."""
        with patch("ta_lab2.tools.ai_orchestrator.adapters.AsyncOpenAI") as mock_class:
            mock_instance = AsyncMock()
            mock_class.return_value = mock_instance

            async with AsyncChatGPTAdapter(api_key="test-key") as adapter:
                assert adapter._client is not None

            mock_instance.close.assert_called_once()

    @pytest.mark.asyncio
    async def test_context_manager_raises_without_key(self):
        """Test context manager raises if no API key."""
        with patch.dict("os.environ", {}, clear=True):
            adapter = AsyncChatGPTAdapter(api_key=None)
            with pytest.raises(ValueError, match="OPENAI_API_KEY"):
                async with adapter:
                    pass


class TestTaskWithConstraints:
    """Test task constraints handling."""

    @pytest.mark.asyncio
    async def test_constraints_passed_to_api(self, mock_openai_client):
        """Test that constraints are passed to API call."""
        adapter = AsyncChatGPTAdapter(api_key="test-key")
        adapter._client = mock_openai_client

        task = Task(
            type=TaskType.CODE_GENERATION,
            prompt="Test",
            constraints=TaskConstraints(
                max_tokens=100,
                temperature=0.5,
                model="gpt-4",
            ),
        )

        task_id = await adapter.submit_task(task)
        await adapter.get_result(task_id)

        # Check API was called with constraints
        call_kwargs = mock_openai_client.chat.completions.create.call_args.kwargs
        assert call_kwargs["model"] == "gpt-4"
        assert call_kwargs["max_tokens"] == 100
        assert call_kwargs["temperature"] == 0.5


class TestUnknownTask:
    """Test handling of unknown tasks."""

    @pytest.mark.asyncio
    async def test_get_status_unknown(self):
        """Test status for unknown task_id."""
        adapter = AsyncChatGPTAdapter(api_key="test-key")
        status = await adapter.get_status("nonexistent-task-id")
        assert status == TaskStatus.UNKNOWN

    @pytest.mark.asyncio
    async def test_get_result_unknown(self):
        """Test result for unknown task_id."""
        adapter = AsyncChatGPTAdapter(api_key="test-key")
        result = await adapter.get_result("nonexistent-task-id")
        assert result.success is False
        assert result.status == TaskStatus.UNKNOWN
```
  </action>
  <verify>
```bash
pytest tests/orchestrator/test_chatgpt_adapter.py -v
```
  </verify>
  <done>
- test_chatgpt_adapter.py exists with 12+ test cases
- Tests cover: initialization, execution, timeout, cancellation, context manager, constraints
- All tests use mocked OpenAI client (no real API calls)
- All tests pass
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Module imports:
```bash
python -c "
from ta_lab2.tools.ai_orchestrator.retry import retry_on_rate_limit
from ta_lab2.tools.ai_orchestrator.adapters import AsyncChatGPTAdapter
print('All ChatGPT modules import successfully')
"
```

2. Run ChatGPT adapter tests:
```bash
pytest tests/orchestrator/test_chatgpt_adapter.py -v
```

3. Verify adapter status:
```bash
python -c "
from ta_lab2.tools.ai_orchestrator.adapters import AsyncChatGPTAdapter
adapter = AsyncChatGPTAdapter(api_key='test')
print(adapter.get_adapter_status())
"
```
</verification>

<success_criteria>
- retry.py exists with retry_on_rate_limit decorator
- AsyncChatGPTAdapter implements all 5 lifecycle methods
- Adapter has execute_streaming for direct streaming
- Token tracking works (tokens_used, cost calculation)
- Retry logic handles OpenAI rate limits
- Context manager properly initializes and closes client
- 12+ tests pass with mocked OpenAI client
- CancelledError is always re-raised (not swallowed)
</success_criteria>

<output>
After completion, create `.planning/phases/04-orchestrator-adapters/04-02-SUMMARY.md`
</output>
