---
phase: 04-orchestrator-adapters
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ta_lab2/tools/ai_orchestrator/core.py
  - src/ta_lab2/tools/ai_orchestrator/adapters.py
  - src/ta_lab2/tools/ai_orchestrator/streaming.py
  - tests/orchestrator/test_async_base.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Task dataclass accepts prompt, context, files, constraints as structured fields"
    - "Result dataclass includes output, status, metadata (tokens/cost/duration), files"
    - "BasePlatformAdapter defines async lifecycle methods (submit_task, get_status, get_result, cancel_task, stream_result)"
    - "TaskStatus enum represents pending/running/completed/failed/cancelled states"
    - "Existing sync adapters continue to work during migration"
  artifacts:
    - path: "src/ta_lab2/tools/ai_orchestrator/core.py"
      provides: "Enhanced Task, Result, TaskStatus dataclasses"
      contains: "class TaskStatus"
    - path: "src/ta_lab2/tools/ai_orchestrator/adapters.py"
      provides: "AsyncBasePlatformAdapter ABC"
      contains: "async def submit_task"
    - path: "src/ta_lab2/tools/ai_orchestrator/streaming.py"
      provides: "Streaming result helpers"
      exports: ["StreamingResult", "collect_stream"]
    - path: "tests/orchestrator/test_async_base.py"
      provides: "Tests for async base infrastructure"
      min_lines: 50
  key_links:
    - from: "src/ta_lab2/tools/ai_orchestrator/adapters.py"
      to: "src/ta_lab2/tools/ai_orchestrator/core.py"
      via: "imports Task, Result, TaskStatus"
      pattern: "from \\.core import.*Task.*Result.*TaskStatus"
---

<objective>
Create async base infrastructure for platform adapters

Purpose: Establish the foundation for all three adapters with async execution model, comprehensive lifecycle methods, and structured Task/Result objects per Phase 4 CONTEXT decisions.

Output: Enhanced core.py with async-ready dataclasses, new AsyncBasePlatformAdapter ABC in adapters.py, streaming.py helper module, and comprehensive tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-orchestrator-adapters/04-CONTEXT.md
@.planning/phases/04-orchestrator-adapters/04-RESEARCH.md
@src/ta_lab2/tools/ai_orchestrator/core.py
@src/ta_lab2/tools/ai_orchestrator/adapters.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enhance core.py with async-ready Task and Result dataclasses</name>
  <files>src/ta_lab2/tools/ai_orchestrator/core.py</files>
  <action>
Update core.py with enhanced dataclasses:

1. Add TaskStatus enum:
   - PENDING, RUNNING, COMPLETED, FAILED, CANCELLED, UNKNOWN

2. Add TaskConstraints dataclass:
   - max_tokens: Optional[int] - token limit
   - timeout_seconds: Optional[float] - execution timeout (default 300)
   - temperature: Optional[float] - model temperature
   - model: Optional[str] - specific model to use

3. Enhance Task dataclass:
   - Keep existing: type, prompt, platform_hint, priority, requires_gsd, metadata, created_at
   - Add: context: dict[str, Any] - memory context, previous task outputs
   - Add: files: list[str] - file paths for Claude Code operations
   - Add: constraints: Optional[TaskConstraints] - execution constraints
   - Add: task_id: Optional[str] - assigned when submitted (UUID format)

4. Enhance Result dataclass:
   - Keep existing: task, platform, output, success, error, cost, tokens_used, duration_seconds, metadata, completed_at
   - Add: status: TaskStatus - execution state
   - Add: files_created: list[str] - output files generated
   - Add: partial_output: Optional[str] - for streaming/cancellation

5. Keep backward compatibility: existing sync code should continue to work with default values for new fields.

Do NOT change the existing sync Orchestrator class or execute methods - those will be updated in Phase 5.
  </action>
  <verify>
Run existing tests to verify backward compatibility:
```bash
pytest tests/orchestrator/test_validation.py tests/orchestrator/test_smoke.py -v
```
All existing tests should pass.
  </verify>
  <done>
- TaskStatus enum exists with all states
- TaskConstraints dataclass exists
- Task has context, files, constraints, task_id fields
- Result has status, files_created, partial_output fields
- All existing tests pass (backward compatible)
  </done>
</task>

<task type="auto">
  <name>Task 2: Create AsyncBasePlatformAdapter ABC in adapters.py</name>
  <files>src/ta_lab2/tools/ai_orchestrator/adapters.py</files>
  <action>
Add new AsyncBasePlatformAdapter class alongside existing BasePlatformAdapter (keep existing for backward compat):

1. Create AsyncBasePlatformAdapter(ABC):
   - Abstract async methods per CONTEXT decisions:
     - async def submit_task(self, task: Task) -> str: Submit and return task_id
     - async def get_status(self, task_id: str) -> TaskStatus: Check task status
     - async def get_result(self, task_id: str, timeout: float = 300) -> Result: Get complete result (blocks until done or timeout)
     - async def stream_result(self, task_id: str) -> AsyncIterator[str]: Yield partial chunks
     - async def cancel_task(self, task_id: str) -> bool: Cancel running task

   - Property methods (keep from existing):
     - @property is_implemented -> bool
     - @property implementation_status -> str
     - get_adapter_status() -> dict

   - Async context manager support:
     - async def __aenter__(self) -> Self
     - async def __aexit__(self, exc_type, exc_val, exc_tb) -> None

2. Add _pending_tasks dict for tracking submitted tasks internally

3. Add utility methods in base class:
   - def _generate_task_id(self, task: Task) -> str: Generate unique task ID (format: {platform}_{timestamp}_{uuid8})
   - async def _wait_with_timeout(self, coro, timeout: float): Wrapper for asyncio.wait_for with proper CancelledError handling

4. Keep existing sync BasePlatformAdapter and all adapter implementations unchanged - they will be migrated in plans 02-04.

Import typing.AsyncIterator and asyncio at top.
  </action>
  <verify>
```bash
python -c "from ta_lab2.tools.ai_orchestrator.adapters import AsyncBasePlatformAdapter, BasePlatformAdapter; print('Both base classes importable')"
```
  </verify>
  <done>
- AsyncBasePlatformAdapter ABC exists with all 5 async lifecycle methods
- Async context manager protocol implemented (__aenter__, __aexit__)
- _generate_task_id and _wait_with_timeout utilities exist
- Existing BasePlatformAdapter and sync adapters unchanged
  </done>
</task>

<task type="auto">
  <name>Task 3: Create streaming.py helper module and tests</name>
  <files>src/ta_lab2/tools/ai_orchestrator/streaming.py, tests/orchestrator/test_async_base.py</files>
  <action>
1. Create streaming.py with helpers for streaming results:

```python
"""Streaming result handlers for async adapters."""
from __future__ import annotations

import asyncio
from dataclasses import dataclass, field
from typing import AsyncIterator, Optional
from datetime import datetime, timezone


@dataclass
class StreamingResult:
    """Accumulates streaming chunks into final result."""
    chunks: list[str] = field(default_factory=list)
    total_tokens: int = 0
    started_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    completed_at: Optional[datetime] = None

    def add_chunk(self, chunk: str):
        """Add a chunk to the accumulator."""
        self.chunks.append(chunk)

    def get_content(self) -> str:
        """Get accumulated content."""
        return "".join(self.chunks)

    def complete(self, tokens: int = 0):
        """Mark streaming as complete."""
        self.completed_at = datetime.now(timezone.utc)
        self.total_tokens = tokens

    @property
    def duration_seconds(self) -> float:
        """Get duration in seconds."""
        end = self.completed_at or datetime.now(timezone.utc)
        return (end - self.started_at).total_seconds()


async def collect_stream(stream: AsyncIterator[str], timeout: float = 300) -> StreamingResult:
    """Collect all chunks from an async stream into a StreamingResult.

    Args:
        stream: Async iterator yielding string chunks
        timeout: Maximum time to wait for stream completion

    Returns:
        StreamingResult with all accumulated chunks

    Raises:
        asyncio.TimeoutError: If stream doesn't complete within timeout
        asyncio.CancelledError: If collection is cancelled (re-raised after cleanup)
    """
    result = StreamingResult()
    try:
        async with asyncio.timeout(timeout):
            async for chunk in stream:
                result.add_chunk(chunk)
    except asyncio.CancelledError:
        # Save partial results before re-raising
        result.complete()
        raise

    result.complete()
    return result
```

2. Create tests/orchestrator/test_async_base.py:

Test cases:
- test_task_status_enum_values: Verify all TaskStatus values exist
- test_task_constraints_defaults: TaskConstraints has reasonable defaults
- test_task_enhanced_fields: Task has context, files, constraints, task_id
- test_result_enhanced_fields: Result has status, files_created, partial_output
- test_streaming_result_accumulation: StreamingResult collects chunks correctly
- test_collect_stream_timeout: collect_stream respects timeout
- test_async_base_adapter_abstract: Cannot instantiate AsyncBasePlatformAdapter directly
- test_generate_task_id_format: Task ID follows expected format

Use pytest-asyncio for async tests (@pytest.mark.asyncio decorator).
  </action>
  <verify>
```bash
pytest tests/orchestrator/test_async_base.py -v
```
All tests pass.
  </verify>
  <done>
- streaming.py exists with StreamingResult and collect_stream
- test_async_base.py has 8+ test cases
- All async tests pass with pytest-asyncio
- StreamingResult handles partial results on cancellation
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Import verification:
```bash
python -c "
from ta_lab2.tools.ai_orchestrator.core import Task, Result, TaskStatus, TaskConstraints
from ta_lab2.tools.ai_orchestrator.adapters import AsyncBasePlatformAdapter, BasePlatformAdapter
from ta_lab2.tools.ai_orchestrator.streaming import StreamingResult, collect_stream
print('All imports successful')
"
```

2. Run all orchestrator tests:
```bash
pytest tests/orchestrator/ -v --ignore=tests/orchestrator/test_full_migration.py
```

3. Verify existing adapters still work:
```bash
python -c "
from ta_lab2.tools.ai_orchestrator import Orchestrator
o = Orchestrator()
status = o.validate_environment()
print(f'Adapters: {list(status.keys())}')
"
```
</verification>

<success_criteria>
- TaskStatus enum with PENDING, RUNNING, COMPLETED, FAILED, CANCELLED, UNKNOWN
- TaskConstraints dataclass with max_tokens, timeout_seconds, temperature, model
- Task dataclass with context, files, constraints, task_id (all optional with defaults)
- Result dataclass with status, files_created, partial_output (all optional with defaults)
- AsyncBasePlatformAdapter ABC with 5 async lifecycle methods
- streaming.py with StreamingResult and collect_stream helpers
- test_async_base.py with 8+ passing tests
- All existing orchestrator tests still pass (backward compatibility)
</success_criteria>

<output>
After completion, create `.planning/phases/04-orchestrator-adapters/04-01-SUMMARY.md`
</output>
