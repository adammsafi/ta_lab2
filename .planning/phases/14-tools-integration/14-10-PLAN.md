---
phase: 14-tools-integration
plan: 10
type: execute
wave: 6
depends_on: ["14-09"]
files_modified:
  - src/ta_lab2/tools/docs/update_doc_memory.py
autonomous: true

must_haves:
  truths:
    - "Memory contains moved_to relationships for all migrated scripts"
    - "Phase 14 snapshot created in Mem0 with metadata"
    - "Memory queries can find migrated script locations"
  artifacts: []
  key_links:
    - from: "Memory"
      to: "src/ta_lab2/tools/data_tools/*"
      via: "moved_to relationship"
      pattern: "Data_Tools.*->.*data_tools"
---

<objective>
Update memory with Data_Tools migration relationships and create phase snapshot

Purpose: Fulfill MEMO-13 (moved_to relationships) and MEMO-14 (phase snapshot) for Phase 14. Memory should track where each script moved from and to.

Output: Mem0 memories for migration tracking; phase 14 completion snapshot
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/14-tools-integration/14-CONTEXT.md
@.planning/phases/14-tools-integration/14-01-SUMMARY.md

# Phase 13 memory patterns
@.planning/phases/13-documentation-consolidation/13-06-SUMMARY.md
@src/ta_lab2/tools/docs/update_doc_memory.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create migration memory records</name>
  <files>None (memory operations)</files>
  <action>
Use Mem0 client to create migration records for each migrated script:

Following Phase 13 patterns (batch with infer=False):

```python
from ta_lab2.tools.ai_orchestrator.memory.mem0_client import get_mem0_client

client = get_mem0_client()

# Migration records for each script
migrations = [
    {
        "source": "C:/Users/asafi/Downloads/Data_Tools/generate_function_map.py",
        "target": "src/ta_lab2/tools/data_tools/analysis/generate_function_map.py",
        "category": "analysis",
    },
    {
        "source": "C:/Users/asafi/Downloads/Data_Tools/tree_structure.py",
        "target": "src/ta_lab2/tools/data_tools/analysis/tree_structure.py",
        "category": "analysis",
    },
    # ... all migrated scripts from 14-01-discovery.json
]

for m in migrations:
    memory_text = f"""Script migration: {m['source'].split('/')[-1]}
Moved from: {m['source']}
Moved to: {m['target']}
Category: {m['category']}
Phase: 14 (Tools Integration)
Relationship: moved_to"""

    client.add(
        memory_text,
        user_id="ta_lab2_system",
        metadata={
            "type": "script_migration",
            "source_path": m['source'],
            "target_path": m['target'],
            "category": m['category'],
            "phase": "14",
            "phase_name": "tools-integration",
            "relationship": "moved_to",
            "created_at": "2026-02-XX",
        },
        infer=False,  # Batch mode following Phase 11 patterns
    )
```

Create records for:
- All migrated scripts (from 14-01-discovery.json "migrate" entries)
- Archived scripts (with relationship: "archived_to")
  </action>
  <verify>Memory queries return migration records</verify>
  <done>Migration memories created for all scripts with moved_to relationships</done>
</task>

<task type="auto">
  <name>Task 2: Create phase 14 completion snapshot</name>
  <files>None (memory operations)</files>
  <action>
Create phase completion snapshot following Phase 13 pattern:

```python
from ta_lab2.tools.ai_orchestrator.memory.mem0_client import get_mem0_client
from datetime import datetime

client = get_mem0_client()

# Gather statistics from 14-01-discovery.json
stats = {
    "total_scripts": 51,
    "migrated_count": N,  # From discovery
    "archived_count": M,  # From discovery
    "categories": ["analysis", "database_utils", "memory", "export", "generators", "context"],
    "target_package": "src/ta_lab2/tools/data_tools/",
    "tests_created": 2,  # smoke tests + path validation
}

snapshot_text = f"""Phase 14: Tools Integration - Completion Snapshot

Summary: Migrated {stats['migrated_count']} scripts from external Data_Tools directory into ta_lab2/tools/data_tools/ package. Archived {stats['archived_count']} prototype/one-off scripts.

Key Accomplishments:
- Created data_tools package with {len(stats['categories'])} functional categories
- Migrated analysis, database_utils, memory, export, generators, context tools
- Standardized imports to ta_lab2 patterns
- Removed hardcoded paths, added CLI arguments
- Created smoke tests and path validation tests
- Archived non-migrated scripts with manifest

Package Structure:
{stats['target_package']}
├── analysis/ ({stats.get('analysis_count', 'N')} scripts)
├── database_utils/ ({stats.get('database_count', 'N')} scripts)
├── memory/ ({stats.get('memory_count', 'N')} scripts)
├── export/ ({stats.get('export_count', 'N')} scripts)
├── generators/ ({stats.get('generator_count', 'N')} scripts)
└── context/ ({stats.get('context_count', 'N')} scripts)

Requirements Satisfied:
- TOOL-01: Data_Tools scripts moved to src/ta_lab2/tools/data_tools/
- TOOL-02: All import paths updated (no hardcoded paths remain)
- TOOL-03: pytest smoke tests pass for migrated scripts
- MEMO-13: Memory updated with moved_to relationships
- MEMO-14: Phase-level memory snapshot created
"""

client.add(
    snapshot_text,
    user_id="ta_lab2_system",
    metadata={
        "type": "phase_snapshot",
        "phase": "14",
        "phase_name": "tools-integration",
        "completed_at": datetime.now().isoformat(),
        "scripts_migrated": stats['migrated_count'],
        "scripts_archived": stats['archived_count'],
        "categories": stats['categories'],
        "requirements_satisfied": ["TOOL-01", "TOOL-02", "TOOL-03", "MEMO-13", "MEMO-14"],
    },
    infer=False,
)
```
  </action>
  <verify>Phase snapshot queryable via semantic search</verify>
  <done>Phase 14 snapshot created with comprehensive metadata</done>
</task>

<task type="auto">
  <name>Task 3: Verify memory queries work</name>
  <files>None (verification only)</files>
  <action>
Test memory queries to confirm migration tracking works:

```python
from ta_lab2.tools.ai_orchestrator.memory.mem0_client import get_mem0_client

client = get_mem0_client()

# Query 1: Find where a specific script moved
results = client.search(
    "Where is generate_function_map.py now?",
    user_id="ta_lab2_system",
)
print("Query 1 results:", results[:2])  # Should show migration record

# Query 2: Find all migrated memory tools
results = client.search(
    "Data_Tools memory scripts migration",
    user_id="ta_lab2_system",
)
print("Query 2 results:", results[:3])

# Query 3: What was Phase 14?
results = client.search(
    "Phase 14 tools integration",
    user_id="ta_lab2_system",
)
print("Query 3 results:", results[:2])  # Should show snapshot
```

Document query results in SUMMARY.

If queries don't return expected results:
- Check metadata is correct
- Verify memories were added (not just dry run)
- Check user_id matches
  </action>
  <verify>All three queries return relevant results</verify>
  <done>Memory queries verified; migration tracking complete</done>
</task>

</tasks>

<verification>
- Memory search for "generate_function_map migration" returns record with moved_to relationship
- Memory search for "Phase 14" returns completion snapshot
- Metadata includes phase, relationship type, paths
</verification>

<success_criteria>
1. Migration memories created for all migrated scripts (MEMO-13)
2. Archive memories created for archived scripts
3. Phase 14 completion snapshot created (MEMO-14)
4. Memory queries return relevant results
5. Metadata includes: source_path, target_path, category, phase, relationship
</success_criteria>

<output>
After completion, create `.planning/phases/14-tools-integration/14-10-SUMMARY.md`
</output>
