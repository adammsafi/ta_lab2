---
phase: 14-tools-integration
plan: 11
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ta_lab2/tools/data_tools/memory/generate_memories_from_diffs.py
  - src/ta_lab2/tools/data_tools/memory/generate_memories_from_conversations.py
  - src/ta_lab2/tools/data_tools/memory/instantiate_final_memories.py
  - src/ta_lab2/tools/data_tools/memory/memory_headers_dedup.py
  - src/ta_lab2/tools/data_tools/memory/memory_headers_step1_deterministic.py
  - src/ta_lab2/tools/data_tools/memory/memory_headers_step2_openai_enrich.py
  - src/ta_lab2/tools/data_tools/memory/memory_instantiate_children_step3.py
  - src/ta_lab2/tools/data_tools/memory/memory_bank_engine_rest.py
  - src/ta_lab2/tools/data_tools/memory/memory_build_registry.py
  - src/ta_lab2/tools/data_tools/memory/combine_memories.py
  - src/ta_lab2/tools/data_tools/memory/__init__.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "All 10 missing memory pipeline scripts exist in src/ta_lab2/tools/data_tools/memory/"
    - "All scripts have ta_lab2 import paths (no sys.path manipulation)"
    - "All hardcoded paths converted to CLI arguments"
    - "memory/__init__.py exports all memory tools"
  artifacts:
    - path: "src/ta_lab2/tools/data_tools/memory/generate_memories_from_diffs.py"
      provides: "Git diff memory generation (~58KB)"
      min_lines: 500
    - path: "src/ta_lab2/tools/data_tools/memory/instantiate_final_memories.py"
      provides: "Final memory processing (~16KB)"
      min_lines: 150
    - path: "src/ta_lab2/tools/data_tools/memory/memory_instantiate_children_step3.py"
      provides: "Child memory instantiation (~20KB)"
      min_lines: 200
    - path: "src/ta_lab2/tools/data_tools/memory/memory_bank_engine_rest.py"
      provides: "Memory Bank with reasoning engine support"
      min_lines: 100
  key_links:
    - from: "src/ta_lab2/tools/data_tools/memory/__init__.py"
      to: "all 10 new scripts"
      via: "module-level imports"
      pattern: "from .generate_memories_from_diffs import"
    - from: "src/ta_lab2/tools/data_tools/context/create_reasoning_engine.py"
      to: "memory_bank_engine_rest.py"
      via: "import statement"
      pattern: "from ta_lab2.tools.data_tools.memory"
---

<objective>
Migrate 10 missing memory pipeline scripts from Data_Tools/chatgpt/ to complete Phase 14 gap closure.

Purpose: Complete the memory tools migration (5/16 scripts migrated, 11 missing). These scripts form the core memory generation, processing, and management infrastructure.

Output: All memory pipeline scripts migrated with working imports, CLI-first design, and graceful dependency handling.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-tools-integration/14-05-SUMMARY.md
@.planning/phases/14-tools-integration/14-01-discovery.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate large memory generation scripts (3 scripts, ~94KB)</name>
  <files>
    src/ta_lab2/tools/data_tools/memory/generate_memories_from_diffs.py
    src/ta_lab2/tools/data_tools/memory/instantiate_final_memories.py
    src/ta_lab2/tools/data_tools/memory/memory_instantiate_children_step3.py
  </files>
  <action>
    Migrate the three largest memory scripts from C:/Users/asafi/Downloads/Data_Tools/chatgpt/:

    1. **generate_memories_from_diffs.py** (58KB):
       - Copy from Data_Tools/chatgpt/generate_memories_from_diffs.py
       - Check for hardcoded paths (convert to CLI args with argparse)
       - Wrap openai/chromadb imports in try/except ImportError with helpful messages
       - Verify no sys.path manipulation
       - Add module docstring if missing

    2. **instantiate_final_memories.py** (16KB):
       - Copy from Data_Tools/chatgpt/instantiate_final_memories.py
       - Same cleanup pattern: hardcoded paths -> CLI, dependency handling
       - This is the final step in memory processing pipeline

    3. **memory_instantiate_children_step3.py** (20KB):
       - Copy from Data_Tools/chatgpt/memory_instantiate_children_step3.py
       - Same cleanup pattern
       - Part of multi-step memory pipeline (step 3)

    Follow patterns from 14-05: CLI-first with argparse, try/except ImportError for openai, consistent logging.
  </action>
  <verify>
    python -c "from ta_lab2.tools.data_tools.memory import generate_memories_from_diffs"
    python -c "from ta_lab2.tools.data_tools.memory import instantiate_final_memories"
    python -c "from ta_lab2.tools.data_tools.memory import memory_instantiate_children_step3"

    All three import without errors (dependencies may be missing but import itself succeeds).
  </verify>
  <done>
    Three large memory scripts exist in memory/ with working ta_lab2 imports, no hardcoded paths, graceful dependency handling.
  </done>
</task>

<task type="auto">
  <name>Task 2: Migrate memory header processing pipeline (4 scripts)</name>
  <files>
    src/ta_lab2/tools/data_tools/memory/memory_headers_dedup.py
    src/ta_lab2/tools/data_tools/memory/memory_headers_step1_deterministic.py
    src/ta_lab2/tools/data_tools/memory/memory_headers_step2_openai_enrich.py
    src/ta_lab2/tools/data_tools/memory/generate_memories_from_conversations.py
  </files>
  <action>
    Migrate the memory header processing pipeline from C:/Users/asafi/Downloads/Data_Tools/chatgpt/:

    1. **memory_headers_dedup.py** (~2KB):
       - Deduplicates memory headers
       - Simple script, likely minimal cleanup needed

    2. **memory_headers_step1_deterministic.py** (~8KB):
       - First step: deterministic header extraction
       - Check for hardcoded paths

    3. **memory_headers_step2_openai_enrich.py** (~10KB):
       - Second step: OpenAI-based enrichment
       - Wrap openai import in try/except

    4. **generate_memories_from_conversations.py** (~7KB):
       - Generates memories from ChatGPT conversation exports
       - Uses OpenAI API

    Apply same patterns: CLI-first, graceful dependency handling, no hardcoded paths.
  </action>
  <verify>
    python -c "from ta_lab2.tools.data_tools.memory import memory_headers_dedup"
    python -c "from ta_lab2.tools.data_tools.memory import memory_headers_step1_deterministic"
    python -c "from ta_lab2.tools.data_tools.memory import memory_headers_step2_openai_enrich"
    python -c "from ta_lab2.tools.data_tools.memory import generate_memories_from_conversations"

    All four import without errors.
  </verify>
  <done>
    Memory header pipeline scripts (dedup, step1, step2) and conversation memory generator migrated with working imports.
  </done>
</task>

<task type="auto">
  <name>Task 3: Migrate remaining memory tools and update __init__.py (3 scripts)</name>
  <files>
    src/ta_lab2/tools/data_tools/memory/memory_bank_engine_rest.py
    src/ta_lab2/tools/data_tools/memory/memory_build_registry.py
    src/ta_lab2/tools/data_tools/memory/combine_memories.py
    src/ta_lab2/tools/data_tools/memory/__init__.py
  </files>
  <action>
    Migrate remaining memory tools from C:/Users/asafi/Downloads/Data_Tools/chatgpt/:

    1. **memory_bank_engine_rest.py** (~11KB):
       - Enhanced Memory Bank client with reasoning engine support
       - Uses google.auth for GCP authentication
       - CRITICAL: This is imported by context/create_reasoning_engine.py (currently causing import failure)

    2. **memory_build_registry.py** (~7KB):
       - Builds registry of memory sources and metadata
       - Memory management infrastructure

    3. **combine_memories.py** (~2KB):
       - Simple JSONL file merger for memory consolidation

    4. **Update memory/__init__.py**:
       - Add exports for all 10 new scripts
       - Follow existing pattern: brief description + usage examples
       - Update __all__ list

    Apply same patterns: CLI-first, graceful dependency handling, no hardcoded paths.
  </action>
  <verify>
    python -c "from ta_lab2.tools.data_tools.memory import memory_bank_engine_rest"
    python -c "from ta_lab2.tools.data_tools.memory import memory_build_registry"
    python -c "from ta_lab2.tools.data_tools.memory import combine_memories"
    python -c "from ta_lab2.tools.data_tools import memory; print(len(dir(memory)))"

    All imports work and memory module has increased exports.
  </verify>
  <done>
    All 10 missing memory scripts migrated. memory/__init__.py updated with comprehensive exports. Import failure in create_reasoning_engine.py resolved.
  </done>
</task>

</tasks>

<verification>
1. All 10 new scripts exist in src/ta_lab2/tools/data_tools/memory/
2. All scripts import without errors: `python -c "from ta_lab2.tools.data_tools import memory"`
3. No hardcoded paths: grep for "C:" or "/Users" returns empty in new scripts
4. No sys.path manipulation: grep for "sys.path" returns empty in new scripts
5. create_reasoning_engine.py import now works (memory_bank_engine_rest dependency satisfied)
</verification>

<success_criteria>
- 10 scripts added to memory/ directory (total now 15 scripts)
- All scripts use ta_lab2 import paths
- All hardcoded paths converted to CLI arguments
- memory/__init__.py exports all 15 memory tools
- context/create_reasoning_engine.py imports successfully
- pytest test_imports_smoke.py passes for all new modules
</success_criteria>

<output>
After completion, create `.planning/phases/14-tools-integration/14-11-SUMMARY.md`
</output>
