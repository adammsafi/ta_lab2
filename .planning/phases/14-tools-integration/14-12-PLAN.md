---
phase: 14-tools-integration
plan: 12
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ta_lab2/tools/data_tools/analysis/generate_function_map_with_purpose.py
  - src/ta_lab2/tools/data_tools/analysis/__init__.py
  - src/ta_lab2/tools/data_tools/generators/generate_commits_txt.py
  - src/ta_lab2/tools/data_tools/generators/__init__.py
  - src/ta_lab2/tools/data_tools/processing/DataFrame_Consolidation.py
  - src/ta_lab2/tools/data_tools/processing/__init__.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "generate_function_map_with_purpose.py exists in analysis/"
    - "generate_commits_txt.py exists in generators/"
    - "DataFrame_Consolidation.py exists in processing/"
    - "All three scripts have working ta_lab2 imports"
  artifacts:
    - path: "src/ta_lab2/tools/data_tools/analysis/generate_function_map_with_purpose.py"
      provides: "Enhanced function mapper with purpose inference (~14KB)"
      min_lines: 150
    - path: "src/ta_lab2/tools/data_tools/generators/generate_commits_txt.py"
      provides: "Git commit history exporter (~7KB)"
      min_lines: 80
    - path: "src/ta_lab2/tools/data_tools/processing/DataFrame_Consolidation.py"
      provides: "Time-series DataFrame merging utilities (~6KB)"
      min_lines: 60
  key_links:
    - from: "src/ta_lab2/tools/data_tools/analysis/__init__.py"
      to: "generate_function_map_with_purpose.py"
      via: "module export"
      pattern: "from .generate_function_map_with_purpose import"
    - from: "src/ta_lab2/tools/data_tools/generators/__init__.py"
      to: "generate_commits_txt.py"
      via: "module export"
      pattern: "generate_commits_txt"
    - from: "src/ta_lab2/tools/data_tools/processing/__init__.py"
      to: "DataFrame_Consolidation.py"
      via: "module export"
      pattern: "DataFrame_Consolidation"
---

<objective>
Migrate 3 remaining non-memory scripts (analysis, generators, processing) to complete Phase 14 migration.

Purpose: Close the final gaps in analysis (1/3), generators (5/6), and processing (0/1) categories.

Output: All 3 scripts migrated with working imports, completing the full 38-script migration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-tools-integration/14-03-SUMMARY.md
@.planning/phases/14-tools-integration/14-07-SUMMARY.md
@.planning/phases/14-tools-integration/14-01-discovery.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate generate_function_map_with_purpose.py to analysis/</name>
  <files>
    src/ta_lab2/tools/data_tools/analysis/generate_function_map_with_purpose.py
    src/ta_lab2/tools/data_tools/analysis/__init__.py
  </files>
  <action>
    Migrate enhanced function mapper from C:/Users/asafi/Downloads/Data_Tools/generate_function_map_with_purpose.py:

    1. **generate_function_map_with_purpose.py** (~14KB):
       - Copy from Data_Tools/generate_function_map_with_purpose.py (root level)
       - Enhanced version of generate_function_map.py with purpose inference from docstrings
       - Adds purpose detection, code snippets, and called symbols tracking
       - Uses ast, csv, argparse, pathlib, fnmatch (all standard library)
       - No external dependencies expected

    2. **Update analysis/__init__.py**:
       - Add export for generate_function_map_with_purpose
       - Update __all__ list
       - Add usage example in docstring

    Cleanup:
    - Check for hardcoded paths (unlikely based on discovery)
    - Ensure argparse CLI exists
    - Add module docstring if missing
  </action>
  <verify>
    python -c "from ta_lab2.tools.data_tools.analysis import generate_function_map_with_purpose"
    python -c "from ta_lab2.tools.data_tools.analysis import generate_function_map_with_purpose; print(generate_function_map_with_purpose.__doc__[:50] if generate_function_map_with_purpose.__doc__ else 'OK')"
  </verify>
  <done>
    generate_function_map_with_purpose.py migrated to analysis/ with working imports and exports.
  </done>
</task>

<task type="auto">
  <name>Task 2: Migrate generate_commits_txt.py to generators/</name>
  <files>
    src/ta_lab2/tools/data_tools/generators/generate_commits_txt.py
    src/ta_lab2/tools/data_tools/generators/__init__.py
  </files>
  <action>
    Migrate git commit exporter from C:/Users/asafi/Downloads/Data_Tools/chatgpt/generate_commits_txt.py:

    1. **generate_commits_txt.py** (~7KB):
       - Copy from Data_Tools/chatgpt/generate_commits_txt.py
       - Generates commits.txt from git history
       - Uses pathlib, subprocess (for git commands) - standard library only
       - Git analysis/export tool

    2. **Update generators/__init__.py**:
       - Add export for generate_commits_txt
       - Update __all__ list (if used)
       - Generators module already has 5 scripts, this completes the set

    Cleanup:
    - Check for hardcoded paths (convert to CLI args if found)
    - Ensure argparse CLI exists
    - Add module docstring if missing (note: generators scripts were flagged for missing docstrings)
  </action>
  <verify>
    python -c "from ta_lab2.tools.data_tools.generators import generate_commits_txt"
    File exists: src/ta_lab2/tools/data_tools/generators/generate_commits_txt.py
  </verify>
  <done>
    generate_commits_txt.py migrated to generators/ with working imports. Generators module now complete (6/6 scripts).
  </done>
</task>

<task type="auto">
  <name>Task 3: Migrate DataFrame_Consolidation.py to processing/</name>
  <files>
    src/ta_lab2/tools/data_tools/processing/DataFrame_Consolidation.py
    src/ta_lab2/tools/data_tools/processing/__init__.py
  </files>
  <action>
    Migrate DataFrame utilities from C:/Users/asafi/Downloads/Data_Tools/DataFrame_Consolidation.py:

    1. **DataFrame_Consolidation.py** (~6KB):
       - Copy from Data_Tools/DataFrame_Consolidation.py (root level)
       - General-purpose time-series DataFrame merging with differing granularities
       - Uses pandas, functools (pandas is external, functools is standard)
       - Well-documented with S/V comment style per discovery
       - No hardcoded paths per discovery

    2. **Update processing/__init__.py**:
       - Currently empty (just package marker)
       - Add module docstring describing processing tools
       - Add exports for DataFrame_Consolidation functions
       - Add __all__ list

    Cleanup:
    - Wrap pandas import in try/except ImportError
    - Add module docstring if missing
    - This is the only script in processing/ so make __init__.py comprehensive
  </action>
  <verify>
    python -c "from ta_lab2.tools.data_tools.processing import DataFrame_Consolidation"
    python -c "from ta_lab2.tools.data_tools import processing; print(dir(processing))"
  </verify>
  <done>
    DataFrame_Consolidation.py migrated to processing/ with working imports. Processing module now has its only planned script.
  </done>
</task>

</tasks>

<verification>
1. All 3 new scripts exist in their target directories
2. All scripts import without errors
3. __init__.py files updated with exports
4. No hardcoded paths in new scripts
5. test_imports_smoke.py updated or passes for new modules
</verification>

<success_criteria>
- generate_function_map_with_purpose.py in analysis/ (now 3/3 scripts)
- generate_commits_txt.py in generators/ (now 6/6 scripts)
- DataFrame_Consolidation.py in processing/ (now 1/1 script)
- All __init__.py files have comprehensive exports
- pytest smoke tests pass for all new modules
- Total migrated scripts: 38/38 (26 + 10 memory + 3 other = 39, but setup_mem0_direct.py is optional)
</success_criteria>

<output>
After completion, create `.planning/phases/14-tools-integration/14-12-SUMMARY.md`
</output>
