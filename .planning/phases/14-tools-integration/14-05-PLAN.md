---
phase: 14-tools-integration
plan: 05
type: execute
wave: 3
depends_on: ["14-02"]
files_modified:
  - src/ta_lab2/tools/data_tools/memory/embed_codebase.py
  - src/ta_lab2/tools/data_tools/memory/embed_memories.py
  - src/ta_lab2/tools/data_tools/memory/generate_memories_from_code.py
  - src/ta_lab2/tools/data_tools/memory/memory_bank_rest.py
  - src/ta_lab2/tools/data_tools/memory/setup_mem0.py
  - src/ta_lab2/tools/data_tools/memory/__init__.py
autonomous: true

must_haves:
  truths:
    - "Core memory/embedding tools migrated with working imports"
    - "OpenAI and ChromaDB dependencies handled with try/except ImportError"
    - "Scripts use ta_lab2 patterns for path handling"
  artifacts:
    - path: "src/ta_lab2/tools/data_tools/memory/embed_codebase.py"
      provides: "Codebase embedding tool"
      min_lines: 50
    - path: "src/ta_lab2/tools/data_tools/memory/__init__.py"
      provides: "Memory module exports"
      contains: "__all__"
  key_links:
    - from: "src/ta_lab2/tools/data_tools/memory/*.py"
      to: "openai"
      via: "optional import"
      pattern: "from openai import|try:.*openai"
---

<objective>
Migrate core memory and embedding tools from Data_Tools/chatgpt/

Purpose: Move the foundational AI memory infrastructure that was developed for embedding codebases and managing memory banks. These tools have external dependencies (OpenAI, ChromaDB) that need careful handling.

Output: Working memory module with core embedding and memory management tools
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/14-tools-integration/14-CONTEXT.md
@.planning/phases/14-tools-integration/14-RESEARCH.md
@.planning/phases/14-tools-integration/14-01-SUMMARY.md
@.planning/phases/14-tools-integration/14-02-SUMMARY.md

# Existing ta_lab2 memory patterns
@src/ta_lab2/tools/ai_orchestrator/memory/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate embedding tools</name>
  <files>
    src/ta_lab2/tools/data_tools/memory/embed_codebase.py
    src/ta_lab2/tools/data_tools/memory/embed_memories.py
  </files>
  <action>
Migrate from C:\Users\asafi\Downloads\Data_Tools\chatgpt\:

**embed_codebase.py** (7.9KB):
1. Read source file
2. Handle dependencies with graceful ImportError:
```python
try:
    from openai import OpenAI
except ImportError:
    raise ImportError(
        "OpenAI library required. Install with: pip install 'ta_lab2[orchestrator]'"
    )

try:
    import chromadb
except ImportError:
    raise ImportError(
        "ChromaDB library required. Install with: pip install chromadb"
    )
```

3. Replace hardcoded paths:
   - `--repo-dir` and `--chroma-dir` are already CLI args (good)
   - Check for any other hardcoded paths

4. Update imports to use pathlib consistently
5. Add module-level logger: `logger = logging.getLogger(__name__)`

**embed_memories.py** (5.6KB):
1. Same pattern: read, handle imports, update paths
2. Document what "memories" format it expects (likely JSON/CSV input)
  </action>
  <verify>
    python -c "from ta_lab2.tools.data_tools.memory.embed_codebase import get_code_chunks, get_embedding"
  </verify>
  <done>Embedding tools migrated with optional dependency handling</done>
</task>

<task type="auto">
  <name>Task 2: Migrate memory generation and bank tools</name>
  <files>
    src/ta_lab2/tools/data_tools/memory/generate_memories_from_code.py
    src/ta_lab2/tools/data_tools/memory/memory_bank_rest.py
    src/ta_lab2/tools/data_tools/memory/setup_mem0.py
  </files>
  <action>
Migrate from C:\Users\asafi\Downloads\Data_Tools\chatgpt\:

**generate_memories_from_code.py** (7.1KB):
1. This likely generates memory records from source code

2. **Duplication check (REQUIRED):**
   Run before migrating:
   ```bash
   # Check if main function names already exist in ai_orchestrator/memory/
   grep -r "def generate_memories" src/ta_lab2/tools/ai_orchestrator/memory/
   grep -r "def extract_memories" src/ta_lab2/tools/ai_orchestrator/memory/
   grep -r "class.*Memory.*Generator" src/ta_lab2/tools/ai_orchestrator/memory/
   ```

   **Decision criteria:**
   - If grep finds matching function/class names -> archive Data_Tools version, add docstring note pointing to existing implementation
   - If grep returns empty -> migrate as unique functionality
   - If partial overlap (similar but different signature/behavior) -> migrate with "_legacy" suffix and deprecation note

3. If unique, migrate with same patterns (pathlib, logging, ImportError handling)

**memory_bank_rest.py** (3.7KB):
1. REST API for memory bank (likely Flask-based)
2. Handle flask dependency with try/except
3. Document API endpoints in docstring

**setup_mem0.py** (4.7KB) and setup_mem0_direct.py (4.2KB):
1. **Duplication check (REQUIRED):**
   ```bash
   grep -r "def setup_mem0\|def configure_mem0\|class Mem0Config" src/ta_lab2/tools/ai_orchestrator/memory/
   ```

2. **Decision criteria:**
   - If grep finds matching patterns -> archive Data_Tools version
   - If grep returns empty -> migrate as unique functionality
   - Check mem0_config.py specifically - if setup logic exists there, archive Data_Tools versions

For each file:
- Remove sys.path manipulation
- Use ta_lab2 patterns for paths
- Add module docstrings
- Handle optional dependencies gracefully
  </action>
  <verify>
    python -c "from ta_lab2.tools.data_tools.memory import memory_bank_rest"
  </verify>
  <done>Memory generation and bank tools migrated (or archived if duplicates found)</done>
</task>

<task type="auto">
  <name>Task 3: Update memory __init__.py with exports and verify cross-imports</name>
  <files>src/ta_lab2/tools/data_tools/memory/__init__.py</files>
  <action>
Create comprehensive __init__.py:

```python
"""
Memory and embedding tools migrated from Data_Tools/chatgpt/.

This module provides utilities for:
- Embedding codebases into vector stores
- Generating memories from code
- REST API for memory bank access
- Mem0 setup utilities

Dependencies:
- OpenAI (for embeddings): pip install openai
- ChromaDB (for vector store): pip install chromadb
- Flask (for REST API): pip install flask

Usage:
    # Embed a codebase
    from ta_lab2.tools.data_tools.memory import embed_codebase

    # Or run as CLI
    python -m ta_lab2.tools.data_tools.memory.embed_codebase --repo-dir /path --chroma-dir /path

Note:
    For production memory operations, use ta_lab2.tools.ai_orchestrator.memory
    which provides Mem0/Qdrant integration. These tools are utilities for
    data preparation and experimentation.
"""
from ta_lab2.tools.data_tools.memory.embed_codebase import (
    get_code_chunks,
    get_embedding,
    main as embed_codebase_cli,
)
from ta_lab2.tools.data_tools.memory.embed_memories import (
    embed_memories,
)
from ta_lab2.tools.data_tools.memory.generate_memories_from_code import (
    generate_memories_from_code,
)

# REST API (optional - requires flask)
try:
    from ta_lab2.tools.data_tools.memory.memory_bank_rest import app as memory_bank_app
except ImportError:
    memory_bank_app = None  # Flask not installed

__all__ = [
    "get_code_chunks",
    "get_embedding",
    "embed_codebase_cli",
    "embed_memories",
    "generate_memories_from_code",
    "memory_bank_app",
]
```

Adjust based on actual functions found in migrated scripts (and remove any that were archived as duplicates).

**Cross-import verification:**
Check if migrated scripts reference other migrated scripts. If found:
- Ensure imports use ta_lab2 paths, NOT relative filesystem imports
- Example: `from ta_lab2.tools.data_tools.memory.embed_codebase import ...`
- NOT: `import embed_codebase` or bare module imports with sys.path hacks

Verify cross-imports use ta_lab2 paths:
```bash
# Should find ta_lab2 imports only (no bare module imports for migrated scripts)
grep -E "^(from|import)" src/ta_lab2/tools/data_tools/memory/*.py | grep -v "ta_lab2" | grep -v "^#" | grep -v "pathlib\|logging\|argparse\|openai\|chromadb\|flask\|json\|typing\|os\|sys"
```

If any non-stdlib, non-ta_lab2 imports reference other migrated scripts, update them.

Commit with message:
```
feat(14): migrate memory/embedding tools from Data_Tools

- embed_codebase.py: AST-based code chunking + OpenAI embeddings
- embed_memories.py: Memory record embedding
- generate_memories_from_code.py: Code -> memory generation
- memory_bank_rest.py: REST API for memory access
- setup_mem0.py: Mem0 configuration utilities
- All dependencies handled with graceful ImportError
```
  </action>
  <verify>
    - `python -c "from ta_lab2.tools.data_tools import memory; print(memory.__all__)"` succeeds
    - No bare imports of other migrated scripts (only ta_lab2 import paths)
  </verify>
  <done>memory module exports core functions; optional deps handled gracefully; cross-imports use ta_lab2 paths</done>
</task>

</tasks>

<verification>
- `python -c "from ta_lab2.tools.data_tools.memory import embed_codebase"` succeeds (or fails with helpful ImportError if openai not installed)
- `grep -r "sys.path" src/ta_lab2/tools/data_tools/memory/` returns nothing
- `grep -r "C:\\\\Users" src/ta_lab2/tools/data_tools/memory/` returns nothing
- Cross-script references use `from ta_lab2.tools.data_tools...` import paths
</verification>

<success_criteria>
1. Core embedding tools (embed_codebase, embed_memories) migrated
2. Memory generation tools migrated (or archived if duplicates confirmed)
3. REST API (memory_bank_rest) migrated with flask handling
4. Mem0 setup utilities migrated (or archived if duplicates confirmed)
5. All optional dependencies have try/except ImportError with helpful messages
6. memory/__init__.py exports public functions
7. No hardcoded paths or sys.path manipulation
8. Any cross-script imports use ta_lab2 import paths (not relative filesystem)
</success_criteria>

<output>
After completion, create `.planning/phases/14-tools-integration/14-05-SUMMARY.md`
</output>
