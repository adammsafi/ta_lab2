{
  "discovery_date": "2026-02-02",
  "data_tools_path": "C:/Users/asafi/Downloads/Data_Tools",
  "scripts": {
    "root": [
      {
        "filename": "generate_function_map.py",
        "path": "generate_function_map.py",
        "decision": "migrate",
        "rationale": "AST-based function/method mapper - generates CSV of all functions in repo. Useful for codebase analysis and documentation.",
        "category": "analysis",
        "target_dir": "src/ta_lab2/tools/data_tools/analysis/",
        "dependencies": ["ast", "csv", "argparse", "pathlib"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 8148,
        "notes": "Well-structured with proper argparse CLI, follows project patterns"
      },
      {
        "filename": "generate_function_map_with_purpose.py",
        "path": "generate_function_map_with_purpose.py",
        "decision": "migrate",
        "rationale": "Enhanced version of function mapper with purpose inference from docstrings and heuristics. More comprehensive than basic version.",
        "category": "analysis",
        "target_dir": "src/ta_lab2/tools/data_tools/analysis/",
        "dependencies": ["ast", "csv", "argparse", "pathlib", "fnmatch"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 13916,
        "notes": "Adds purpose detection, code snippets, and called symbols tracking"
      },
      {
        "filename": "tree_structure.py",
        "path": "tree_structure.py",
        "decision": "migrate",
        "rationale": "Generates directory tree visualizations (text, MD, JSON, CSV) and API maps. Useful for documentation and structure analysis.",
        "category": "analysis",
        "target_dir": "src/ta_lab2/tools/data_tools/analysis/",
        "dependencies": ["ast", "csv", "json", "pathlib", "importlib"],
        "has_hardcoded_paths": true,
        "complexity": "moderate",
        "size_bytes": 12317,
        "notes": "Has hardcoded ROOT path at bottom (line 327), needs parameterization"
      },
      {
        "filename": "DataFrame_Consolidation.py",
        "path": "DataFrame_Consolidation.py",
        "decision": "migrate",
        "rationale": "General-purpose time-series DataFrame merging utilities with differing granularities. Could be useful for multi-timeframe analysis.",
        "category": "processing",
        "target_dir": "src/ta_lab2/tools/data_tools/processing/",
        "dependencies": ["pandas", "functools"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 6146,
        "notes": "Well-documented with S/V comment style, no hardcoded paths"
      },
      {
        "filename": "write_daily_emas.py",
        "path": "write_daily_emas.py",
        "decision": "archive",
        "rationale": "Simple runner script - just imports ta_lab2.features.ema.write_daily_ema_to_db and calls it. Functionality already exists in ta_lab2.",
        "category": "archive",
        "target_dir": ".archive/data_tools/one_offs/",
        "dependencies": ["ta_lab2.features.ema"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 313,
        "notes": "One-off script, 18 lines total, no unique logic"
      },
      {
        "filename": "write_multi_tf_emas.py",
        "path": "write_multi_tf_emas.py",
        "decision": "archive",
        "rationale": "Simple runner script - imports and calls ta_lab2 multi-timeframe EMA writer. Functionality already exists.",
        "category": "archive",
        "target_dir": ".archive/data_tools/one_offs/",
        "dependencies": ["ta_lab2.features.m_tf.ema_multi_timeframe"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 346,
        "notes": "One-off script, 18 lines total"
      },
      {
        "filename": "write_ema_multi_tf_cal.py",
        "path": "write_ema_multi_tf_cal.py",
        "decision": "archive",
        "rationale": "Simple runner script - imports and calls ta_lab2 calendar-aligned multi-TF EMA writer. Functionality already exists.",
        "category": "archive",
        "target_dir": ".archive/data_tools/one_offs/",
        "dependencies": ["ta_lab2.features.m_tf.ema_multi_tf_cal"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 446,
        "notes": "One-off script, 18 lines total"
      },
      {
        "filename": "upsert_new_emas_canUpdate.py",
        "path": "upsert_new_emas_canUpdate.py",
        "decision": "archive",
        "rationale": "Helper script that imports and runs ta_lab2.scripts.run_ema_refresh_examples.example_incremental_all_ids_all_targets(). Functionality already in ta_lab2.",
        "category": "archive",
        "target_dir": ".archive/data_tools/one_offs/",
        "dependencies": ["ta_lab2.scripts.run_ema_refresh_examples"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 1272,
        "notes": "Wrapper script for existing ta_lab2 functionality"
      },
      {
        "filename": "github instruction.py",
        "path": "github instruction.py",
        "decision": "archive",
        "rationale": "One-off instruction file with git commands. Not a reusable tool, just a note/reminder.",
        "category": "archive",
        "target_dir": ".archive/data_tools/one_offs/",
        "dependencies": [],
        "has_hardcoded_paths": true,
        "complexity": "simple",
        "size_bytes": 203,
        "notes": "Just contains 3 git commands for initial repo setup"
      }
    ],
    "chatgpt": [
      {
        "filename": "embed_codebase.py",
        "path": "chatgpt/embed_codebase.py",
        "decision": "migrate",
        "rationale": "AST-based code chunking and embedding generator for AI memory systems. Core memory infrastructure tool.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["openai", "chromadb", "ast", "pathlib"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 7973,
        "notes": "Uses OpenAI embeddings and ChromaDB for code indexing"
      },
      {
        "filename": "embed_memories.py",
        "path": "chatgpt/embed_memories.py",
        "decision": "migrate",
        "rationale": "Embeds memory objects (not code) for semantic search. Complements embed_codebase for memory retrieval.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["openai", "chromadb", "pathlib"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 5680,
        "notes": "Memory-specific embedding tool"
      },
      {
        "filename": "generate_memories_from_code.py",
        "path": "chatgpt/generate_memories_from_code.py",
        "decision": "migrate",
        "rationale": "Uses OpenAI to generate structured memories from code chunks. Core memory generation tool.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["openai", "ast", "pathlib"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 7178,
        "notes": "Generates memories via Chat Completions API"
      },
      {
        "filename": "generate_memories_from_conversations.py",
        "path": "chatgpt/generate_memories_from_conversations.py",
        "decision": "migrate",
        "rationale": "Generates memories from ChatGPT conversation exports. Useful for conversation analysis and knowledge extraction.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["openai", "pathlib"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 6854,
        "notes": "Processes conversation history to extract knowledge"
      },
      {
        "filename": "generate_memories_from_diffs.py",
        "path": "chatgpt/generate_memories_from_diffs.py",
        "decision": "migrate",
        "rationale": "Large (58KB) tool for generating memories from git diffs. Complex but potentially valuable for tracking code evolution.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["openai", "pathlib", "git"],
        "has_hardcoded_paths": false,
        "complexity": "complex",
        "size_bytes": 58320,
        "notes": "Largest script, comprehensive diff-to-memory pipeline"
      },
      {
        "filename": "combine_memories.py",
        "path": "chatgpt/combine_memories.py",
        "decision": "migrate",
        "rationale": "Merges multiple memory JSONL files, useful for consolidating memory sources.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 1822,
        "notes": "Simple memory file merger"
      },
      {
        "filename": "memory_bank_rest.py",
        "path": "chatgpt/memory_bank_rest.py",
        "decision": "migrate",
        "rationale": "REST client for Vertex AI Memory Bank (retrieve/create). Infrastructure for GCP-based memory storage.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["google.auth", "requests"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 3789,
        "notes": "GCP Vertex AI integration"
      },
      {
        "filename": "memory_bank_engine_rest.py",
        "path": "chatgpt/memory_bank_engine_rest.py",
        "decision": "migrate",
        "rationale": "Enhanced Memory Bank client with reasoning engine support. Extends memory_bank_rest functionality.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["google.auth", "requests"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 10934,
        "notes": "More comprehensive than memory_bank_rest"
      },
      {
        "filename": "memory_build_registry.py",
        "path": "chatgpt/memory_build_registry.py",
        "decision": "migrate",
        "rationale": "Builds registry of memory sources and their metadata. Memory management infrastructure.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 7285,
        "notes": "Registry management for memory sources"
      },
      {
        "filename": "memory_headers_dedup.py",
        "path": "chatgpt/memory_headers_dedup.py",
        "decision": "migrate",
        "rationale": "Deduplicates memory headers. Part of memory processing pipeline.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 1923,
        "notes": "Deduplication step in memory pipeline"
      },
      {
        "filename": "memory_headers_step1_deterministic.py",
        "path": "chatgpt/memory_headers_step1_deterministic.py",
        "decision": "migrate",
        "rationale": "Step 1 of memory header processing - deterministic extraction. Part of multi-step pipeline.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 7640,
        "notes": "First step in header extraction pipeline"
      },
      {
        "filename": "memory_headers_step2_openai_enrich.py",
        "path": "chatgpt/memory_headers_step2_openai_enrich.py",
        "decision": "migrate",
        "rationale": "Step 2 of memory header processing - OpenAI-based enrichment. Complements step1.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["openai", "pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 10473,
        "notes": "OpenAI enrichment step in pipeline"
      },
      {
        "filename": "memory_instantiate_children_step3.py",
        "path": "chatgpt/memory_instantiate_children_step3.py",
        "decision": "migrate",
        "rationale": "Step 3 of memory processing - instantiates child memories. Large (20KB), part of comprehensive pipeline.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["openai", "pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "complex",
        "size_bytes": 20121,
        "notes": "Child memory instantiation, complex logic"
      },
      {
        "filename": "instantiate_final_memories.py",
        "path": "chatgpt/instantiate_final_memories.py",
        "decision": "migrate",
        "rationale": "Finalizes memory instantiation. May overlap with step3, but includes final validation/formatting.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["openai", "pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 15712,
        "notes": "Final memory processing step"
      },
      {
        "filename": "setup_mem0.py",
        "path": "chatgpt/setup_mem0.py",
        "decision": "migrate",
        "rationale": "Mem0 integration setup script. Configures Mem0 memory system.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["mem0"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 4756,
        "notes": "Mem0 setup and configuration"
      },
      {
        "filename": "setup_mem0_direct.py",
        "path": "chatgpt/setup_mem0_direct.py",
        "decision": "migrate",
        "rationale": "Direct Mem0 setup variant. Alternative to setup_mem0.py with different initialization approach.",
        "category": "memory",
        "target_dir": "src/ta_lab2/tools/data_tools/memory/",
        "dependencies": ["mem0"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 4208,
        "notes": "Alternative Mem0 setup approach"
      },
      {
        "filename": "export_chatgpt_conversations.py",
        "path": "chatgpt/export_chatgpt_conversations.py",
        "decision": "migrate",
        "rationale": "Converts ChatGPT export JSON to Markdown transcripts and CSV index. Valuable for conversation analysis.",
        "category": "export",
        "target_dir": "src/ta_lab2/tools/data_tools/export/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 11635,
        "notes": "Well-documented ChatGPT export processor"
      },
      {
        "filename": "chatgpt_export_clean.py",
        "path": "chatgpt/chatgpt_export_clean.py",
        "decision": "migrate",
        "rationale": "Cleans ChatGPT export data. Part of export processing pipeline.",
        "category": "export",
        "target_dir": "src/ta_lab2/tools/data_tools/export/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 11544,
        "notes": "Export cleaning utilities"
      },
      {
        "filename": "chatgpt_export_diff.py",
        "path": "chatgpt/chatgpt_export_diff.py",
        "decision": "migrate",
        "rationale": "Large (24KB) script for diffing ChatGPT exports between versions. Useful for tracking export changes.",
        "category": "export",
        "target_dir": "src/ta_lab2/tools/data_tools/export/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "complex",
        "size_bytes": 23975,
        "notes": "Comprehensive export diff tool"
      },
      {
        "filename": "extract_kept_chats_from_keepfile.py",
        "path": "chatgpt/extract_kept_chats_from_keepfile.py",
        "decision": "migrate",
        "rationale": "Extracts specific chats based on keepfile. Useful for filtering conversation exports.",
        "category": "export",
        "target_dir": "src/ta_lab2/tools/data_tools/export/",
        "dependencies": ["pathlib"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 8436,
        "notes": "Chat filtering and extraction"
      },
      {
        "filename": "process_claude_history.py",
        "path": "chatgpt/process_claude_history.py",
        "decision": "migrate",
        "rationale": "Processes Claude conversation history for analysis. Complements ChatGPT export tools.",
        "category": "export",
        "target_dir": "src/ta_lab2/tools/data_tools/export/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 5323,
        "notes": "Claude conversation processor"
      },
      {
        "filename": "process_new_chatgpt_dump.py",
        "path": "chatgpt/process_new_chatgpt_dump.py",
        "decision": "migrate",
        "rationale": "Processes new ChatGPT data dumps. Export processing pipeline component.",
        "category": "export",
        "target_dir": "src/ta_lab2/tools/data_tools/export/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 8033,
        "notes": "New dump processor"
      },
      {
        "filename": "convert_claude_code_to_chatgpt_format.py",
        "path": "chatgpt/convert_claude_code_to_chatgpt_format.py",
        "decision": "migrate",
        "rationale": "Converts Claude Code format to ChatGPT format. Format conversion utility.",
        "category": "export",
        "target_dir": "src/ta_lab2/tools/data_tools/export/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 6123,
        "notes": "Format converter between AI tools"
      },
      {
        "filename": "ask_project.py",
        "path": "chatgpt/ask_project.py",
        "decision": "migrate",
        "rationale": "Large (17KB) RAG tool - semantic search over codebase with LLM chat. Core context/query tool.",
        "category": "context",
        "target_dir": "src/ta_lab2/tools/data_tools/context/",
        "dependencies": ["openai", "chromadb"],
        "has_hardcoded_paths": false,
        "complexity": "complex",
        "size_bytes": 17520,
        "notes": "RAG-based project Q&A system"
      },
      {
        "filename": "chat_with_context.py",
        "path": "chatgpt/chat_with_context.py",
        "decision": "migrate",
        "rationale": "Chat interface with semantic context injection. Complements ask_project.",
        "category": "context",
        "target_dir": "src/ta_lab2/tools/data_tools/context/",
        "dependencies": ["openai", "chromadb"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 5236,
        "notes": "Context-aware chat interface"
      },
      {
        "filename": "get_context.py",
        "path": "chatgpt/get_context.py",
        "decision": "migrate",
        "rationale": "Retrieves semantic context for queries. Context retrieval utility.",
        "category": "context",
        "target_dir": "src/ta_lab2/tools/data_tools/context/",
        "dependencies": ["chromadb"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 4600,
        "notes": "Context retrieval from ChromaDB"
      },
      {
        "filename": "create_reasoning_engine.py",
        "path": "chatgpt/create_reasoning_engine.py",
        "decision": "migrate",
        "rationale": "Creates Vertex AI reasoning engine. GCP infrastructure setup tool.",
        "category": "context",
        "target_dir": "src/ta_lab2/tools/data_tools/context/",
        "dependencies": ["google.auth"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 9816,
        "notes": "GCP reasoning engine creation"
      },
      {
        "filename": "query_reasoning_engine.py",
        "path": "chatgpt/query_reasoning_engine.py",
        "decision": "migrate",
        "rationale": "Queries Vertex AI reasoning engine. Complements create_reasoning_engine.",
        "category": "context",
        "target_dir": "src/ta_lab2/tools/data_tools/context/",
        "dependencies": ["google.auth"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 2754,
        "notes": "Reasoning engine query interface"
      },
      {
        "filename": "intelligence_report_generator.py",
        "path": "chatgpt/intelligence_report_generator.py",
        "decision": "migrate",
        "rationale": "Generates intelligence reports from memory JSONL files. Report generation tool.",
        "category": "generators",
        "target_dir": "src/ta_lab2/tools/data_tools/generators/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 7859,
        "notes": "Memory-based intelligence report generator"
      },
      {
        "filename": "category_digest_generator.py",
        "path": "chatgpt/category_digest_generator.py",
        "decision": "migrate",
        "rationale": "Generates category-based digests from memories. Report generation tool.",
        "category": "generators",
        "target_dir": "src/ta_lab2/tools/data_tools/generators/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 4645,
        "notes": "Category digest generator"
      },
      {
        "filename": "review_generator.py",
        "path": "chatgpt/review_generator.py",
        "decision": "migrate",
        "rationale": "Generates code/project reviews. Report generation tool.",
        "category": "generators",
        "target_dir": "src/ta_lab2/tools/data_tools/generators/",
        "dependencies": ["pathlib"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 3857,
        "notes": "Review generator"
      },
      {
        "filename": "review_triage_generator.py",
        "path": "chatgpt/review_triage_generator.py",
        "decision": "migrate",
        "rationale": "Generates review triage reports. Extends review_generator functionality.",
        "category": "generators",
        "target_dir": "src/ta_lab2/tools/data_tools/generators/",
        "dependencies": ["pathlib"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 4960,
        "notes": "Review triage generator"
      },
      {
        "filename": "finetuning_data_generator.py",
        "path": "chatgpt/finetuning_data_generator.py",
        "decision": "migrate",
        "rationale": "Generates training data for model finetuning. ML data preparation tool.",
        "category": "generators",
        "target_dir": "src/ta_lab2/tools/data_tools/generators/",
        "dependencies": ["pathlib", "json"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 6439,
        "notes": "Finetuning dataset generator"
      },
      {
        "filename": "generate_commits_txt.py",
        "path": "chatgpt/generate_commits_txt.py",
        "decision": "migrate",
        "rationale": "Generates commits.txt from git history. Git analysis/export tool.",
        "category": "generators",
        "target_dir": "src/ta_lab2/tools/data_tools/generators/",
        "dependencies": ["pathlib", "git"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 7403,
        "notes": "Git commit history exporter"
      },
      {
        "filename": "chatgpt_script_look.py",
        "path": "chatgpt/chatgpt_script_look.py",
        "decision": "archive",
        "rationale": "Prototype/test script - very small (452 bytes), likely exploratory. Numbered variations (look, look1, look2) indicate iteration.",
        "category": "archive",
        "target_dir": ".archive/data_tools/prototypes/",
        "dependencies": ["shutil", "pathlib"],
        "has_hardcoded_paths": true,
        "complexity": "simple",
        "size_bytes": 452,
        "notes": "Prototype, has hardcoded paths for specific task"
      },
      {
        "filename": "chatgpt_script_keep_look.py",
        "path": "chatgpt/chatgpt_script_keep_look.py",
        "decision": "archive",
        "rationale": "Prototype variant - numbered series (keep_look, keep_look1, keep_look2) indicates experimentation.",
        "category": "archive",
        "target_dir": ".archive/data_tools/prototypes/",
        "dependencies": ["shutil", "pathlib"],
        "has_hardcoded_paths": true,
        "complexity": "simple",
        "size_bytes": 646,
        "notes": "Prototype, part of numbered series"
      },
      {
        "filename": "chatgpt_script_keep_look1.py",
        "path": "chatgpt/chatgpt_script_keep_look1.py",
        "decision": "archive",
        "rationale": "Prototype variant 1 - numbered iteration.",
        "category": "archive",
        "target_dir": ".archive/data_tools/prototypes/",
        "dependencies": ["shutil", "pathlib"],
        "has_hardcoded_paths": true,
        "complexity": "simple",
        "size_bytes": 536,
        "notes": "Prototype iteration 1"
      },
      {
        "filename": "chatgpt_script_keep_look2.py",
        "path": "chatgpt/chatgpt_script_keep_look2.py",
        "decision": "archive",
        "rationale": "Prototype variant 2 - numbered iteration.",
        "category": "archive",
        "target_dir": ".archive/data_tools/prototypes/",
        "dependencies": ["shutil", "pathlib"],
        "has_hardcoded_paths": true,
        "complexity": "simple",
        "size_bytes": 360,
        "notes": "Prototype iteration 2"
      },
      {
        "filename": "chatgpt_pipeline.py",
        "path": "chatgpt/chatgpt_pipeline.py",
        "decision": "archive",
        "rationale": "Pipeline orchestration script - likely experimental or one-off pipeline setup. May have hardcoded workflow.",
        "category": "archive",
        "target_dir": ".archive/data_tools/prototypes/",
        "dependencies": ["pathlib"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 8338,
        "notes": "Pipeline orchestration, may be superseded by other tools"
      },
      {
        "filename": "main.py",
        "path": "chatgpt/main.py",
        "decision": "archive",
        "rationale": "Empty/minimal stub file (171 bytes). Likely placeholder or abandoned entry point.",
        "category": "archive",
        "target_dir": ".archive/data_tools/prototypes/",
        "dependencies": [],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 171,
        "notes": "Minimal stub file, likely unused"
      },
      {
        "filename": "run_instantiate_final_memories_tests.py",
        "path": "chatgpt/run_instantiate_final_memories_tests.py",
        "decision": "archive",
        "rationale": "Test runner script - testing infrastructure, not production tool. Tests should live in tests/ directory.",
        "category": "archive",
        "target_dir": ".archive/data_tools/prototypes/",
        "dependencies": ["pathlib"],
        "has_hardcoded_paths": false,
        "complexity": "moderate",
        "size_bytes": 11139,
        "notes": "Test runner, should be in tests/ if kept"
      },
      {
        "filename": "test_code_search.py",
        "path": "chatgpt/test_code_search.py",
        "decision": "archive",
        "rationale": "Test script for code search functionality. Testing infrastructure.",
        "category": "archive",
        "target_dir": ".archive/data_tools/prototypes/",
        "dependencies": ["chromadb"],
        "has_hardcoded_paths": false,
        "complexity": "simple",
        "size_bytes": 1862,
        "notes": "Test script, should be in tests/ if kept"
      }
    ]
  },
  "summary": {
    "total_scripts": 51,
    "migrate_count": 40,
    "archive_count": 11,
    "skip_count": 0,
    "categories": {
      "analysis": {
        "count": 3,
        "target": "src/ta_lab2/tools/data_tools/analysis/",
        "scripts": ["generate_function_map.py", "generate_function_map_with_purpose.py", "tree_structure.py"]
      },
      "processing": {
        "count": 1,
        "target": "src/ta_lab2/tools/data_tools/processing/",
        "scripts": ["DataFrame_Consolidation.py"]
      },
      "memory": {
        "count": 16,
        "target": "src/ta_lab2/tools/data_tools/memory/",
        "scripts": [
          "embed_codebase.py",
          "embed_memories.py",
          "generate_memories_from_code.py",
          "generate_memories_from_conversations.py",
          "generate_memories_from_diffs.py",
          "combine_memories.py",
          "memory_bank_rest.py",
          "memory_bank_engine_rest.py",
          "memory_build_registry.py",
          "memory_headers_dedup.py",
          "memory_headers_step1_deterministic.py",
          "memory_headers_step2_openai_enrich.py",
          "memory_instantiate_children_step3.py",
          "instantiate_final_memories.py",
          "setup_mem0.py",
          "setup_mem0_direct.py"
        ]
      },
      "export": {
        "count": 7,
        "target": "src/ta_lab2/tools/data_tools/export/",
        "scripts": [
          "export_chatgpt_conversations.py",
          "chatgpt_export_clean.py",
          "chatgpt_export_diff.py",
          "extract_kept_chats_from_keepfile.py",
          "process_claude_history.py",
          "process_new_chatgpt_dump.py",
          "convert_claude_code_to_chatgpt_format.py"
        ]
      },
      "context": {
        "count": 5,
        "target": "src/ta_lab2/tools/data_tools/context/",
        "scripts": [
          "ask_project.py",
          "chat_with_context.py",
          "get_context.py",
          "create_reasoning_engine.py",
          "query_reasoning_engine.py"
        ]
      },
      "generators": {
        "count": 6,
        "target": "src/ta_lab2/tools/data_tools/generators/",
        "scripts": [
          "intelligence_report_generator.py",
          "category_digest_generator.py",
          "review_generator.py",
          "review_triage_generator.py",
          "finetuning_data_generator.py",
          "generate_commits_txt.py"
        ]
      },
      "archive": {
        "count": 2,
        "target": ".archive/data_tools/one_offs/",
        "scripts": []
      }
    },
    "archive_targets": {
      "one_offs": {
        "count": 5,
        "target": ".archive/data_tools/one_offs/",
        "scripts": [
          "write_daily_emas.py",
          "write_multi_tf_emas.py",
          "write_ema_multi_tf_cal.py",
          "upsert_new_emas_canUpdate.py",
          "github instruction.py"
        ]
      },
      "prototypes": {
        "count": 6,
        "target": ".archive/data_tools/prototypes/",
        "scripts": [
          "chatgpt_script_look.py",
          "chatgpt_script_keep_look.py",
          "chatgpt_script_keep_look1.py",
          "chatgpt_script_keep_look2.py",
          "chatgpt_pipeline.py",
          "main.py",
          "run_instantiate_final_memories_tests.py",
          "test_code_search.py"
        ]
      }
    },
    "external_dependencies": [
      "openai",
      "chromadb",
      "mem0",
      "google.auth",
      "google.auth.transport.requests",
      "requests",
      "pandas"
    ],
    "complexity_breakdown": {
      "simple": 18,
      "moderate": 23,
      "complex": 10
    },
    "hardcoded_paths_found": 6,
    "total_size_bytes": 436763,
    "largest_scripts": [
      {
        "filename": "generate_memories_from_diffs.py",
        "size_bytes": 58320,
        "category": "memory"
      },
      {
        "filename": "chatgpt_export_diff.py",
        "size_bytes": 23975,
        "category": "export"
      },
      {
        "filename": "memory_instantiate_children_step3.py",
        "size_bytes": 20121,
        "category": "memory"
      },
      {
        "filename": "ask_project.py",
        "size_bytes": 17520,
        "category": "context"
      },
      {
        "filename": "instantiate_final_memories.py",
        "size_bytes": 15712,
        "category": "memory"
      }
    ]
  }
}
