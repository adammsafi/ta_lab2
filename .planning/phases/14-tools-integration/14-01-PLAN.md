---
phase: 14-tools-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/14-tools-integration/14-01-discovery.json
autonomous: true

must_haves:
  truths:
    - "All 51 Data_Tools scripts inspected and categorized"
    - "Migrate vs archive decisions documented with rationale"
    - "Functional groupings defined for migrated scripts"
  artifacts:
    - path: ".planning/phases/14-tools-integration/14-01-discovery.json"
      provides: "Script categorization manifest"
      contains: "migrate|archive"
  key_links: []
---

<objective>
Discover and categorize all Data_Tools scripts for migration vs archiving

Purpose: Establish clear migration scope before moving any files. Inspect each script to determine: (1) migrate to data_tools/, (2) archive as prototype/one-off, or (3) skip as duplicate of existing ta_lab2 functionality.

Output: Discovery manifest (14-01-discovery.json) with categorization decisions
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/14-tools-integration/14-CONTEXT.md
@.planning/phases/14-tools-integration/14-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Inspect root-level Data_Tools scripts</name>
  <files>.planning/phases/14-tools-integration/14-01-discovery.json</files>
  <action>
Inspect each Python script in C:\Users\asafi\Downloads\Data_Tools\ (root level only, not chatgpt/ subdirectory):

1. Read file header/docstring to understand purpose
2. Check for experiment/prototype markers (test_, temp_, scratch_, experimental_)
3. Check if functionality already exists in ta_lab2 (especially tools/ai_orchestrator/memory/)
4. Determine functional category:
   - analysis: Code analysis tools (function maps, tree structure)
   - database_utils: Database/EMA utilities (write_*, upsert_*)
   - processing: Data transformation (DataFrame_*)
   - archive: One-offs, instructions, duplicates

Scripts to inspect:
- generate_function_map.py (likely: analysis)
- generate_function_map_with_purpose.py (likely: analysis)
- tree_structure.py (likely: analysis)
- DataFrame_Consolidation.py (likely: processing)
- write_daily_emas.py (likely: database_utils)
- write_multi_tf_emas.py (likely: database_utils)
- write_ema_multi_tf_cal.py (likely: database_utils)
- upsert_new_emas_canUpdate.py (likely: database_utils)
- github instruction.py (likely: archive - one-off instruction file)

Create initial JSON structure:
```json
{
  "discovery_date": "2026-02-02",
  "data_tools_path": "C:/Users/asafi/Downloads/Data_Tools",
  "scripts": {
    "root": [],
    "chatgpt": []
  }
}
```

Each script entry:
```json
{
  "filename": "script.py",
  "path": "relative/path",
  "decision": "migrate|archive|skip",
  "rationale": "Why this decision",
  "category": "analysis|database_utils|memory|export|processing|utilities",
  "target_dir": "src/ta_lab2/tools/data_tools/{category}/",
  "dependencies": ["openai", "chromadb"],
  "has_hardcoded_paths": true|false,
  "complexity": "simple|moderate|complex"
}
```
  </action>
  <verify>JSON file exists with all 12 root-level scripts categorized</verify>
  <done>Root scripts have decision, rationale, category, and target_dir fields populated</done>
</task>

<task type="auto">
  <name>Task 2: Inspect chatgpt/ subdirectory scripts</name>
  <files>.planning/phases/14-tools-integration/14-01-discovery.json</files>
  <action>
Inspect each Python script in C:\Users\asafi\Downloads\Data_Tools\chatgpt\:

Categorization guidance:
- **migrate/memory**: Core memory/embedding tools (embed_*, generate_memories_*, memory_bank*, setup_mem0*)
- **migrate/export**: ChatGPT export processing (export_*, chatgpt_export_*, extract_kept_chats*)
- **migrate/context**: Context/reasoning tools (get_context, chat_with_context, create_reasoning_engine, query_reasoning_engine)
- **migrate/processing**: Data processing (process_*, convert_*, combine_memories)
- **migrate/generators**: Report generators (review_generator, category_digest_generator, intelligence_report_generator, finetuning_data_generator)
- **archive**: Prototype/test scripts (chatgpt_script_look*.py, chatgpt_script_keep_look*.py - numbered variations indicate prototyping)

Scripts requiring careful inspection:
- ask_project.py - Large (17KB), check if it overlaps with existing ta_lab2 tools
- generate_memories_from_diffs.py - Very large (58KB), check complexity
- memory_instantiate_children_step3.py - Pipeline step, check dependencies
- run_instantiate_final_memories_tests.py - Test file, likely archive
- instantiate_final_memories.py - Check if duplicate of memory_instantiate_children_step3

Default rule: When in doubt, migrate. Better to have and clean up than lose useful tools.

Add entries to scripts.chatgpt array in discovery JSON.
  </action>
  <verify>JSON file has all 39 chatgpt/ scripts categorized</verify>
  <done>All chatgpt scripts have decision, rationale, category fields; total 51 scripts categorized</done>
</task>

<task type="auto">
  <name>Task 3: Create functional grouping summary</name>
  <files>.planning/phases/14-tools-integration/14-01-discovery.json</files>
  <action>
Add summary section to discovery JSON with counts per category:

```json
{
  "summary": {
    "total_scripts": 51,
    "migrate_count": N,
    "archive_count": M,
    "skip_count": 0,
    "categories": {
      "analysis": {"count": X, "target": "src/ta_lab2/tools/data_tools/analysis/"},
      "database_utils": {"count": X, "target": "src/ta_lab2/tools/data_tools/database_utils/"},
      "memory": {"count": X, "target": "src/ta_lab2/tools/data_tools/memory/"},
      "export": {"count": X, "target": "src/ta_lab2/tools/data_tools/export/"},
      "processing": {"count": X, "target": "src/ta_lab2/tools/data_tools/processing/"},
      "generators": {"count": X, "target": "src/ta_lab2/tools/data_tools/generators/"},
      "context": {"count": X, "target": "src/ta_lab2/tools/data_tools/context/"}
    },
    "archive_targets": {
      "prototypes": {"count": X, "target": ".archive/data_tools/prototypes/"},
      "one_offs": {"count": X, "target": ".archive/data_tools/one_offs/"}
    },
    "external_dependencies": ["openai", "chromadb", "flask", "etc"]
  }
}
```

Validate:
- migrate_count + archive_count + skip_count = 51
- All migrated scripts have valid category and target_dir
- Dependencies list is complete (will inform pyproject.toml updates)
  </action>
  <verify>Summary section exists with accurate counts; total = 51</verify>
  <done>Discovery manifest complete with categorization summary and dependency list</done>
</task>

</tasks>

<verification>
- `cat .planning/phases/14-tools-integration/14-01-discovery.json | python -m json.tool` - Valid JSON
- JSON has scripts.root array with ~12 entries
- JSON has scripts.chatgpt array with ~39 entries
- JSON has summary section with counts
- All scripts have "decision" field with value "migrate", "archive", or "skip"
</verification>

<success_criteria>
1. All 51 Data_Tools Python scripts inspected and categorized
2. Each script has: decision, rationale, category (if migrating), target_dir, dependencies
3. Summary shows migrate/archive breakdown and functional categories
4. External dependencies identified for pyproject.toml update in later plan
</success_criteria>

<output>
After completion, create `.planning/phases/14-tools-integration/14-01-SUMMARY.md`
</output>
