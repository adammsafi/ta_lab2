---
phase: 08-ta_lab2-signals
plan: 06
type: execute
wave: 4
depends_on: [08-05]
files_modified:
  - src/ta_lab2/scripts/signals/validate_reproducibility.py
  - src/ta_lab2/scripts/signals/run_all_signal_refreshes.py
  - tests/signals/test_reproducibility.py
  - tests/signals/test_signal_pipeline_integration.py
autonomous: true

must_haves:
  truths:
    - "Running same backtest twice produces identical results (PnL, metrics)"
    - "Feature hash mismatch detected when underlying data changes"
    - "Reproducibility validation CLI confirms or fails determinism"
    - "Orchestrated signal refresh runs all three signal types with validation"
    - "Pipeline continues when one signal type fails (partial failure handling)"
  artifacts:
    - path: "src/ta_lab2/scripts/signals/validate_reproducibility.py"
      provides: "Reproducibility validation utilities"
      exports: ["validate_backtest_reproducibility", "compare_backtest_runs"]
    - path: "src/ta_lab2/scripts/signals/run_all_signal_refreshes.py"
      provides: "Orchestrated signal pipeline"
      min_lines: 100
    - path: "tests/signals/test_reproducibility.py"
      provides: "Reproducibility test suite"
      min_lines: 80
  key_links:
    - from: "src/ta_lab2/scripts/signals/validate_reproducibility.py"
      to: "src/ta_lab2/scripts/signals/signal_utils.py"
      via: "uses compute_feature_hash for validation"
      pattern: "compute_feature_hash"
    - from: "src/ta_lab2/scripts/signals/run_all_signal_refreshes.py"
      to: "refresh_cmc_signals_*.py"
      via: "orchestrates all signal refresh scripts"
      pattern: "generate_signals_(ema|rsi|atr)"
---

<objective>
Implement reproducibility validation: run identical backtests and verify deterministic results, detect feature data changes via hash comparison, orchestrate full signal pipeline with validation.

Purpose: Complete Phase 8 success criteria #3 (reproducibility). Provides confidence that backtest results are repeatable and detectable when underlying data changes.

Output: Reproducibility validation module, orchestrated pipeline script, 20+ tests including determinism tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-ta_lab2-signals/08-CONTEXT.md
@.planning/phases/08-ta_lab2-signals/08-RESEARCH.md
@.planning/phases/08-ta_lab2-signals/08-05-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create reproducibility validation module</name>
  <files>
    src/ta_lab2/scripts/signals/validate_reproducibility.py
  </files>
  <action>
    Create validation utilities for reproducibility:

    ```python
    """
    Reproducibility validation for signal generation and backtesting.

    Implements triple-layer reproducibility:
    1. Deterministic timestamp queries (no random sampling)
    2. Feature hashing (git-style content hash)
    3. Version tracking (signal_version, vbt_version, params_hash)
    """

    from dataclasses import dataclass
    from typing import Optional
    import pandas as pd
    from sqlalchemy import Engine, text
    from ta_lab2.scripts.signals.signal_utils import compute_feature_hash, compute_params_hash
    from ta_lab2.scripts.backtests import SignalBacktester, BacktestResult

    @dataclass
    class ReproducibilityReport:
        is_reproducible: bool
        run_id_1: str
        run_id_2: str
        pnl_match: bool
        metrics_match: bool
        trade_count_match: bool
        feature_hash_match: bool
        differences: list[str]

    def validate_backtest_reproducibility(
        backtester: SignalBacktester,
        signal_type: str,
        signal_id: int,
        asset_id: int,
        start_ts: pd.Timestamp,
        end_ts: pd.Timestamp,
        strict: bool = True,
    ) -> ReproducibilityReport:
        """
        Run backtest twice and verify identical results.

        Args:
            backtester: Configured SignalBacktester
            signal_type, signal_id, asset_id: Backtest target
            start_ts, end_ts: Date range
            strict: If True, fail on any difference. If False, warn but proceed.

        Returns:
            ReproducibilityReport with comparison results
        """
        # Run 1
        result1 = backtester.run_backtest(
            signal_type, signal_id, asset_id, start_ts, end_ts
        )

        # Run 2 (identical parameters)
        result2 = backtester.run_backtest(
            signal_type, signal_id, asset_id, start_ts, end_ts
        )

        # Compare results
        differences = []

        pnl_match = abs(result1.total_return - result2.total_return) < 1e-10
        if not pnl_match:
            differences.append(f"PnL mismatch: {result1.total_return} vs {result2.total_return}")

        metrics_match = _compare_metrics(result1.metrics, result2.metrics)
        if not metrics_match:
            differences.append("Metrics differ between runs")

        trade_count_match = result1.trade_count == result2.trade_count
        if not trade_count_match:
            differences.append(f"Trade count: {result1.trade_count} vs {result2.trade_count}")

        is_reproducible = pnl_match and metrics_match and trade_count_match

        return ReproducibilityReport(
            is_reproducible=is_reproducible,
            run_id_1=result1.run_id,
            run_id_2=result2.run_id,
            pnl_match=pnl_match,
            metrics_match=metrics_match,
            trade_count_match=trade_count_match,
            feature_hash_match=True,  # Same data source, always matches
            differences=differences,
        )

    def compare_backtest_runs(
        engine: Engine,
        run_id_1: str,
        run_id_2: str,
    ) -> ReproducibilityReport:
        """
        Compare two stored backtest runs.

        Used to compare historical runs after data updates.
        """
        # Load runs from cmc_backtest_runs
        run1 = _load_run(engine, run_id_1)
        run2 = _load_run(engine, run_id_2)

        # Compare feature hashes
        feature_hash_match = run1['feature_hash'] == run2['feature_hash']

        # Compare results
        pnl_match = abs(run1['total_return'] - run2['total_return']) < 1e-10

        # Load and compare trades
        trades1 = _load_trades(engine, run_id_1)
        trades2 = _load_trades(engine, run_id_2)
        trade_count_match = len(trades1) == len(trades2)

        differences = []
        if not feature_hash_match:
            differences.append(f"Feature hash: {run1['feature_hash']} vs {run2['feature_hash']}")
        if not pnl_match:
            differences.append(f"PnL: {run1['total_return']} vs {run2['total_return']}")

        return ReproducibilityReport(...)

    def validate_feature_hash_current(
        engine: Engine,
        signal_type: str,
        signal_id: int,
        asset_id: int,
        mode: str = 'warn',  # 'strict', 'warn', 'trust'
    ) -> tuple[bool, Optional[str]]:
        """
        Validate that current feature data matches stored hash.

        Args:
            mode:
                'strict': Fail if hash mismatch
                'warn': Log warning but proceed
                'trust': Skip validation

        Returns:
            (is_valid, message)
        """
        if mode == 'trust':
            return True, None

        # Get stored hash from most recent signal
        stored_hash = _get_latest_feature_hash(engine, signal_type, signal_id, asset_id)

        if stored_hash is None:
            return True, "No stored hash found (first run)"

        # Compute current hash
        current_hash = _compute_current_feature_hash(engine, asset_id)

        if stored_hash == current_hash:
            return True, None

        message = f"Feature data changed. Stored: {stored_hash}, Current: {current_hash}"

        if mode == 'strict':
            return False, message

        # mode == 'warn'
        return True, f"WARNING: {message}"

    def _compare_metrics(m1: dict, m2: dict, tolerance: float = 1e-10) -> bool:
        """Compare metric dictionaries with tolerance."""
        for key in m1:
            if key not in m2:
                return False
            if abs(m1[key] - m2[key]) > tolerance:
                return False
        return True
    ```
  </action>
  <verify>
    Run: `python -c "from ta_lab2.scripts.signals.validate_reproducibility import validate_backtest_reproducibility; print('OK')"`
    Verify imports work.
  </verify>
  <done>
    Reproducibility validation module with validate_backtest_reproducibility, compare_backtest_runs, validate_feature_hash_current functions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create orchestrated signal pipeline</name>
  <files>
    src/ta_lab2/scripts/signals/run_all_signal_refreshes.py
  </files>
  <action>
    Create orchestrated pipeline following run_all_feature_refreshes.py pattern:

    ```python
    #!/usr/bin/env python
    """
    Orchestrated signal generation pipeline.

    Runs all signal types in parallel, then validates reproducibility.

    Usage:
        python run_all_signal_refreshes.py
        python run_all_signal_refreshes.py --full-refresh
        python run_all_signal_refreshes.py --validate-only
    """

    import argparse
    import logging
    import os
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from dataclasses import dataclass
    from datetime import datetime
    from typing import List, Optional
    from sqlalchemy import create_engine

    from ta_lab2.scripts.signals import SignalStateManager, SignalStateConfig, load_active_signals
    from ta_lab2.scripts.signals.generate_signals_ema import EMASignalGenerator
    from ta_lab2.scripts.signals.generate_signals_rsi import RSISignalGenerator
    from ta_lab2.scripts.signals.generate_signals_atr import ATRSignalGenerator
    from ta_lab2.scripts.signals.validate_reproducibility import validate_backtest_reproducibility
    from ta_lab2.scripts.backtests import SignalBacktester
    from ta_lab2.backtests.costs import CostModel

    @dataclass
    class RefreshResult:
        signal_type: str
        signals_generated: int
        duration_seconds: float
        success: bool
        error: Optional[str] = None

    def refresh_signal_type(
        engine,
        signal_type: str,
        ids: list[int],
        full_refresh: bool,
    ) -> RefreshResult:
        """
        Refresh a single signal type.

        Catches exceptions to allow partial failure handling.
        """
        start = datetime.now()

        try:
            config = SignalStateConfig(signal_type=signal_type)
            state_manager = SignalStateManager(engine, config)
            state_manager.ensure_state_table()

            # Select generator based on type
            generators = {
                'ema_crossover': EMASignalGenerator,
                'rsi_mean_revert': RSISignalGenerator,
                'atr_breakout': ATRSignalGenerator,
            }

            generator = generators[signal_type](engine, state_manager)
            configs = load_active_signals(engine, signal_type)

            total_signals = 0
            for cfg in configs:
                n = generator.generate_for_ids(ids, cfg, full_refresh=full_refresh)
                total_signals += n

            duration = (datetime.now() - start).total_seconds()
            return RefreshResult(signal_type, total_signals, duration, True)

        except Exception as e:
            duration = (datetime.now() - start).total_seconds()
            return RefreshResult(signal_type, 0, duration, False, str(e))

    def run_parallel_refresh(
        engine,
        ids: list[int],
        full_refresh: bool,
        max_workers: int = 3,
    ) -> List[RefreshResult]:
        """
        Run all signal types in parallel.

        Partial failure handling: Each signal type runs independently.
        One failure does not stop the others.
        """
        signal_types = ['ema_crossover', 'rsi_mean_revert', 'atr_breakout']
        results = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(refresh_signal_type, engine, st, ids, full_refresh): st
                for st in signal_types
            }

            for future in as_completed(futures):
                result = future.result()
                results.append(result)

        return results

    def validate_pipeline_reproducibility(
        engine,
        sample_asset_id: int,
        sample_start: pd.Timestamp,
        sample_end: pd.Timestamp,
    ) -> bool:
        """
        Validate reproducibility for all signal types.

        Uses sample asset and date range for validation.
        Returns True if all pass.
        """
        cost_model = CostModel()  # Clean mode for validation
        backtester = SignalBacktester(engine, cost_model)

        all_pass = True
        for signal_type in ['ema_crossover', 'rsi_mean_revert', 'atr_breakout']:
            configs = load_active_signals(engine, signal_type)
            for cfg in configs:
                report = validate_backtest_reproducibility(
                    backtester,
                    signal_type,
                    cfg['signal_id'],
                    sample_asset_id,
                    sample_start,
                    sample_end,
                )
                if not report.is_reproducible:
                    logger.error(f"Reproducibility FAILED: {signal_type}/{cfg['signal_name']}")
                    logger.error(f"  Differences: {report.differences}")
                    all_pass = False
                else:
                    logger.info(f"Reproducibility OK: {signal_type}/{cfg['signal_name']}")

        return all_pass

    def main():
        parser = argparse.ArgumentParser()
        parser.add_argument('--ids', type=int, nargs='+')
        parser.add_argument('--full-refresh', action='store_true')
        parser.add_argument('--validate-only', action='store_true',
                            help='Skip refresh, only validate reproducibility')
        parser.add_argument('--skip-validation', action='store_true',
                            help='Skip reproducibility validation')
        parser.add_argument('--fail-fast', action='store_true',
                            help='Exit immediately on first signal type failure (default: continue)')
        parser.add_argument('--parallel', type=int, default=3,
                            help='Max parallel workers')
        parser.add_argument('--verbose', '-v', action='store_true')
        args = parser.parse_args()

        logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)
        logger = logging.getLogger(__name__)

        engine = create_engine(os.environ['TARGET_DB_URL'])

        ids = args.ids or _get_all_asset_ids(engine)

        # Phase 1: Signal generation (unless validate-only)
        if not args.validate_only:
            logger.info("Phase 1: Signal generation")
            results = run_parallel_refresh(engine, ids, args.full_refresh, args.parallel)

            for r in results:
                status = "OK" if r.success else f"FAILED: {r.error}"
                logger.info(f"  {r.signal_type}: {r.signals_generated} signals in {r.duration_seconds:.1f}s [{status}]")

            failed = [r for r in results if not r.success]
            succeeded = [r for r in results if r.success]

            if failed:
                logger.warning(f"{len(failed)} signal type(s) failed, {len(succeeded)} succeeded")
                for r in failed:
                    logger.error(f"  FAILED: {r.signal_type} - {r.error}")

                if args.fail_fast:
                    logger.error("--fail-fast enabled, exiting")
                    return 1
                else:
                    logger.info("Continuing with partial results (default behavior)")

        # Phase 2: Reproducibility validation (unless skipped)
        if not args.skip_validation:
            logger.info("Phase 2: Reproducibility validation")
            sample_asset = ids[0]
            sample_start = pd.Timestamp('2023-01-01', tz='UTC')
            sample_end = pd.Timestamp('2023-12-31', tz='UTC')

            if validate_pipeline_reproducibility(engine, sample_asset, sample_start, sample_end):
                logger.info("Reproducibility validation PASSED")
            else:
                logger.error("Reproducibility validation FAILED")
                return 1

        logger.info("Signal pipeline complete")
        return 0

    if __name__ == '__main__':
        exit(main())
    ```
  </action>
  <verify>
    Run: `python src/ta_lab2/scripts/signals/run_all_signal_refreshes.py --help`
    Verify help text shows --full-refresh, --validate-only, --skip-validation, --fail-fast flags.
  </verify>
  <done>
    Orchestrated pipeline with parallel execution, partial failure handling (continues by default), and reproducibility validation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create comprehensive reproducibility and integration tests</name>
  <files>
    tests/signals/test_reproducibility.py
    tests/signals/test_signal_pipeline_integration.py
  </files>
  <action>
    **test_reproducibility.py:**
    Critical tests for reproducibility guarantees:

    ```python
    import pytest
    import pandas as pd
    from unittest.mock import MagicMock, patch

    class TestReproducibility:
        """Tests for reproducibility validation."""

        def test_identical_backtests_produce_identical_pnl(self, mock_backtester):
            """CRITICAL: Same backtest twice = same PnL."""
            # Run twice with identical params
            result1 = mock_backtester.run_backtest(...)
            result2 = mock_backtester.run_backtest(...)
            assert result1.total_return == result2.total_return

        def test_identical_backtests_produce_identical_sharpe(self, mock_backtester):
            """CRITICAL: Same backtest twice = same Sharpe."""
            result1 = mock_backtester.run_backtest(...)
            result2 = mock_backtester.run_backtest(...)
            assert result1.sharpe_ratio == result2.sharpe_ratio

        def test_identical_backtests_produce_identical_trade_count(self, mock_backtester):
            """CRITICAL: Same backtest twice = same trade count."""
            result1 = mock_backtester.run_backtest(...)
            result2 = mock_backtester.run_backtest(...)
            assert result1.trade_count == result2.trade_count

        def test_validate_backtest_reproducibility_returns_true_on_match(self):
            """validate_backtest_reproducibility returns is_reproducible=True when results match."""
            ...

        def test_validate_backtest_reproducibility_returns_false_on_mismatch(self):
            """Returns is_reproducible=False when results differ."""
            ...

        def test_feature_hash_mismatch_detected(self):
            """validate_feature_hash_current detects data changes."""
            ...

        def test_feature_hash_validation_strict_mode_fails(self):
            """strict mode raises error on hash mismatch."""
            ...

        def test_feature_hash_validation_warn_mode_proceeds(self):
            """warn mode logs warning but returns True."""
            ...

        def test_feature_hash_validation_trust_mode_skips(self):
            """trust mode skips validation entirely."""
            ...

        def test_compare_backtest_runs_loads_from_db(self):
            """compare_backtest_runs queries cmc_backtest_runs."""
            ...

        def test_reproducibility_report_includes_all_fields(self):
            """ReproducibilityReport has all required fields."""
            ...

    # Integration tests (skipif not TARGET_DB_URL)
    @pytest.mark.skipif(not os.environ.get('TARGET_DB_URL'), reason="No database")
    class TestReproducibilityIntegration:

        def test_end_to_end_reproducibility(self, engine):
            """
            Full reproducibility test:
            1. Generate signals
            2. Run backtest
            3. Run backtest again
            4. Verify identical results
            """
            ...

        def test_data_change_detected_by_hash(self, engine):
            """
            1. Generate signals with feature_hash
            2. Modify underlying feature data
            3. validate_feature_hash_current returns False
            """
            ...
    ```

    **test_signal_pipeline_integration.py:**
    Integration tests for full pipeline:

    - test_run_parallel_refresh_all_signal_types: All 3 types refresh
    - test_pipeline_handles_partial_failure: One failure doesn't stop others
    - test_pipeline_partial_failure_logs_both_success_and_failure: Verify logging
    - test_pipeline_fail_fast_exits_on_first_failure: --fail-fast mode
    - test_validate_pipeline_reproducibility_all_pass: Returns True when all pass
    - test_validate_pipeline_reproducibility_one_fails: Returns False on any failure
    - test_full_pipeline_end_to_end: Refresh -> backtest -> validate
    - test_cli_full_refresh_flag: --full-refresh triggers full recalc
    - test_cli_validate_only_skips_refresh: --validate-only skips generation
    - test_cli_skip_validation_flag: --skip-validation skips validation
  </action>
  <verify>
    Run: `pytest tests/signals/test_reproducibility.py tests/signals/test_signal_pipeline_integration.py -v`
    All tests pass.
  </verify>
  <done>
    20+ tests for reproducibility validation and pipeline integration, including partial failure handling tests.
  </done>
</task>

</tasks>

<verification>
Phase verification checklist:
- [ ] validate_backtest_reproducibility returns is_reproducible=True for identical runs
- [ ] Feature hash mismatch detected when data changes
- [ ] Reproducibility validation CLI works with --strict, --warn, --trust modes
- [ ] run_all_signal_refreshes.py orchestrates all 3 signal types
- [ ] Parallel execution works with ThreadPoolExecutor
- [ ] Partial failure handling: pipeline continues when one signal type fails (default)
- [ ] --fail-fast flag exits immediately on first failure
- [ ] Validation phase runs after generation (unless skipped)
- [ ] All tests pass (20+ tests)
</verification>

<success_criteria>
1. Backtest reruns produce identical signals and PnL
2. Feature hash changes detected when underlying data modified
3. Orchestrated pipeline generates all signal types with validation
4. Pipeline continues on partial failure (unless --fail-fast)
5. CLI supports various modes (full-refresh, validate-only, skip-validation, fail-fast)
</success_criteria>

<output>
After completion, create `.planning/phases/08-ta_lab2-signals/08-06-SUMMARY.md`
</output>
