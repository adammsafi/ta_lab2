---
phase: 19-memory-validation-release
plan: 05
type: execute
wave: 4
depends_on: [19-04]
files_modified:
  - .planning/phases/19-memory-validation-release/19-VALIDATION.md
  - src/ta_lab2/tools/ai_orchestrator/memory/run_validation.py
autonomous: false

must_haves:
  truths:
    - "VALIDATION.md has clear PASS/FAIL status at top"
    - "Full metrics report with coverage statistics"
    - "Query test results documented"
    - "Duplicate detection report included"
    - "Validation is release blocker (must pass for v0.5.0)"
  artifacts:
    - path: ".planning/phases/19-memory-validation-release/19-VALIDATION.md"
      provides: "Complete memory validation report"
      contains: "# Memory Validation Report"
    - path: "src/ta_lab2/tools/ai_orchestrator/memory/run_validation.py"
      provides: "Script to run full validation suite"
      exports: ["run_full_validation", "main"]
  key_links:
    - from: "run_validation.py"
      to: "graph_validation.py"
      via: "imports validation"
      pattern: "from .graph_validation import"
    - from: "run_validation.py"
      to: "similarity.py"
      via: "imports duplicate detection"
      pattern: "from .similarity import"
---

<objective>
Run full memory validation and generate VALIDATION.md report

Purpose: Execute the complete memory validation suite (graph integrity, query capabilities, duplicate detection) and generate VALIDATION.md with clear PASS/FAIL status. This is a release blocker - validation must pass before v0.5.0 tag.

Output: VALIDATION.md report and run_validation.py script
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-memory-validation-release/19-CONTEXT.md

# Validation modules from Plans 19-01 through 19-04
@src/ta_lab2/tools/ai_orchestrator/memory/__init__.py
@src/ta_lab2/tools/ai_orchestrator/memory/graph_validation.py
@src/ta_lab2/tools/ai_orchestrator/memory/query_validation.py
@src/ta_lab2/tools/ai_orchestrator/memory/similarity.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create run_validation.py script</name>
  <files>src/ta_lab2/tools/ai_orchestrator/memory/run_validation.py</files>
  <action>
Create run_validation.py to orchestrate full validation:

```python
"""Run full memory validation suite.

Executes:
1. Function extraction and indexing (if not already done)
2. Relationship linking (if not already done)
3. Duplicate detection
4. Memory graph validation
5. Query capability tests
6. Report generation

Usage:
    # Run from command line
    python -m ta_lab2.tools.ai_orchestrator.memory.run_validation

    # Or import and run
    from ta_lab2.tools.ai_orchestrator.memory.run_validation import run_full_validation
    report = run_full_validation()
"""
```

1. **ValidationReport dataclass:**
   - timestamp: datetime
   - phase: str = "19-memory-validation-release"
   - milestone: str = "v0.5.0"

   - indexing_result: Optional[IndexingResult]
   - linking_result: Optional[LinkingResult]
   - duplicate_report: DuplicateReport
   - graph_validation: MemoryGraphValidation
   - query_validation: QueryValidation

   - overall_status: str  # "PASS" or "FAIL"
   - blocking_issues: List[str]
   - warnings: List[str]
   - duration_seconds: float

2. **run_full_validation(index_if_needed: bool = True, output_path: Path = None) -> ValidationReport:**
   Steps:
   a. Initialize Mem0Client
   b. Check if indexing needed (query for function_definition count)
   c. If index_if_needed and count < threshold:
      - Run index_codebase_functions() on src/ta_lab2
      - Run link_codebase_relationships()
   d. Run detect_duplicates() on all extracted functions
   e. Run validate_memory_graph()
   f. Run validate_queries()
   g. Determine overall_status:
      - PASS: graph_validation.is_valid AND query_validation.is_valid
      - FAIL: Either validation failed
   h. Collect blocking_issues and warnings
   i. If output_path provided, write VALIDATION.md

3. **generate_validation_md(report: ValidationReport) -> str:**
   Generate markdown report:

   ```markdown
   # Memory Validation Report

   **Status:** [PASS/FAIL]
   **Timestamp:** YYYY-MM-DD HH:MM:SS
   **Phase:** 19-memory-validation-release
   **Milestone:** v0.5.0
   **Duration:** X.XX seconds

   ## Summary

   | Check | Status | Details |
   |-------|--------|---------|
   | Function Indexing | OK/WARN | X functions indexed |
   | Relationship Linking | OK/WARN | X relationships created |
   | Graph Validation | PASS/FAIL | X% orphan rate |
   | Query Validation | PASS/FAIL | X/5 tests passed |
   | Duplicate Detection | INFO | X exact, Y similar |

   ## Blocking Issues

   [List any issues that block release, or "None"]

   ## Graph Validation Details

   [Include graph_validation.markdown_report()]

   ## Query Validation Details

   [Include query_validation.markdown_report()]

   ## Duplicate Detection Details

   [Include duplicate_report.markdown_summary()]

   ## Warnings

   [List any non-blocking concerns]

   ---
   *Generated by run_validation.py*
   ```

4. **main():**
   - Parse CLI args (--index, --output, --verbose)
   - Run validation
   - Print summary to console
   - Write VALIDATION.md to output path
   - Exit with code 0 if PASS, 1 if FAIL

Make this script executable with proper argparse and progress output.
  </action>
  <verify>
`python -c "from ta_lab2.tools.ai_orchestrator.memory.run_validation import ValidationReport, run_full_validation; print('Import OK')"`
  </verify>
  <done>run_validation.py script created with full validation orchestration</done>
</task>

<task type="auto">
  <name>Task 2: Run validation and generate VALIDATION.md</name>
  <files>.planning/phases/19-memory-validation-release/19-VALIDATION.md</files>
  <action>
Run the full validation suite:

```bash
python -m ta_lab2.tools.ai_orchestrator.memory.run_validation \
  --index \
  --output .planning/phases/19-memory-validation-release/19-VALIDATION.md \
  --verbose
```

The script will:
1. Index src/ta_lab2 functions if not already indexed
2. Create relationships (contains, calls, imports)
3. Detect duplicates with three-tier thresholds
4. Validate memory graph (orphans, targets)
5. Test query capabilities
6. Generate VALIDATION.md

If validation FAILS:
- Document the specific failures in VALIDATION.md
- List blocking issues at top of report
- Do NOT proceed to release plan (19-06)
- Checkpoint will require user decision on how to proceed

If validation PASSES:
- VALIDATION.md will show status: PASS
- Ready to proceed to release plan
  </action>
  <verify>
Check VALIDATION.md exists and has content:
`head -20 .planning/phases/19-memory-validation-release/19-VALIDATION.md`
  </verify>
  <done>VALIDATION.md generated with full memory validation report</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete memory validation suite:
- Function extraction and indexing (AST-based)
- Relationship linking (contains, calls, imports)
- Duplicate detection with three tiers (95%+, 85-95%, 70-85%)
- Memory graph validation (orphan detection, target verification)
- Query capability tests (5 query types)
- VALIDATION.md report with clear PASS/FAIL status
  </what-built>
  <how-to-verify>
1. Review VALIDATION.md at `.planning/phases/19-memory-validation-release/19-VALIDATION.md`

2. Check overall status at top of report:
   - **PASS**: All validations passed, ready for v0.5.0 release
   - **FAIL**: Review blocking issues section

3. If PASS, verify key metrics look reasonable:
   - Function count (should be hundreds for ta_lab2)
   - Relationship count (should be significant)
   - Orphan rate (should be < 5%)
   - Query pass rate (should be >= 80%)

4. If FAIL, review blocking issues:
   - Orphan rate too high? May need to adjust threshold or index more
   - Query tests failing? May need to fix query logic
   - Missing targets? May need to re-run relationship linking

5. Review duplicate detection section:
   - Are canonical suggestions reasonable?
   - Any unexpected exact duplicates to investigate?

Expected outcome: Validation PASSES with reasonable metrics, allowing v0.5.0 release.
  </how-to-verify>
  <resume-signal>
If PASS: Type "validation-approved" to proceed to release
If FAIL: Describe the blocking issues and desired action (fix vs accept)
  </resume-signal>
</task>

</tasks>

<verification>
1. run_validation.py imports and runs without errors
2. VALIDATION.md exists in phase directory
3. VALIDATION.md has clear PASS/FAIL status at top
4. Report includes all sections (graph, query, duplicates)
5. User reviews and approves validation results
</verification>

<success_criteria>
- run_validation.py orchestrates all validation steps
- VALIDATION.md generated with full report
- Clear PASS/FAIL status at top of report
- Blocking issues section identifies release blockers
- User checkpoint verifies validation results
- If PASS, ready for v0.5.0 release (Plan 19-06)
</success_criteria>

<output>
After completion, create `.planning/phases/19-memory-validation-release/19-05-SUMMARY.md`
</output>
