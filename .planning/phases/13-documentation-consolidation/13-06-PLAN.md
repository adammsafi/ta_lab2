---
phase: 13-documentation-consolidation
plan: 06
type: execute
wave: 4
depends_on: [13-05]
files_modified:
  - src/ta_lab2/tools/docs/update_doc_memory.py
autonomous: true
user_setup:
  - service: qdrant
    why: "Memory system backend for storing document relationships"
    env_vars:
      - name: QDRANT_SERVER_MODE
        source: "Set to 'true' for server mode"
    dashboard_config: []

must_haves:
  truths:
    - "Memory contains moved_to relationships for converted documents"
    - "Section-level memories created for major document sections"
    - "Phase 13 memory snapshot tagged appropriately"
  artifacts:
    - path: "src/ta_lab2/tools/docs/update_doc_memory.py"
      provides: "Memory update script for document conversions"
      exports: ["update_memory_for_doc", "create_phase_snapshot"]
  key_links:
    - from: "update_doc_memory.py"
      to: "mem0_client"
      via: "import"
      pattern: "from ta_lab2.tools.ai_orchestrator.memory"
    - from: "memory"
      to: "converted docs"
      via: "moved_to relationships"
      pattern: "moved_to.*docs/"
---

<objective>
Update memory with document conversion relationships and create phase snapshot.

Purpose: Create moved_to relationships in memory for each converted document (MEMO-13), then create a phase-level memory snapshot tagged with phase 13 (MEMO-14).
Output: Memory updated with document relationships, phase snapshot created
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-documentation-consolidation/13-CONTEXT.md

# Prior plan outputs
@.planning/phases/13-documentation-consolidation/13-05-SUMMARY.md
@.planning/phases/13-documentation-consolidation/projecttt_inventory.json
@.archive/documentation/manifest.json

# Memory infrastructure (Phase 11 patterns)
@src/ta_lab2/tools/ai_orchestrator/memory/snapshot/batch_indexer.py
@src/ta_lab2/tools/ai_orchestrator/memory/mem0_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create document memory update module</name>
  <files>src/ta_lab2/tools/docs/update_doc_memory.py</files>
  <action>
Create memory update module for document conversions.

1. Create update_doc_memory.py implementing:

   @dataclass
   class DocConversionRecord:
       original_path: str      # Path to original .docx/.xlsx
       converted_path: str     # Path to converted .md
       archive_path: str       # Path in .archive/documentation/
       document_type: str      # "docx" or "xlsx"
       sections: list[str]     # H2 headings extracted from converted doc
       converted_at: str       # ISO timestamp

   - extract_sections_from_markdown(md_path: Path) -> list[str]
     - Parse Markdown file for H2 headings (## ...)
     - Return list of section titles
     - Used for granular memory tracking

   - update_memory_for_doc(record: DocConversionRecord, dry_run: bool = False) -> int
     - Create document-level memory:
       f"Document {record.original_path} converted to Markdown at {record.converted_path}. "
       f"Original archived in {record.archive_path}."
     - Create section-level memories for each H2 heading
     - Use metadata:
       - source: "doc_conversion_phase13"
       - category: "file_migration"
       - original_path, converted_path, archive_path
       - phase: 13
     - Return count of memories created
     - Follow batch_indexer.py pattern for Mem0 client usage

   - check_memory_exists(client, original_path: str) -> bool
     - Search for existing conversion memory
     - Prevent duplicates (idempotent)

   - batch_update_memories(records: list[DocConversionRecord], dry_run: bool = False) -> BatchIndexResult
     - Process all conversion records
     - Track totals (added, skipped, errors)
     - Log progress

2. Add to __init__.py exports
  </action>
  <verify>
    python -c "from ta_lab2.tools.docs.update_doc_memory import update_memory_for_doc, DocConversionRecord; print('Import OK')"
  </verify>
  <done>
    - update_doc_memory.py module created
    - DocConversionRecord dataclass for tracking conversions
    - Memory update functions follow Phase 11 patterns
    - Idempotent with duplicate checking
  </done>
</task>

<task type="auto">
  <name>Task 2: Update memory with document conversions</name>
  <files>None (memory operations)</files>
  <action>
Execute memory updates for all converted documents.

1. Load conversion data:
   - Read .archive/documentation/manifest.json for archive paths
   - Read projecttt_inventory.json for original paths
   - Scan docs/ for converted .md files

2. Build DocConversionRecord for each converted document:
   - Match original path to converted path via filename
   - Extract sections from converted Markdown
   - Set archive_path from manifest

3. Run batch memory update:
   - First with dry_run=True to verify counts
   - Then with dry_run=False to create memories

4. Expected memory structure:
   - Document-level: ~40-50 memories (one per converted file)
   - Section-level: ~100-200 memories (varies by document complexity)
   - Total: ~150-250 new memories

5. Log results:
   - Documents processed
   - Sections indexed
   - Memories created
   - Any errors or skips

6. Verify memories queryable:
   from ta_lab2.tools.ai_orchestrator.memory import MemoryService
   memory = MemoryService()
   results = memory.search("CoreComponents documentation", limit=5)
   # Should return converted doc info
  </action>
  <verify>
    python -c "
from ta_lab2.tools.ai_orchestrator.memory.mem0_client import get_mem0_client
client = get_mem0_client()
results = client.search('document converted phase 13', user_id='orchestrator', limit=5)
print(f'Found {len(results)} phase 13 conversion memories')
"
  </verify>
  <done>
    - Memory updated with moved_to relationships for all converted docs
    - Section-level memories created for granular navigation
    - Memories queryable via search
    - No duplicate memories created
  </done>
</task>

<task type="auto">
  <name>Task 3: Create phase 13 memory snapshot</name>
  <files>None (memory operations)</files>
  <action>
Create phase-level memory snapshot following Phase 11 patterns.

1. Create phase snapshot memory:
   f"Phase 13 Documentation Consolidation complete. "
   f"Converted {N} ProjectTT documents to Markdown in docs/. "
   f"Original files archived in .archive/documentation/. "
   f"Memory updated with {M} document relationships."

2. Use snapshot metadata pattern from Phase 11:
   metadata = {
       "source": "phase_snapshot",
       "phase": 13,
       "phase_name": "documentation-consolidation",
       "milestone": "v0.5.0",
       "completed_at": datetime.now(timezone.utc).isoformat(),
       "documents_converted": N,
       "memories_created": M,
       "categories": ["architecture", "features", "planning", "reference"],
       "tags": ["phase_13_complete", "doc_consolidation_v0.5.0"]
   }

3. Add snapshot to memory system:
   client.add(
       messages=[{"role": "user", "content": snapshot_message}],
       user_id="orchestrator",
       metadata=metadata,
       infer=False  # Skip LLM conflict detection
   )

4. Verify snapshot queryable:
   results = client.search("Phase 13 complete", user_id="orchestrator", limit=5)
   # Should return phase snapshot
  </action>
  <verify>
    python -c "
from ta_lab2.tools.ai_orchestrator.memory.mem0_client import get_mem0_client
client = get_mem0_client()
results = client.search('Phase 13 Documentation Consolidation complete', user_id='orchestrator', limit=5)
print(f'Phase 13 snapshot found: {len(results) > 0}')
"
  </verify>
  <done>
    - Phase 13 memory snapshot created
    - Snapshot includes completion metrics
    - Snapshot queryable via search
    - Tags include phase_13_complete
  </done>
</task>

</tasks>

<verification>
All verification commands must pass:

```bash
# Module imports
python -c "from ta_lab2.tools.docs.update_doc_memory import update_memory_for_doc, batch_update_memories"

# Memories created (requires Qdrant running)
python -c "
from ta_lab2.tools.ai_orchestrator.memory.mem0_client import get_mem0_client
client = get_mem0_client()

# Check document memories
doc_results = client.search('converted to Markdown', user_id='orchestrator', limit=10)
print(f'Document conversion memories: {len(doc_results)}')

# Check phase snapshot
snapshot_results = client.search('Phase 13 complete', user_id='orchestrator', limit=5)
print(f'Phase snapshot found: {len(snapshot_results) > 0}')
"
```
</verification>

<success_criteria>
- [ ] update_doc_memory.py module created with memory update functions
- [ ] DocConversionRecord dataclass captures conversion metadata
- [ ] Memory contains moved_to relationships for converted documents
- [ ] Section-level memories created for major headings
- [ ] Phase 13 memory snapshot created with completion metrics
- [ ] All memories queryable via search
- [ ] MEMO-13 and MEMO-14 requirements satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/13-documentation-consolidation/13-06-SUMMARY.md`
</output>
