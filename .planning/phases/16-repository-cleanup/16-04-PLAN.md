---
phase: 16-repository-cleanup
plan: 04
type: execute
wave: 2
depends_on: [16-01, 16-02, 16-03]
files_modified:
  - src/ta_lab2/tools/cleanup/__init__.py
  - src/ta_lab2/tools/cleanup/duplicates.py
  - .planning/phases/16-repository-cleanup/duplicates_report.json
  - .archive/duplicates/manifest.json
autonomous: true

must_haves:
  truths:
    - "SHA256-based duplicate detection tool exists and works"
    - "Exact duplicates identified across codebase"
    - "Duplicates report generated with canonical/duplicate pairs"
    - "Non-canonical duplicates archived (src/ copy kept as canonical)"
  artifacts:
    - path: "src/ta_lab2/tools/cleanup/duplicates.py"
      provides: "Duplicate detection tool"
      exports: ["find_duplicates", "generate_duplicate_report"]
    - path: ".planning/phases/16-repository-cleanup/duplicates_report.json"
      provides: "Duplicate detection results"
      contains: "exact_duplicates"
  key_links:
    - from: "duplicates.py"
      to: "archive manifest"
      via: "archive_duplicate function"
      pattern: "def archive_duplicate"
---

<objective>
Create SHA256-based duplicate detection tool and identify/archive exact duplicates

Purpose: Find and archive exact duplicate files per CLEAN-03 - SHA256 matching for content deduplication
Output: Duplicate detection tool, report of duplicates found, non-canonical duplicates archived
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-repository-cleanup/16-CONTEXT.md
@.planning/phases/16-repository-cleanup/16-RESEARCH.md

# Archive infrastructure
@src/ta_lab2/tools/archive/__init__.py
@src/ta_lab2/tools/archive/manifest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create duplicate detection module</name>
  <files>
    src/ta_lab2/tools/cleanup/__init__.py
    src/ta_lab2/tools/cleanup/duplicates.py
  </files>
  <action>
Create `src/ta_lab2/tools/cleanup/duplicates.py` for SHA256-based duplicate detection:

**Module structure:**
```python
"""Duplicate file detection via SHA256 checksums.

Provides tools for finding exact duplicate files across the codebase
using content-based hashing, not filename or path comparison.

Example:
    >>> from ta_lab2.tools.cleanup import find_duplicates
    >>> duplicates = find_duplicates(Path("."), pattern="**/*.py")
    >>> print(f"Found {len(duplicates)} duplicate groups")
"""
from pathlib import Path
from collections import defaultdict
import hashlib
import json
from dataclasses import dataclass, field, asdict
from typing import Optional

# Import existing archive checksum function
from ta_lab2.tools.archive import compute_file_checksum

# Directories to exclude from scanning
EXCLUDE_DIRS = {".git", ".venv", ".venv311", "__pycache__", ".pytest_cache", "node_modules"}


@dataclass
class DuplicateGroup:
    """Group of files with identical content."""
    sha256: str
    size_bytes: int
    files: list[Path] = field(default_factory=list)
    canonical: Optional[Path] = None  # Preferred file to keep

    def __str__(self) -> str:
        return f"DuplicateGroup({len(self.files)} files, {self.size_bytes:,} bytes, sha256={self.sha256[:16]}...)"


def find_duplicates(
    root: Path,
    pattern: str = "**/*",
    min_size: int = 1,
    exclude_dirs: set[str] | None = None
) -> dict[str, DuplicateGroup]:
    """Find duplicate files by SHA256 hash.

    Args:
        root: Root directory to scan
        pattern: Glob pattern for files to include
        min_size: Minimum file size in bytes (skip tiny files)
        exclude_dirs: Directory names to exclude (defaults to EXCLUDE_DIRS)

    Returns:
        Dict mapping SHA256 hash to DuplicateGroup for files with duplicates
    """
    exclude = exclude_dirs or EXCLUDE_DIRS
    hash_to_files: dict[str, list[tuple[Path, int]]] = defaultdict(list)

    for file_path in root.glob(pattern):
        # Skip directories
        if not file_path.is_file():
            continue

        # Skip excluded directories
        if any(part in exclude for part in file_path.parts):
            continue

        # Skip small files
        try:
            size = file_path.stat().st_size
            if size < min_size:
                continue
        except OSError:
            continue

        # Hash file
        try:
            file_hash = compute_file_checksum(file_path)
            hash_to_files[file_hash].append((file_path, size))
        except (OSError, IOError):
            continue

    # Build DuplicateGroup for hashes with multiple files
    duplicates = {}
    for file_hash, files in hash_to_files.items():
        if len(files) >= 2:
            # Sort by path for consistent ordering
            files.sort(key=lambda x: str(x[0]))
            group = DuplicateGroup(
                sha256=file_hash,
                size_bytes=files[0][1],
                files=[f[0] for f in files]
            )
            # Prefer src/ file as canonical
            src_files = [f for f in group.files if "src" in f.parts]
            group.canonical = src_files[0] if src_files else group.files[0]
            duplicates[file_hash] = group

    return duplicates


def generate_duplicate_report(duplicates: dict[str, DuplicateGroup]) -> dict:
    """Generate JSON-serializable report of duplicates.

    Returns:
        Report dict with summary and duplicate groups categorized by type
    """
    report = {
        "$schema": "https://ta_lab2.local/schemas/duplicate-report/v1.0.0",
        "version": "1.0.0",
        "summary": {
            "total_duplicate_groups": len(duplicates),
            "total_duplicate_files": sum(len(g.files) for g in duplicates.values()),
            "total_wasted_bytes": sum(g.size_bytes * (len(g.files) - 1) for g in duplicates.values())
        },
        "src_canonical": [],  # Duplicates where src/ copy is canonical
        "non_src_duplicates": [],  # Duplicates with no src/ copy
    }

    for sha256, group in duplicates.items():
        entry = {
            "sha256": sha256,
            "size_bytes": group.size_bytes,
            "canonical": str(group.canonical),
            "duplicates": [str(f) for f in group.files if f != group.canonical]
        }

        if any("src" in f.parts for f in group.files):
            report["src_canonical"].append(entry)
        else:
            report["non_src_duplicates"].append(entry)

    return report
```

**Create __init__.py for cleanup module:**
```python
"""Cleanup tools for repository maintenance.

Provides tools for duplicate detection, similarity analysis, and
repository organization.
"""
from ta_lab2.tools.cleanup.duplicates import (
    DuplicateGroup,
    EXCLUDE_DIRS,
    find_duplicates,
    generate_duplicate_report,
)

__all__ = [
    "DuplicateGroup",
    "EXCLUDE_DIRS",
    "find_duplicates",
    "generate_duplicate_report",
]
```
  </action>
  <verify>
- `python -c "from ta_lab2.tools.cleanup import find_duplicates; print('OK')"`
- `python -c "from ta_lab2.tools.cleanup import generate_duplicate_report; print('OK')"`
- Module has docstrings and type hints
  </verify>
  <done>
Duplicate detection module created with find_duplicates() and generate_duplicate_report() functions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run duplicate detection and generate report</name>
  <files>
    .planning/phases/16-repository-cleanup/duplicates_report.json
  </files>
  <action>
Run duplicate detection across the codebase and generate report:

```python
from pathlib import Path
import json
from ta_lab2.tools.cleanup import find_duplicates, generate_duplicate_report

# Scan entire project for Python file duplicates
root = Path(".")
duplicates = find_duplicates(root, pattern="**/*.py", min_size=100)

# Generate report
report = generate_duplicate_report(duplicates)

# Save report
report_path = Path(".planning/phases/16-repository-cleanup/duplicates_report.json")
report_path.write_text(json.dumps(report, indent=2))

print(f"Found {report['summary']['total_duplicate_groups']} duplicate groups")
print(f"Total wasted space: {report['summary']['total_wasted_bytes']:,} bytes")
```

**Expected output categories:**
1. **src_canonical**: Files where src/ copy should be kept, others archived
2. **non_src_duplicates**: Duplicates outside src/ - review and decide canonical

**Analysis guidance:**
- Duplicates in .archive/ are expected (archived copies)
- Duplicates in tests/ might be fixture files (may be intentional)
- Duplicates in scripts/ likely need consolidation
  </action>
  <verify>
- `test -f .planning/phases/16-repository-cleanup/duplicates_report.json`
- `cat .planning/phases/16-repository-cleanup/duplicates_report.json | python -m json.tool`
- Report contains "$schema" and "summary" keys
  </verify>
  <done>
Duplicate detection complete. Report saved to .planning/phases/16-repository-cleanup/duplicates_report.json
  </done>
</task>

<task type="auto">
  <name>Task 3: Archive non-canonical duplicates</name>
  <files>
    .archive/duplicates/2026-02-03/
    .archive/duplicates/manifest.json
  </files>
  <action>
Archive non-canonical duplicate files based on the report:

**Decision rules:**
1. If one copy is in src/ta_lab2/: Keep src/ copy, archive others
2. If no src/ copy: Keep the first alphabetically, archive others
3. Skip files already in .archive/ (they're already archived)
4. Skip test fixtures (files in tests/fixtures/ or test_data/)

**For each duplicate group where archiving makes sense:**
1. Verify canonical file exists and matches checksum
2. Create archive path: `.archive/duplicates/2026-02-03/{relative_path}`
3. Use `git mv` to move duplicate to archive (if tracked)
4. For untracked files: `cp` to archive, then `rm` source
5. Add entry to manifest:
```json
{
  "original_path": "path/to/duplicate.py",
  "archive_path": ".archive/duplicates/2026-02-03/path/to/duplicate.py",
  "action": "duplicate_archived",
  "timestamp": "2026-02-03T...",
  "sha256_checksum": "...",
  "canonical_path": "src/ta_lab2/path/to/original.py",
  "archive_reason": "Exact duplicate of canonical file at {canonical_path}"
}
```

**Manual review if:**
- Duplicate is in different subsystem (might be intentional copy)
- Duplicate has different filename but same content
- Duplicate is in tests/ (might be test fixture)

Create manifest at `.archive/duplicates/manifest.json` following existing schema.
  </action>
  <verify>
- `cat .archive/duplicates/manifest.json | python -m json.tool`
- Re-run duplicate detection: groups involving archived files should show fewer duplicates
- Canonical files still exist at original locations
  </verify>
  <done>
Non-canonical duplicates archived to .archive/duplicates/2026-02-03/ with manifest tracking. Canonical copies preserved.
  </done>
</task>

</tasks>

<verification>
Phase 16 Plan 04 verification:

1. Duplicate detection tool works:
   ```bash
   python -c "from ta_lab2.tools.cleanup import find_duplicates; print(len(find_duplicates(Path('.'), '**/*.py')))"
   ```

2. Report generated:
   ```bash
   python -c "import json; r=json.load(open('.planning/phases/16-repository-cleanup/duplicates_report.json')); print(r['summary'])"
   ```

3. Manifest valid:
   ```bash
   python -c "import json; json.load(open('.archive/duplicates/manifest.json'))"
   ```

4. No unexpected duplicates remain (re-scan should show only expected duplicates):
   ```bash
   python -c "
   from pathlib import Path
   from ta_lab2.tools.cleanup import find_duplicates
   dups = find_duplicates(Path('.'), '**/*.py')
   # Filter out expected duplicates (tests, fixtures, etc.)
   unexpected = [g for g in dups.values() if not any('.archive' in str(f) for f in g.files)]
   print(f'Unexpected duplicate groups: {len(unexpected)}')
   "
   ```
</verification>

<success_criteria>
- src/ta_lab2/tools/cleanup/duplicates.py exists with find_duplicates() and generate_duplicate_report()
- .planning/phases/16-repository-cleanup/duplicates_report.json exists with valid schema
- .archive/duplicates/manifest.json tracks all archived duplicates
- Non-canonical duplicates archived (except intentional ones like test fixtures)
- Canonical copies remain in place and are importable
</success_criteria>

<output>
After completion, create `.planning/phases/16-repository-cleanup/16-04-SUMMARY.md`
</output>
